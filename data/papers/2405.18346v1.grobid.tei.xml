<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation</title>
				<funder ref="#_aqJgAdw #_bY95PgC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>International Journal of Innovative Science and Research Technology</publisher>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-28">2024-05-28</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anjanava</forename><surname>Biswas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wrick</forename><surname>Talukdar</surname></persName>
						</author>
						<title level="a" type="main">Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">International Journal of Innovative Science and Research Technology (IJISRT)</title>
						<title level="j" type="abbrev">International Journal of Innovative Science and Research Technology (IJISRT)</title>
						<idno type="ISSN">No:-2456-2165</idno>
						<idno type="eISSN">2456-2165</idno>
						<imprint>
							<publisher>International Journal of Innovative Science and Research Technology</publisher>
							<biblScope unit="page" from="994" to="1008"/>
							<date type="published" when="2024-05-28" />
						</imprint>
					</monogr>
					<idno type="MD5">60E210A5E42C0F248B0744BE1693D25F</idno>
					<idno type="DOI">10.38124/ijisrt/ijisrt24may1483</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-24T14:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety. This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings. The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care. I.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Clinical documentation is a critical component of healthcare delivery, serving as a comprehensive record of patient encounters, diagnoses, treatment plans, and progress. However, the time-consuming nature of documentation has become a significant burden for healthcare professionals, leading to burnout, medical errors, and compromised patient safety <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. With multiple researches finding that physicians and clinicians spending an average of two to three hours per day on documentation tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">6</ref>] (cite source), there is a pressing need for innovative solutions to streamline this process.</p><p>Generative AI, a branch of artificial intelligence focused on generating new content based on training data, holds immense potential for transforming clinical documentation practices. By leveraging natural language processing (NLP) and automatic speech recognition (ASR) technologies, generative AI models can transcribe patientclinician interactions and generate draft clinical notes, capturing the subjective patient information, objective examination findings, assessments, and treatment plans.</p><p>This paper presents a case study exploring the application of generative AI for generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes, two widely recognized formats for behavioral health related clinical documentation. We demonstrate the use of advanced prompting techniques to guide large language models (LLMs) in generating comprehensive and structured clinical notes based on transcribed patient-clinician interactions.</p><p>We also discuss how these formats of clinical notes can be enhanced and improved to reduce accuracy issues, errors, and improve notes quality throughout the patient's treatment journey via augmenting the system with document data, and additional audio/video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>Several studies have investigated approaches to alleviate the documentation burden on healthcare professionals and improve the accuracy and quality of clinical notes. One line of research has focused on leveraging artificial intelligence (AI) models to generate clinical notes automatically.</p><p>A study by <ref type="bibr" target="#b4">Kernberg et al. (2023)</ref>  <ref type="bibr" target="#b4">[7]</ref> evaluated the performance of ChatGPT-4, which is a conversational AI interface built by OpenAI based on GPT-3.5 and GPT-4 large language models, in generating SOAP (Subjective, Objective, Assessment, and Plan) notes based on transcripts of simulated patient-provider encounters. The findings revealed significant variations in errors, accuracy, and note quality produced by the AI model. On average, 23.6 errors per clinical case were identified, with omissions being the predominant type of error, accounting for 86% of the errors. Notably, the accuracy of the generated notes exhibited an inverse relationship with the length of the transcripts and the complexity of the data elements, suggesting potential limitations in handling intricate medical cases. The study concluded that the quality and reliability of AI-generated clinical notes did not meet the standards required for clinical use, highlighting the need for further research to address accuracy, variability, and potential error issues.</p><p>Another line of research has explored the use of medical scribes, individuals who accompany healthcare providers during patient encounters and document the interactions in real-time. <ref type="bibr" target="#b5">Rule et al. (2022)</ref>  <ref type="bibr" target="#b5">[8]</ref> conducted a retrospective cross-sectional study analyzing over 50,000 outpatient progress notes, some written with scribe assistance and others without. The study revealed that scribed notes were consistently longer than those written without scribe assistance, with much of the additional content originating from note templates. Furthermore, scribed notes were more likely to include certain templated lists, such as the patient's medications or past medical history. However, the study also observed significant variations in how working with scribes affected each provider's documentation workflow, suggesting that providers adapt their note-taking practices to varying degrees when assisted by scribes. The findings indicate that while the use of scribes may contribute to note bloat, individual providers' documentation workflows and note templates play a significant role in shaping the contents of scribed notes.</p><p>These studies underscore the ongoing efforts to improve clinical documentation practices and highlight the potential benefits, as well as the limitations, of leveraging AI-based solutions and medical scribes to address this challenge. We extend these existing studies, specifically focusing on the ability of AI models to generate accurate, detailed, yet succinct medical notes using commercially available as well as open-source Large Language Models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The methodology employed in this case study aimed to simulate a real-world healthcare scenario, leveraging cutting-edge technologies to streamline the clinical documentation process. The study followed a systematic approach, encompassing data collection, transcription, prompt engineering, and model selection and deployment.</p><p>Ethical considerations, such as maintaining patient confidentiality and adhering to guidelines, were of utmost importance throughout the process. This included using research and educational synthetic data available in the public domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>To obtain authentic patient-clinician interactions, we collaborated with University of Leeds Researcher and Clinical Psychologist Lecturer Judith Johnson and utilized her experimental therapy sessions available on YouTube <ref type="bibr">[9]</ref>. These videos featured unscripted dialogues between Johnson and several subjects and mental/behavioral health patients portraying as clients, providing realistic examples of therapeutic encounters. While the sessions were simulated for educational purposes, they accurately depicted the nuances and dynamics of patient-clinician interactions. Appropriate measures were taken to ensure the protection of privacy and confidentiality, as the video content did not contain any personally identifiable information (PII) or protected health information (PHI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transcription</head><p>The recorded patient-clinician interactions were then processed using state-of-the-art automatic speech recognition (ASR) models, such as OpenAI Whisper <ref type="bibr" target="#b6">[10]</ref>. These models were trained on vast datasets to accurately transcribe the audio or video files into text format, capturing the nuances and intricacies of the conversations. The transcription process was crucial for providing the necessary input for the subsequent steps.</p><p>While Whisper excels at transcribing audio accurately, it does not perform speaker diarization out of the box. Speaker diarization, the process of separating speech segments by different speakers, is crucial for understanding the context and flow of conversations, especially in multispeaker scenarios like patient-clinician interactions. To address this challenge, we analyzed two approaches to diarize the speech with a utterance classification mechanism. This model is an extension of the original Whisper model, specifically designed to perform fast transcription of long audio and comes in with built in support for plugging in diarization model such as pyannotate/speaker-diarization <ref type="bibr" target="#b7">[12]</ref>. However, we failed to achieve any significant and successful diarization with the speech audio rendering the result not appropriate for notes generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÉò Diarization Leveraging GPT-3.5 for Utterance Classification</head><p>In this approach, we first obtained the plain text transcription from Whisper. Then, we utilized GPT-3.5, a powerful language model, to classify each utterance as either spoken by the patient or the clinician. This classification task can be formulated as a binary sequence labeling problem, where each token in the input sequence is assigned a label (0 for clinician, 1 for patient). The probabilities for each class are normalized using the softmax function, which is expressed mathematically as:</p><formula xml:id="formula_0">Softmax(z i ) = e z i ‚àë e z k k</formula><p>Here, ( z ) represents the logits or raw outputs from the model's final neural network layer. This normalization ensures that the predicted probabilities are distributed over the two classes, facilitating a clear classification.</p><p>To measure the performance of our model, we use the cross-entropy loss function, which quantifies the difference between the predicted probabilities and the actual labels. The loss function is crucial for models dealing with probabilities and is defined as:</p><formula xml:id="formula_1">ùêø = -‚àë(ùë¶ ùëñ log(ùëù ùëñ ) + (1 -ùë¶ ùëñ ) log(1 -ùëù ùëñ )) ùëÅ ùëñ=1</formula><p>Here, (ùë¶ ùëñ ) is the true label, and (ùëù ùëñ ) is the model's predicted probability for each class. The function effectively penalizes the probability divergence from the actual label, driving the model to improve its predictions during training. We also discuss the sequence labeling schema in detail, explaining how the model handles dependencies between labels in a sequence. This aspect is critical since the context within which words or phrases appear can significantly influence their classification. So while I'm looking at these, just tell me in your own words how you've been feeling this week. Well, I would say, say I think a little bit better. I don't know if I know exactly why, but I feel a little bit better. Like when I woke up in the morning, I was able to get up more easily. And I think that when I was just like reading the paper, even the sports section, I felt like I was able to concentrate a little better. Oh, that's wonderful. I'm really glad to hear that. And it looks like you're sleeping better too. Well, I think that what I meant by that was mostly that I didn't oversleep. Okay. Because I had been spending a lot of time in bed and I didn't get up. I mean, I would say I got up at seven, but I didn't really get up at seven.</p><p>[0] Hi, Eve. Good to see you again.</p><p>[1] Hi.</p><p>[0] Can I take a look at your scores?</p><p>[1] Sure.</p><p>[0] So while I'm looking at these, just tell me in your own words how you've been feeling this week.</p><p>[1] Well, I would say, say I think a little bit better. I don't know if I know exactly why, but I feel a little bit better. Like when I woke up in the morning, I was able to get up more easily. And I think that when I was just like reading the paper, even the sports section, I felt like I was able to concentrate a little better.</p><p>[0] Oh, that's wonderful. I'm really glad to hear that. And it looks like you're sleeping better too.</p><p>[1] Well, I think that what I meant by that was mostly that I didn't oversleep.</p><p>[0] Okay.</p><p>[1] Because I had been spending a lot of time in bed and I didn't get up. I mean, I would say I got up at seven, but I didn't really get up at seven. Additionally, we consider various metrics to evaluate the model's performance, including accuracy, precision, recall, and F1-score. Accuracy is calculated by: Accuracy =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Correct Predictions Total Number of Predictions</head><p>Precision and recall are particularly useful for assessing performance in scenarios where class distribution is imbalanced, which is often the case in conversational datasets where one party may speak more than the other. These metrics, combined with the confusion matrix, provide a comprehensive evaluation of the model. It is worth noting, however, that while this study did not evaluate the effect on the quality of clinical notes due to error rates in classification, such a classification task could potentially be conducted using cheaper and much smaller models such as BERT. These alternatives might offer a more cost-effective solution while still maintaining reasonable accuracy, especially in resource-constrained environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prompt Engineering</head><p>To generate structural SOAP and BIRP notes, we considered four large language models for evaluation using diarized speech to text data -GPT-3.5 Turbo, GPT-4 Turbo, Claude V3, and Mixtral8x7b Instruct, and Llama-3 70B Instruct. We employed advanced prompting techniqueswith zero shot, and one shotto guide the models in generating structural SOAP and BIRP notes. With GPT-4 Turbo, we were able to leverage its unique function calling feature to further generate programmatically structured SOAP/BIRP notes in JSON format which made it easier to process and consume. For this study, we considered a standard SOAP and BIRP note format (see appendix for detailed format). We employed two main approaches: basic prompting and advanced prompting techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÉò Basic Prompting</head><p>In the basic prompting approach, we provided the diarized transcript as input to the language models, along with instructions to generate SOAP or BIRP notes based on the conversation. This method relied on the model's understanding of the prompt and its ability to extract relevant information from the transcript to construct the clinical note structure. The basic prompting technique served as a baseline to evaluate the models' out-of-the-box performance in generating structured clinical notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example prompt ÔÇ∑ Transcript: {{ Diarized transcript of patient-clinician interaction }}</head><p>ÔÇ∑ Instructions: Based on the above transcript, please generate a SOAP note following the Subjective, Objective, Assessment, and Plan format. Include all relevant details from the conversation, and maintain patient confidentiality by avoiding the use of any personally identifiable information.</p><p>The basic prompting technique is analogous to using chat-based interfaces or conversational AI applications like ChatGPT or Claude.ai, where users provide prompts or instructions, and the model generates responses based on its understanding of the input. However, these conversational interfaces often have limitations in terms of the level of control and customization available for the prompts.</p><p>While basic prompting allowed the models to generate notes in the desired format, the quality, completeness, and adherence to the specified structure varied significantly across models and transcripts. Some models struggled to capture all the relevant information or organize it correctly within the SOAP or BIRP sections. Additionally, the notes generated through basic prompting often lacked consistency in terms of content organization, level of detail, and overall coherence.</p><p>The limitations of basic prompting highlighted the need for more advanced techniques to guide the language models effectively. Factors such as the model's understanding of the prompt, its ability to comprehend the context and nuances of the conversation, and its capacity to structure information coherently played crucial roles in determining the quality of the generated notes.</p><p>Despite its shortcomings, the basic prompting approach served as a valuable starting point, providing insights into the models' inherent capabilities and revealing areas for improvement through more sophisticated prompting techniques.</p><p>By directly accessing the models and leveraging advanced prompting techniques, researchers and developers can potentially overcome the limitations of chat-based interfaces and gain greater control over the prompting process. Advanced prompting techniques, such as zero-shot and one-shot learning methods, allow for more explicit guidance and customization, enabling the models to generate more accurate, consistent, and structured outputs. Additionally, direct access to the models may unlock advanced features and capabilities that are not available through conversational interfaces, such as programmatic access, fine-tuning, and integration with other systems or applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÉò Advanced Prompting</head><p>To enhance the quality and consistency of the generated notes, we employed advanced prompting techniques, including zero-shot and one-shot learning methods. These techniques aimed to provide more explicit guidance to the models, leveraging their few-shot learning capabilities.</p><p>ÔÇ∑ Zero-Shot Prompting In this approach, we provided detailed instructions related to well-structured SOAP or BIRP notes within the prompt itself, the formatting instructions. The models were expected to understand the desired format using the plain language of the detailed formatting instructions and generate notes accordingly, without any prior fine-tuning or training on similar examples.</p><p>Example prompt ÔÇ∑ Formatting instructions: {{ Detailed instructions on the SOAP/BIRP note structure and formatting, including the specific sections and the information to be included in each section }} ÔÇ∑ Transcript: {{ Diarized transcript of patient-clinician interaction }} ÔÇ∑ Instructions: Based on the above transcript and the provided example, please generate a SOAP/BIRP note following the specified structure and format. Ensure that all relevant information from the transcript is captured in the appropriate sections of the note. Maintain patient confidentiality by avoiding the use of any personally identifiable information.</p><p>The zero-shot prompting approach relied on the model's ability to understand and generalize from the provided instructions. By giving it detailed instructions related to the structure of a SOAP/BIRP note, we aimed to guide the model in generating notes with a similar level of organization and completeness.</p><p>ÔÇ∑ One-shot Prompting Building upon the zero-shot approach, we incorporated a few examples of wellstructured SOAP or BIRP notes within the prompt, along with the corresponding transcripts. This method aimed to provide the models with a better understanding of the desired output format and the mapping between the transcript and the generated note. Ensure that all relevant information from the transcript is captured in the appropriate sections of the note, while maintaining patient confidentiality by avoiding the use of any personally identifiable information.</p><p>By providing multiple examples of well-structured notes and their corresponding transcripts, we aimed to enhance the model's understanding of the desired output format and the relationship between the transcript content and the generated note. This approach leveraged the model's few-shot learning capabilities, allowing it to learn from the provided examples and generalize to new transcripts.</p><p>ÔÇ∑ Structured prompting <ref type="bibr" target="#b9">[13]</ref>: In addition to zero-shot and one-shot prompting, we explored a third prompting technique that leverages the models' ability to understand structured data formats like JSON (JavaScript Object Notation). This approach involved providing the models with detailed instructions and a JSON schema that defined the structure and fields required for the SOAP or BIRP notes.</p><p>JSON Schema is a vocabulary that allows for the annotation and validation of JSON documents. It provides a concise description of the structure and data types expected in a JSON document, enabling the validation of data against the defined schema. By utilizing JSON Schema, we could precisely specify the desired structure and fields for the clinical notes, guiding the models to generate well-formatted and structured outputs.</p><p>Example Prompt ÔÇ∑ Formatting instructions: {{ Detailed instructions on the SOAP/BIRP note structure and formatting, including the specific sections and the information to be included in each section }} [JSON Schema defining the structure and fields for the SOAP/BIRP note] ÔÇ∑ Transcript:</p><p>{{ Diarized transcript of patient-clinician interaction }} ÔÇ∑ Instructions: Based on the above transcript and the provided example, please generate a SOAP/BIRP note following the specified structure and format. Ensure that all relevant information from the transcript is captured in the appropriate sections of the note. Maintain patient confidentiality by avoiding the use of any personally identifiable information.</p><p>In this prompting technique, we provided the models with a JSON schema that defined the structure and fields required for the SOAP or BIRP note. The schema included a description of each section (subjective, objective, assessment, plan) and the specific fields or properties expected within each section.</p><p>For example, the "subjective" section included fields like "chiefComplaint," "symptoms," and "medicalHistory," each with a defined data type (string or array of strings). Similarly, the "objective" section included fields for "vitalSigns," "physicalExamFindings," and "labResults," with nested schemas defining the structure of these fields.</p><p>By providing the models with this structured schema, we aimed to guide them in generating notes that strictly adhered to the specified format and included all the required fields. The models were expected to understand the JSON schema and generate a well-structured JSON object representing the clinical note, with the relevant information from the transcript populating the appropriate fields. This prompting technique leveraged the models' ability to understand and generate structured data formats, allowing for a more precise and controlled generation of clinical notes. Additionally, by using JSON Schema, we could easily validate the generated outputs against the defined schema, ensuring that the notes adhered to the expected structure and field requirements.</p><p>In addition to the zero-shot and one-shot prompting techniques, we explored various strategies to optimize the prompts further. These strategies included:</p><p>ÔÇ∑ Iterative Refinement: We analyzed the initial outputs generated by the models and used the feedback to refine the prompts, improving clarity and specificity. ÔÇ∑ Prompt Chaining: <ref type="bibr" target="#b10">[14]</ref> We experimented with breaking down the note generation task into smaller subtasks and chaining the prompts together, allowing for a more structured and controlled generation process. ÔÇ∑ Prompt Ensembling: <ref type="bibr" target="#b11">[15]</ref> We explored combining outputs from multiple models using different prompting techniques, leveraging the strengths of each model and prompting approach.</p><p>ISSN No:-2456-2165 <ref type="url" target="https://doi.org/10.38124/ijisrt/IJISRT24MAY1483">https://doi.org/10.38124/ijisrt/IJISRT24MAY1483</ref> IJISRT24MAY1483 www.ijisrt.com 1000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Selection and Deployment</head><p>The selection and deployment of the LLM were critical steps in the process. We evaluated various models based on their performance, computational resource requirements, and ethical considerations. Factors such as accuracy, efficiency, and adherence to ethical principles were taken into account. Additionally, we assessed the models' ability to maintain patient confidentiality and avoid the inclusion of identifiable information in the generated notes.</p><p>While Mixtral8x7b Instruct and Llama-3 70B Instruct models are open-source models that can be deployed on selfprovisioned compute environments, Claude V3 and GPT models are proprietary models accessible only via Anthropic and OpenAI platforms respectively (or via partnering cloud provider platforms) as a hosted model service accessible via API calls.</p><p>We considered MMLU <ref type="bibr" target="#b12">[16]</ref> (Multitask Multilingual), Narrative QA <ref type="bibr" target="#b13">[17]</ref>, and MedQA <ref type="bibr" target="#b14">[18]</ref> (open domain QA from professional medical board exams) benchmarks for comparison of the four models. Per CRFM (Center for Research on Foundation Models) HELM (Holistic Evaluation of Language Models) framework <ref type="bibr" target="#b15">[19]</ref>.</p><p>ÔÇ∑ MMLU: Claude V3 outperforms all the three models closely followed by GPT-4, Mixtral, and Llama respectively.</p><p>ÔÇ∑ MedQA: GPT-4 outperforms all the three models closely followed by Llama, Claude V3 and Mixtral respectively. ÔÇ∑ NarrativeQA: Llama outperformed all the three models followed by Mixtral, GPT-4, and Claude V3 respectively.</p><p>We utilized cloud provider platforms to access these models via Python SDK based API calls. Public Cloud providers have fully managed cloud-based services that make all the four models available to use. This reduced the heavy-lifting of securing compute capacity, and deploy the models in a scalable manner. Additionally, models such as Anthropic Claude V3, and OpenAI GPT-4 are only available via SaaS (Software-as-a-Service) platforms since these are proprietary models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparative Analysis of Models for SOAP and BIRP Notes</head><p>We evaluated across 20 SOAP and BIRP notes, each graded for quality by humans and ranging from simple to complex. We investigated the performance of the four models in SOAP and BIRP notes generation tasks using ROUGE-1 <ref type="bibr" target="#b16">[20]</ref> F1 scores. The models assessed include GPT-4, Claude, Llama, and Mixtral, with summaries ranging from basic to complex. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ITERATIVE NOTE IMPROVEMENT FOR EVOLVING PATIENT CARE</head><p>Effective healthcare delivery relies on accurate and comprehensive documentation that captures the patient's journey from initial assessment to ongoing treatment and follow-up. However, patient conditions and treatment plans are not static; they evolve over time as new information emerges, and adjustments are made based on the patient's response and progress. This dynamic nature of healthcare necessitates a flexible and adaptive approach to clinical documentation.</p><p>In the context of generative AI-powered SOAP and BIRP note generation, the ability to iteratively improve and refine these notes becomes paramount. As patients undergo subsequent clinic visits or encounters, additional data is collected, shedding light on their evolving condition, emerging symptoms, or changes in treatment plans. By harnessing this new information, healthcare providers can ensure that the generated SOAP and BIRP notes remain upto-date, comprehensive, and reflective of the patient's current state, ultimately enhancing the quality of care and facilitating better clinical decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Iterative Note Improvement with Subsequent Patient</head><p>Encounters Patient therapy is an ongoing process, and with each visitation or encounter, new information may emerge. This presents an opportunity to refine and improve the generated SOAP and BIRP notes, ensuring that they accurately reflect the patient's evolving condition and treatment plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Incorporating Data from Subsequent Encounters</head><p>During each follow-up visit or encounter, additional data can be collected in the form of audio recordings, transcripts, or supplementary documents (e.g., test results, progress reports). This data can be leveraged to enhance the existing SOAP and BIRP notes, making them more comprehensive and up to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Incremental Note Generation</head><p>Instead of generating entirely new notes from scratch, the LLMs can be prompted to update and refine the existing notes iteratively, incorporating the new information from subsequent encounters. This approach can be achieved through two main methods:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÉò Conditional Note Generation</head><p>In this method, the LLM is provided with the existing SOAP or BIRP note, along with the new data (e.g., transcript, audio recording, supplementary documents) from the subsequent encounter. The prompt instructs the model to generate an updated version of the note, considering the previously documented information and incorporating the new relevant details.</p><p>Example prompt ÔÇ∑ Existing SOAP Note: [Previous SOAP note] ÔÇ∑ New Transcript: [Transcript of the current encounter] ÔÇ∑ Instructions: Based on the existing SOAP note and the new transcript, please generate an updated version of the SOAP note that incorporates relevant information from the current encounter. Maintain the structure and format of the SOAP note, and ensure that all pertinent details from the previous note and the current encounter are accurately reflected.</p><p>ISSN No:-2456-2165 <ref type="url" target="https://doi.org/10.38124/ijisrt/IJISRT24MAY1483">https://doi.org/10.38124/ijisrt/IJISRT24MAY1483</ref> IJISRT24MAY1483 www.ijisrt.com 1002</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÉò Iterative Note Refinement</head><p>This method involves a multi-step process where the LLM is first prompted to extract the relevant information from the new data (transcript, audio recording, supplementary documents) and then integrate it into the existing SOAP or BIRP note.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÇ∑ Step 1: Extract Relevant Information from the New Data</head><p>Example Prompt ÔÇ∑ New Transcript: [Transcript of the current encounter] ÔÇ∑ Instructions: Based on the provided transcript, please extract and summarize the relevant information that should be incorporated into the existing SOAP/BIRP note, such as new symptoms, examination findings, assessments, or treatment plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÇ∑ Step 2: Integrate the Extracted Information into the Existing Note</head><p>Example prompt ÔÇ∑ Existing SOAP Note: [Previous SOAP note] ÔÇ∑ New Information Summary: [Summary from <ref type="bibr">Step 1]</ref> ÔÇ∑ Instructions: Based on the existing SOAP note and the new information summary, please generate an updated version of the SOAP note that seamlessly incorporates the new relevant details while maintaining the structure and format of the note.</p><p>The overall idea is to iterate on the first version of the clinical note by considering it as the primary context and then augmenting it with further context from subsequent information from documents, or audio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Continuous Learning and Adaptation</head><p>As the iterative note improvement process continues over multiple encounters, the LLMs can continuously learn and adapt to the specific patient's case, capturing the nuances and evolution of their condition and treatment plan. This iterative approach not only enhances the accuracy and completeness of the clinical notes but also facilitates a more personalized and patient-centered approach to care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Version Control and Auditing</head><p>To maintain a comprehensive record of the patient's journey and track the changes made to the SOAP and BIRP notes, it is essential to implement version control and auditing mechanisms. Each iteration of the note can be timestamped and archived, allowing healthcare professionals to review the historical progression of the patient's condition and treatment plan if needed.</p><p>By leveraging the iterative note improvement process, healthcare providers can ensure that the SOAP and BIRP notes remain accurate, up-to-date, and reflective of the patient's evolving needs, ultimately enhancing the quality of care and facilitating better clinical decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CHALLENGES AND FURTHER RESEARCH</head><p>While the integration of generative AI in clinical documentation offers numerous benefits, it also presents several challenges that must be addressed to ensure the responsible and effective deployment of this technology in healthcare settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Quality and Representation</head><p>The performance of generative AI models is heavily dependent on the quality and representativeness of the data used for training. In the context of clinical notes generation, the models must be trained on a diverse and comprehensive dataset encompassing a wide range of medical conditions, patient demographics, and clinical scenarios. Failure to do so can lead to biases and inaccuracies in the generated notes, potentially compromising patient care. Moreover, the healthcare domain is characterized by complex medical terminology, abbreviations, and contextspecific language, which can pose challenges for language models to accurately understand and generate relevant content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Privacy and Security Concerns</head><p>Ensuring the privacy and security of patient data is a critical consideration when leveraging generative AI for clinical documentation. Healthcare organizations must implement robust data protection measures to prevent unauthorized access, data breaches, or unintentional disclosure of sensitive patient information.</p><p>Additionally, the generated notes themselves must be carefully scrutinized to ensure that they do not inadvertently include any personally identifiable information (PII) or protected health information (PHI), which could violate privacy regulations and breach patient confidentiality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Interpretability and Transparency</head><p>While generative AI models can produce human-like text, their decision-making processes and reasoning are often opaque, making it challenging to understand and interpret the rationale behind the generated content. In the context of clinical notes, it is crucial for healthcare professionals to understand the basis for the model's assessments, diagnoses, and treatment recommendations to ensure appropriate patient care and mitigate potential errors or biases.</p><p>Efforts must be made to enhance model interpretability and transparency, such as through the development of explainable AI (XAI) techniques or the incorporation of domain-specific knowledge and reasoning capabilities into the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Reliability and Robustness</head><p>The reliability and robustness of generative AI models in generating accurate and consistent clinical notes is a significant challenge. These models may exhibit hallucinations or generate factually incorrect information, which can have severe consequences in healthcare settings.</p><p>Rigorous testing and validation processes must be implemented to assess the models' performance across a diverse range of scenarios, including edge cases and rare medical conditions. Additionally, mechanisms for detecting and mitigating potential errors or inconsistencies in the generated notes should be developed to ensure patient safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Regulatory Compliance and Liability</head><p>The use of generative AI in clinical documentation must comply with relevant healthcare regulations and guidelines, such as those related to patient privacy, data protection, and medical record-keeping. Failure to adhere to these regulations can result in legal and financial consequences for healthcare organizations. Furthermore, there are potential liability concerns surrounding the use of AI-generated clinical notes. Determining accountability and responsibility in cases where errors or inaccuracies in the generated notes lead to adverse patient outcomes is a complex legal and ethical issue that requires careful consideration and the development of appropriate risk management strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Human Oversight and Validation</head><p>While generative AI can streamline the clinical documentation process, it is crucial to maintain human oversight and validation. Healthcare professionals must review and verify the accuracy and completeness of the generated notes, ensuring that they align with their clinical judgment and observations. This human-in-the-loop approach not only enhances patient safety but also facilitates the continuous improvement of the generative AI models through feedback and corrections provided by domain experts.</p><p>Addressing these challenges requires a collaborative effort involving technology developers, healthcare professionals, regulatory bodies, and policymakers. By fostering open dialogue, conducting rigorous research, and implementing appropriate safeguards and best practices, the responsible and effective deployment of generative AI in clinical documentation can be achieved, ultimately contributing to improved patient care and healthcare outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>The integration of generative AI in clinical documentation presents a transformative opportunity to streamline the documentation process, alleviate administrative burdens on healthcare professionals, and enhance the overall quality and efficiency of patient care. By leveraging natural language processing, automatic speech recognition, and advanced prompting techniques, generative AI models can transcribe patient-clinician interactions and generate draft clinical notes in structured formats such as SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan).</p><p>The case study presented in this paper demonstrates the feasibility and potential benefits of this approach, highlighting the time savings and improved documentation quality achieved through the use of generative AI models. The iterative note improvement process, which incorporates data from subsequent patient encounters, further enhances the accuracy and comprehensiveness of the generated notes, ensuring that they remain up-to-date and reflective of the patient's evolving condition and treatment plan. However, the responsible and effective deployment of generative AI in healthcare settings requires addressing several challenges, including data quality and representation, privacy and security concerns, model interpretability and transparency, model reliability and robustness, regulatory compliance and liability considerations, and the need for human oversight and validation.</p><p>Addressing these challenges necessitates a collaborative effort involving technology developers, healthcare professionals, regulatory bodies, and policymakers. By fostering open dialogue, conducting rigorous research, and implementing appropriate safeguards and best practices, the potential benefits of generative AI in clinical documentation can be realized while mitigating potential risks and ensuring patient safety and privacy.</p><p>Furthermore, the integration of generative AI in clinical documentation is just the beginning of a broader transformation in healthcare. As this technology continues to evolve and mature, it holds the potential to revolutionize various aspects of healthcare delivery, from personalized treatment planning and decision support to drug discovery and clinical trial design. By embracing the power of generative AI while prioritizing ethical and responsible practices, the healthcare industry can unlock new frontiers in patient care, driving improvements in clinical outcomes, operational efficiency, and overall healthcare quality.</p><p>Ultimately, the success of generative AI in healthcare will depend on a delicate balance between technological innovation and an unwavering commitment to patientcentered care, ethical principles, and regulatory compliance. By striking this balance, generative AI can become a catalyst for positive change, transforming the way healthcare is delivered and experienced, and ultimately improving the lives of patients worldwide.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1 :</head><label>1</label><figDesc>Fig 1: Audio Transcription with ASR and Transcript Diarization</figDesc><graphic coords="2,41.90,538.56,511.20,191.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2 :Fig 3 :</head><label>23</label><figDesc>Fig 2: Confusion Matrix of Utterance Classification using Whisper and Pyannotate</figDesc><graphic coords="4,37.73,78.60,237.45,165.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 4 :</head><label>4</label><figDesc>Fig 4: Basic and Advanced Prompting to Generate Structured Clinical Notes</figDesc><graphic coords="4,322.07,262.58,233.67,227.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Example prompt ÔÇ∑ Formatting instructions: {{ Detailed instructions on the SOAP/BIRP note structure and formatting }} ÔÇ∑ Example: {{ Transcript and corresponding wellstructured SOAP/BIRP note }} ÔÇ∑ Transcript: {{ Diarized transcript of patient-clinician interaction }} ÔÇ∑ Instructions: Based on the above examples and the provided transcript, please generate a SOAP/BIRP note following the specified structure and format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 5 :</head><label>5</label><figDesc>Fig 5: ROUGE-1 F1 Scores for Different Models Across SOAP note SamplesThe analysis reveals that GPT-4 consistently achieves superior performance, with ROUGE-1 F1 scores ranging from 0.90 to 0.95. This indicates its robustness and high accuracy across different summary complexities. In contrast, Claude and Llama exhibit similar performance levels, with</figDesc><graphic coords="7,40.40,412.79,514.23,225.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 7 :</head><label>7</label><figDesc>Fig 7: Iterative Clinical Notes Improvement by Augmenting Additional Patient Encounter Specific Data</figDesc><graphic coords="9,39.65,351.10,515.58,157.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,41.65,288.65,513.65,130.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,38.90,78.60,517.24,233.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Audio Transcript Generated with Whisper (Non-Diarized) vs. Diarized Audio Transcript with Utterance Classification SR</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Audio Transcript from Whisper Model Non-Diarized Transcript Diarized Transcript</head><label></label><figDesc></figDesc><table><row><cell>Hi, Eve. Good to see you again.</cell></row><row><cell>Hi.</cell></row><row><cell>Can I take a look at your scores?</cell></row><row><cell>Sure.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Volume 9, Issue 5, May -2024 International Journal of Innovative Science and Research Technology ISSN No:-<rs type="grantNumber">2456-2165</rs> <ref type="url" target="https://doi.org/10.38124/ijisrt/IJISRT24MAY1483">https://doi.org/10.38124/ijisrt/IJISRT24MAY1483</ref> IJISRT24MAY1483 www.ijisrt.com</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aqJgAdw">
					<idno type="grant-number">2456-2165</idno>
				</org>
				<org type="funding" xml:id="_bY95PgC">
					<idno type="grant-number">IJISRT24MAY1483 IJISRT24MAY1483</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX SOAP NOTE FORMAT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SUBJECTIVE ÔÉò Presentation</head><p>ÔÇ∑ Chief Complaint: The client reported experiencing persistent anxiety, difficulty sleeping, and frequent headaches. Quote (Chief Complaint): "I can't seem to relax, and my head always hurts." ÔÇ∑ Impairments and Challenges: The client described struggles with concentration at work, decreased social interactions, and difficulty managing stress. Their anxiety appeared to contribute to sleep disturbances and chronic tension headaches. Quote (Impairments and Challenges): "I can't focus on anything, and I feel like I'm always on edge." ÔÉº Quote (Symptom): "I just go places where there's no one and sit there alone."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÔÉò Therapist Observations and Reflections:</head><p>The client displays cognitive distortions like mental filtering that focus on negative aspects of situations. Increased awareness of these patterns through thought tracking will be beneficial. The client requires support and encouragement to challenge avoidance behaviors. Negative automatic thoughts and cognitive distortions will also pose a challenge. ÔÇ∑ Therapist Observations And Reflections: Client is fused with their negative thoughts, might need to introduce defusion techniques. ÔÇ∑ Therapeutic Alliance: The client showed some resistance. She was hesitant to talk about certain things related to her anxiety. The therapist processed that with her.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PLAN</head><p>ÔÉò Follow-Up Actions and Plans:</p><p>ÔÇ∑ Homework: Complete thought records identifying automatic negative thoughts and labeling cognitive distortions. Engage in one social activity. ÔÇ∑ Plan For Future Session: Review thought records, continue cultivating motivation and self-efficacy, begin discussing behavioral activation steps. ÔÇ∑ Plans For Continued Treatment: Continue weekly therapy, consider psychiatric referral if lack of progress. ÔÇ∑ Coordination Of Care: No coordination of care indicated at this time.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Healthcare Staff Wellbeing, Burnout, and Patient Safety: A Systematic Review</title>
		<author>
			<persName><forename type="first">Louise</forename><forename type="middle">&amp;</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">&amp;</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">&amp;</forename><surname>Watt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><forename type="middle">&amp;</forename><surname>Tsipa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daryl</forename><surname>O'connor</surname></persName>
		</author>
		<idno type="DOI">11.e0159015.10.1371/journal.pone.0159015</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Burnout and patient safety perceptions among surgeons in the United Kingdom during the early phases of the coronavirus disease 2019 pandemic: A two-wave survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Al-Ghunaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Biyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yiasemidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'connor</forename><surname>Db</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scottish Medical Journal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Healthcare Staff Wellbeing, Burnout, and Patient Safety: A Systematic Review</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Watt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsipa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'connor</forename><surname>Db</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0159015</idno>
		<idno type="PMID">27391946</idno>
		<idno type="PMCID">PMC4938539</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">159015</biblScope>
			<date type="published" when="2016-07-08">2016 Jul 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The time needed for clinical documentation versus direct patient care. A work-sampling analysis of physicians&apos; activities</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ammenwerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Sp√∂tl</surname></persName>
		</author>
		<idno type="PMID">19151888</idno>
	</analytic>
	<monogr>
		<title level="j">Methods Inf Med</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="91" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Quality, Accuracy and Reproducibility of Publicly-Available ChatGPT-4-Generated Documentation For Generation of Medical Notes</title>
		<author>
			<persName><forename type="first">Annessa</forename><forename type="middle">&amp;</forename><surname>Kernberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">&amp;</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><surname>Vishnu</surname></persName>
		</author>
		<idno type="DOI">10.2196/preprints.54419</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparing Scribed and Non-scribed Outpatient Progress Notes</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">&amp;</forename><surname>Rule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">&amp;</forename><surname>Florig</surname></persName>
		</author>
		<author>
			<persName><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName><surname>Steven &amp; Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">&amp;</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Hribar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA ... Annual Symposium proceedings. AMIA Symposium</title>
		<imprint>
			<date type="published" when="2021">2022. 2021</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Robust Speech Recognition via Large-Scale Weak Supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><forename type="middle">&amp;</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">&amp;</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><forename type="middle">&amp;</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">&amp;</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">&amp;</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m">End-to-end speaker segmentation for overlap-aware resegmentation</title>
		<editor>
			<persName><surname>Bredin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Bredin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">LMDX: Language Model-based Document Information Extraction and Localization</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramya Sree</forename><surname>Boppana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<idno>ArXiv abs/2309.10952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">PromptChainer: Chaining Large Language Model Prompts through Visual Programming</title>
		<author>
			<persName><forename type="first">Tongshuang</forename><forename type="middle">&amp;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">&amp;</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">&amp;</forename><surname>Donsbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">&amp;</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandra</forename><forename type="middle">&amp;</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">&amp;</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">1-10.10.1145/3491101.3519729</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boosted Prompt Ensembles for Large Language Models</title>
		<author>
			<persName><forename type="first">Silviu</forename><forename type="middle">&amp;</forename><surname>Pitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">&amp;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">&amp;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Measuring Massive Multitask Language Understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">&amp;</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><forename type="middle">&amp;</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">&amp;</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">&amp;</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Mantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">&amp;</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The NarrativeQA Reading Comprehension Challenge</title>
		<author>
			<persName><forename type="first">Tom√°≈°</forename><forename type="middle">&amp;</forename><surname>Koƒçisk√Ω</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">&amp;</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><forename type="middle">&amp;</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">&amp;</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">&amp;</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>G√°bor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">6.10.1162/tacl_a_00023</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams</title>
		<author>
			<persName><forename type="first">Di</forename><forename type="middle">&amp;</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><forename type="middle">&amp;</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Nassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung &amp;</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Hanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="DOI">11.6421.10.3390/app11146421</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Holistic Evaluation of Language Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cosgrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>R√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Acosta-Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koreeda</surname></persName>
		</author>
		<idno>ArXiv abs/2211.09110</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop: Text Summarization Braches Out</title>
		<meeting>the ACL Workshop: Text Summarization Braches Out</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Media Insights Engine for Advanced Media Analysis: A Case Study of a Computer Vision Innovation for Pet Health Diagnosis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Health Care Analytics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Artificial intelligence and participatory leadership: The role of technological transformation in business management and its impact on employee participation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sarioguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Miser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Research Journal</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
