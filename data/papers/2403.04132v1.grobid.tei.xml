<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</title>
				<funder>
					<orgName type="full">HuggingFace</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder>
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder>
					<orgName type="full">Samsung SDS</orgName>
				</funder>
				<funder>
					<orgName type="full">Kaggle</orgName>
				</funder>
				<funder>
					<orgName type="full">Anyscale</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder>
					<orgName type="full">Together AI</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft</orgName>
				</funder>
				<funder>
					<orgName type="full">VMware</orgName>
				</funder>
				<funder>
					<orgName type="full">AMD</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder ref="#_9HzYjn6">
					<orgName type="full">MBZUAI</orgName>
				</funder>
				<funder>
					<orgName type="full">Uber</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-07">7 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
							<email>&lt;weichiang@berkeley.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Stanford</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anastasios</forename><forename type="middle">N</forename><surname>Angelopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianle</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Banghua</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UCSD</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-07">7 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">55FBF55FB5B862B541170B74F3003A48</idno>
					<idno type="arXiv">arXiv:2403.04132v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-24T14:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Codeforces Weekly Contests MMLU, HellaSwag</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advancements in large language models (LLMs) have significantly expanded their capabilities beyond traditional natural language processing boundaries, addressing a broad array of general tasks <ref type="bibr">(OpenAI, 2023;</ref><ref type="bibr" target="#b15">Gemini et al., 2023;</ref><ref type="bibr" target="#b42">Touvron et al., 2023)</ref>. These developments underscore the potential of LLMs but also have raised concerns with respect to performance evaluation. Current benchmarks often fail to capture the nuanced and diverse aspects of these models, particularly in assessing their alignment with human Figure <ref type="figure">1</ref>. Classification of LLM benchmarks: We categorize along two dimensions: whether the questions are from a static dataset or a live, fresh source, and whether the evaluation metric relies on ground truth or (approximated) human preferences. MMLU <ref type="bibr" target="#b18">(Hendrycks et al., 2020)</ref>, HellaSwag <ref type="bibr" target="#b48">(Zellers et al., 2019)</ref>, GSM-8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref>, MT-Bench <ref type="bibr">(Zheng et al., 2023b)</ref>, and AlpacaEval <ref type="bibr" target="#b29">(Li et al., 2023)</ref> are common examples of static benchmarks. Chatbot Arena is the platform introduced in this paper.</p><p>preferences in real-world, open-ended tasks.</p><p>To assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors: the source of questions (either static or live) and the evaluation metric (either ground truth or human preference). According to these factors, benchmarks can be classified into four categories, as shown in Figure <ref type="figure">1</ref>. While a range of benchmarks is beneficial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly because such evaluations are inexpensive and reproducible.</p><p>However, these static, ground-truth-based benchmarks exhibit several limitations. Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings <ref type="bibr">(Zheng et al., 2023b)</ref>. Secondly, the test sets in these benchmarks are static, meaning they can become contaminated over time, which undermines the reliability of the evaluation results <ref type="bibr" target="#b47">(Yang et al., 2023)</ref>. Furthermore, for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable. Consequently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage.</p><p>Creating such a benchmark platform entails significant challenges. It requires the collection of live, fresh, and diverse user questions to accurately represent real-world scenarios.</p><p>Additionally, developing scalable, incremental, and efficient ranking systems is essential for evaluating a large number of models. Moreover, ensuring the quality of human evaluations is crucial given the noisy nature of human preferences.</p><p>To this end, we introduce Chatbot Arena, a benchmarking platform for LLMs that features anonymous, randomized battles in a crowdsourced setting. Chatbot Arena is a free website open to all users.<ref type="foot" target="#foot_0">foot_0</ref> On this website, a user can ask a question and get answers from two anonymous LLMs. Afterward, the user casts a vote for the model that delivers the preferred response, with the models' identities revealed only after voting. This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications. Armed with this data, we employ a suite of powerful statistical techniques, ranging from the statistical model of <ref type="bibr" target="#b2">Bradley &amp; Terry (1952)</ref> to the E-values of <ref type="bibr" target="#b43">Vovk &amp; Wang (2021)</ref>, to estimate the ranking over models as reliably and sample-efficiently as possible. With these tools in hand, we have designed efficient sampling algorithms specifically to select model pairs in a way that accelerates the convergence of rankings while retaining statistical validity.</p><p>We conduct a thorough analysis of the collected data to ensure the credibility of our platform. We demonstrate that the user-generated questions are sufficiently diverse to encompass a wide range of LLM use cases and are sufficiently challenging to differentiate between models. Furthermore, we confirm that the crowd-sourced votes are highly consistent with expert evaluations.</p><p>We have been running our system since Apr 2023 and have received over 240K votes from about 90K users in over 100 different languages as of Jan 2024. To encourage user engagement, we have made over 50 state-of-the-art models available for free. We also collaborate with leading model developers such as OpenAI, Google, Anthropic, Mistral, Hugging Face, and various universities, incorporating their latest models into our platform. We keep the community engaged by routinely updating the leaderboard, publishing analytical blogs, releasing datasets, and sharing information via tweets. Because of its unique and significant value, our leaderboard has emerged as one of the most referenced in the LLM field and has become a benchmark for the industry. We commit to making our data and code available, ensuring that this platform is open-source and open-accessible.</p><p>We make the following contributions:</p><p>• We build the first large-scale crowd-sourced live LLM evaluation platform with over 1M users visit.<ref type="foot" target="#foot_1">foot_1</ref> </p><p>• We conduct an in-depth analysis of the collected data, including prompt diversity, quality, vote quality, and insights on human feedback.</p><p>• We will publicly release a human preference dataset with over 100K pairwise votes collected from Chatbot Arena.</p><p>• We design an efficient sampling algorithm that actively chooses which model pairs to show, such that our sample efficiency improves, sometimes to a large degree.  <ref type="bibr" target="#b18">(Hendrycks et al., 2020)</ref>, Hel-laSwag <ref type="bibr" target="#b48">(Zellers et al., 2019)</ref>, GSM-8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref>, BigBench <ref type="bibr" target="#b39">(Srivastava et al., 2023)</ref>, AGIEval <ref type="bibr" target="#b51">(Zhong et al., 2023)</ref>, and HumanEval <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>. Benchmarks focusing on safety, such as ToxicChat <ref type="bibr" target="#b32">(Lin et al., 2023)</ref>, and comprehensive suites like HELM <ref type="bibr" target="#b31">(Liang et al., 2022)</ref>, also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk <ref type="bibr" target="#b24">(Karpinska et al., 2021;</ref><ref type="bibr" target="#b16">Geng et al., 2023;</ref><ref type="bibr" target="#b44">Wang et al., 2023)</ref>. The recent trend includes utilizing GPT-4 for approximating human judgment <ref type="bibr" target="#b7">(Chiang &amp; Lee, 2023)</ref>, with notable instances being MT-Bench <ref type="bibr">(Zheng et al., 2023b)</ref> and AlpacaEval <ref type="bibr" target="#b29">(Li et al., 2023)</ref>. In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces <ref type="bibr" target="#b30">(Li et al., 2022;</ref><ref type="bibr" target="#b20">Huang et al., 2023)</ref>. They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference <ref type="bibr" target="#b0">(Bai et al., 2022;</ref><ref type="bibr" target="#b36">Ouyang et al., 2022;</ref><ref type="bibr" target="#b42">Touvron et al., 2023)</ref>. However, these studies are typically limited to specific organizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.</p><p>Risks of Static Benchmarks. Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment <ref type="bibr" target="#b47">(Yang et al., 2023;</ref><ref type="bibr" target="#b35">Oren et al., 2023)</ref>. DynaBench <ref type="bibr" target="#b25">(Kiela et al., 2021)</ref> identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.</p><p>Ranking System. Ranking systems have been a wellstudied topic in statistics. Related topics include probability models <ref type="bibr" target="#b22">(Hunter, 2004;</ref><ref type="bibr" target="#b38">Rao &amp; Kupper, 1967)</ref>, rank elicitation <ref type="bibr">(Szörényi et al., 2015;</ref><ref type="bibr">Busa-Fekete et al., 2014a;</ref><ref type="bibr">b)</ref>, and online experiment design <ref type="bibr" target="#b6">(Chernoff, 1992;</ref><ref type="bibr" target="#b23">Karimi et al., 2021)</ref>. The Elo rating system has also been used for LLMs <ref type="bibr" target="#b0">(Bai et al., 2022;</ref><ref type="bibr" target="#b1">Boubdir et al., 2023)</ref>. Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.</p><p>Human Preference Dataset. Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include Ope-nAssistant <ref type="bibr" target="#b26">(Köpf et al., 2023)</ref>, HH-RLHF <ref type="bibr" target="#b0">(Bai et al., 2022)</ref>, LMSYS-Chat-1M <ref type="bibr">(Zheng et al., 2023a)</ref>, and synthetic approximations of human preferences like UltraFeedback <ref type="bibr" target="#b9">(Cui et al., 2023)</ref> and Nectar <ref type="bibr" target="#b52">(Zhu et al., 2023)</ref>. Our prior data release, LMSYS-Chat-1M <ref type="bibr">(Zheng et al., 2023a)</ref>, is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Human Preference Data Collection</head><p>In this section, we discuss our interface design to collect human preferences and present summary statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Interface</head><p>Chatbot Arena crowd-sources feedback from users for model evaluation. Our goal is to design an ease-of-use interface to reduce friction for users to contribute data. Since we collect feedback from many users, it is difficult to set a consistent grading rubric across different people. Hence, we adopt a pairwise comparison mechanism where users only need to compare two model responses and vote for the better one, instead of requiring users to provide an absolute score.</p><p>In each battle, two anonymous models are sampled. To encourage data diversity, we do not preset any input prompt on the website. Users are free to input any prompt to the two models. We believe this creates incentives for user engagement, particularly given that we offer a free service.</p><p>It also helps us collect a diverse set of inputs representing real-world usage. After models provide their answers, user compare them side-by-side and vote for the preferred answer. If a user cannot choose in the first turn, the user can continue chatting until identifying a winner. For those who are unsure, we also present two buttons, "tie" or "both are bad." Figure <ref type="figure" target="#fig_4">8</ref> shows a screenshot of our interface. Before using our service, users are required to accept terms of use, which gives us their consent to release the data publicly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Statistics</head><p>We began collecting data in April 2023. As of Jan 2024, we have received around 240K votes from over 90K users.</p><p>Our data involves more than 50 models, including both proprietary models like GPT-4, Claude, and Gemini, as well as open models such as LLaMA and Mistral. These conversations cover more than 100 languages, with 77% being in English, 5% in Chinese, and the remaining languages, such as Russian, German, Spanish, French, and Japanese, each representing less than 2% of the total. Each data point includes multi-turn conversations between the user and two LLMs, and a vote to indicate which model the user prefers.</p><p>We summarize statistics in Table <ref type="table" target="#tab_1">1</ref> along with other existing human preference datasets.</p><p>Figure <ref type="figure">10</ref> in the Appendix shows the vote count per model. On average, 8K votes are collected for each model. In Figure <ref type="figure" target="#fig_0">2</ref>, we select a set of representative models and present their win rate and the number of battles. Note that we employ non-uniform sampling to concentrate votes on model pairs that have similar performance due to higher uncertainty. This helps us reduce the number of votes required to reach stable results. We later develop an adaptive sampling method and demonstrate its effectiveness against random sampling. See Section 5 for further analysis.</p><p>To ensure anonymity, we use keywords to filter out conversations containing model identity such as model name (e.g., GPT, Claude) or companies (e.g., OpenAI, Anthropic).</p><p>To avoid misuse, we adopt OpenAI moderation API to flag conversations that contain unsafe content. The flagged user requests account for 3% of the total requests. Figure <ref type="figure">9</ref> in the Appendix shows the number of valid user votes over time, where we get 1-2K votes per day in recent months and spikes as we introduce new models or leaderboard updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">From Pairwise Comparisons to Rankings</head><p>Our data consists of pairwise comparisons-but how can we use these comparisons to recover a ranking over all M models? This is a well-studied topic in the literature on learning to rank <ref type="bibr" target="#b33">(Liu et al., 2009)</ref>, and we present our perspective here. We let A = {(m, m ′ ) : m &lt; m ′ and m, m ′ ∈ [M ]} denote our comparative data set.</p><p>We consider a sequential setting, where at time t ∈ N, we serve the human a pair of models A t ∈ A (which we pick), and in turn we observe the human's response H t ∈ [0, 1].</p><p>As an example, we might have that A t = (1, 2) and H t = 1, indicating that the human prefers model 2 over model 1. In the ensuing text, we will primarily focus on the binary casewhere H t ∈ {0, 1}-but our approach will generalize to  any form of feedback, including the possibility of allowing the human to express different degrees of preference or to say the models are tied.</p><formula xml:id="formula_0">gpt-4-turbo gpt-4-0613 mistral-medium mixtral-8x7b-instruct-v0.1 gemini-pro-dev-api claude-2.1 gpt-3.5-turbo-0613 claude-instant-1 llama-2-70b-chat llama-2-13b-chat llama-2-7b-chat mistral-7b-instruct mistral-7b-instruct llama-2-7b-chat llama-2-13b-chat llama-2-70b-chat claude-instant</formula><p>One critical goal is to estimate the win matrix:</p><formula xml:id="formula_1">θ * (a) = E[H t | A t = a]</formula><p>, for all a ∈ A; see the left panel of Figure <ref type="figure" target="#fig_0">2</ref> for an illustration of the (empirical) win matrix. In the binary case, the a entry in the win matrix corresponds to the probability the human prefers model a 2 to a 1 when shown the pair a. Finding the win matrix is a relatively straightforward mean-estimation problem; we will provide details in Section 5.</p><p>Formally, consider a score s(P) ∈ R M , where P is a joint distribution over A and H (by default, we will target a uniform distribution over A). Each model has a true score s(P) m , and better models will have higher scores. In particular, we have the rank of model m:</p><formula xml:id="formula_2">rank(P) m = 1 + m ′ ∈[M ] 1 {s(P) m ′ &gt; s(P) m } . (1)</formula><p>The best model has rank 1. If there is another model tied for best, they will both get assigned rank 1.</p><p>Picking a score. A standard score function in this setting is the vector of Bradley-Terry (BT) coefficients <ref type="bibr" target="#b2">(Bradley &amp; Terry, 1952)</ref>. In the Bradley-Terry model, H t ∈ {0, 1}, and the probability model m beats model m ′ is modeled via a logistic relationship:</p><formula xml:id="formula_3">P(H t = 1) = 1 1 + e ξ m ′ -ξm , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where ξ is an M -length vector of so-called BT coefficients.</p><p>Without loss of generality, we take ξ 1 = 0 (since the model is invariant to addition in ξ). Our goal is to estimate the population Bradley-Terry coefficients, i.e., those that minimize the binary cross-entropy:</p><formula xml:id="formula_5">s(P) = argmin ξ E (A,H)∼P ℓ H, 1 1 + e ξ A 2 -ξ A 1 ,<label>(3)</label></formula><p>where ℓ is the binary cross-entropy loss,</p><formula xml:id="formula_6">ℓ(h, p) = -(h log(p) + (1 -h) log(1 -p)).</formula><p>Although the BT model technically assumes a parametric form for the model win rates, the seminal results of <ref type="bibr" target="#b21">Huber et al. (1967)</ref>; <ref type="bibr" target="#b46">White (1982)</ref> show that maximum likelihood estimators are still asymptotically normal even when these assumptions do not hold, so long as the so-called "sandwich" covariance matrix is used; see Section 5 for details, and see Appendix B for a nonparametric extension of the Bradley-Terry model. Finally, we remark that previous evolutions of our online interface have reported different ranking scores, such as the Elo score <ref type="bibr" target="#b12">(Elo, 1967)</ref> instead of the BT coefficients. We made this change because the BT coefficients are better for the purpose of statistical estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Efficient Approximate Ranking</head><p>In Section 4 we described how to calculate the win matrix, score, and rank. Now we describe our estimation procedures.</p><p>Win matrix estimation. Estimation of the win matrix is relatively straightforward. Define X t (a) = 1 Pt(a) H t 1 {A t = a}, where P t (a) is the probability of sampling pair a at time t, and X t as the according vector. Then the estimator is</p><formula xml:id="formula_7">θT = 1 T T t=1 X t .<label>(4)</label></formula><p>Note that E[X t (a)] = θ * (a) for all t, and thus θT is an unbiased estimator of θ * . We will furthermore estimate the covariance matrix as</p><formula xml:id="formula_8">Σ T = 1 T T t=1 (X t -θT )(X t -θT ) ⊤ . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>Under the appropriate regularity conditions, we have that</p><formula xml:id="formula_10">√ T Σ -1/2 ( θ -θ * ) → N (0, I d ),<label>(6)</label></formula><p>and we construct confidence intervals accordingly. For an understanding of the appropriate regularity conditions, see <ref type="bibr" target="#b11">Durrett (2019)</ref>, Theorem 8.2.8, where condition (ii) is trivially satisfied so long as P t (a) &gt; ϵ &gt; 0, and condition (i) is implied by the almost-sure convergence of P t (a) to a limiting distribution P (a).</p><p>Estimating the BT scores. To estimate the BT coefficients, mirroring (3), we perform (reweighted) maximum likelihood estimation on our data points:</p><formula xml:id="formula_11">s( P) = argmin ξ T t=1 1 P (A t ) ℓ H t , 1 1 + e ξ A t,2 -ξ A t,1 ,<label>(7)</label></formula><p>where A t ∼ P . We perform the inverse weighting by P (A t ) because this allows us to target a score with a uniform distribution over A.</p><p>To compute confidence intervals on the BT coefficients, we employ two strategies: (1) the pivot bootstrap <ref type="bibr" target="#b10">(DiCiccio &amp; Efron, 1996)</ref>, and (2) the "sandwich" robust standard errors outlined in <ref type="bibr" target="#b21">Huber et al. (1967)</ref> (see also <ref type="bibr" target="#b14">Freedman (2006)</ref> for an outline of the necessary technical assumptions). Ultimately, based on the results of a simulation study described in Appendix A, we choose to deploy the sandwich intervals due to their smaller size in large samples.</p><p>Approximate rankings. Finally, we report an approximate ranking for each model that accounts for the uncertainty in the estimation of the score. Given an M -dimensional confidence set C satisfying</p><formula xml:id="formula_12">P(s(P) ∈ C) ≥ 1 -α,<label>(8)</label></formula><p>we extract an approximate ranking</p><formula xml:id="formula_13">R m = 1 + m ′ ∈[M ] 1 {inf C m ′ &gt; sup C m }.</formula><p>The uniform validity of C directly implies that P(∃m : R m &gt; rank(P) m ) ≤ αi.e., with high probability, no model's performance is understated. A guarantee on the other side-that no model's performance is overstated-is possible by interchanging the inf and sup. To get the uniform confidence set, we construct the chi-squared interval implied by the central limit theorem using the sandwich estimate of the variance. In other words, we construct the interval {ξ</p><formula xml:id="formula_14">: T V -1/2 ( ξ -ξ) ≤ χ 2 1-α,M -1</formula><p>, where ξ is our MLE of the BT coefficients and Vξ is the sandwich variance of the logistic regression.</p><p>Active sampling rule. Our sampling rule was to choose the model pair a ∈ A proportionally to the reduction in confidence interval size by sampling that pair:</p><formula xml:id="formula_15">P t (a) ∝ Σt,a,a |{t : A t = a}| - Σt,a,a |{t : A t = a}| + 1 . (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Detecting Anomalous Users</head><p>On a different note, we take a first step towards identifying anomalous IP addresses in our dataset. In a dataset of U unique IPs, we let IP = {1, . . . , U } be the set of all IP addresses. Consider a "test" user, outside this database, who gives ratings H ′ 1 , . . . , H ′ n when presented actions A ′ 1 , . . . , A ′ n . The idea of our procedure is to compare the distribution of ratings for the new user to the historical distribution of ratings for a given action. We let H a = {H t : A t = a} and every time a user submits a vote, we calculate the following number:</p><formula xml:id="formula_16">p i = 1 |H A ′ i | + 1   1 + h∈H A ′ i 1 {h ≥ H ′ i }    . (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>Under the null hypothesis that H A ′ i is exchangeable with H ′ i , p i is a valid p-value (see Appendix C for a proof). Furthermore, the dependence of these p-values asymptotically is negligible.</p><p>With this p-value in hand, we can test against this null hypothesis sequentially by using Fisher's combination test <ref type="bibr" target="#b13">(Fisher, 1928)</ref> along with a variant of the Bonferroni correction. In particular, for each user, after their jth vote, we compute M j = -2 j i=1 log(p i ). At 5 randomly chosen values of j between 1 and 100, we identify a user as anomalous if M j ≥ χ 2 2j,1-α/5 . (The times are randomly chosen, as to avoid anomalous users strategizing to hack this p-value.) Despite the heuristic application of this procedure, it seems to work well in our small-scale tests reported in Table <ref type="table" target="#tab_5">5</ref>. Note that similarity is computed by cluster's centroid embeddings, hence diagonals are always one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Data Analysis</head><p>To examine whether Arena's crowdsourced data reflects real-world use cases, we conduct topic modeling on the user prompts. We show how effective are these prompts in distinguishing models. Lastly, we validate the vote quality by relabeling data with experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Topic Modeling on User Prompts</head><p>To study the prompt diversity, we build a topic modeling pipeline with BERTopic<ref type="foot" target="#foot_2">foot_2</ref>  <ref type="bibr" target="#b17">(Grootendorst, 2022)</ref>. We start with transforming user prompts into representation vectors using OpenAI's text embedding model (text-embedding-3small). To mitigate the curse of dimensionality for data clustering, we employ UMAP (Uniform Manifold Approximation and Projection) <ref type="bibr" target="#b34">(McInnes et al., 2020)</ref> to reduce the embedding dimension from 1,536 to 5. We then use the hierarchical density-based clustering algorithm, HDB-SCAN, to identify topic clusters with minimum cluster size 32. Finally, to obtain topic labels, we sample 10 prompts from each topic cluster and feed into GPT-4-Turbo for topic summarization.</p><p>The pipeline identifies 600 clusters covering a wide range of topics including poetry writing, coding, math, and medical queries. We present the top-16 topic clusters in Figure <ref type="figure" target="#fig_1">3</ref>. We observe that the largest cluster only accounts for 1% of the entire set and the rest quickly drop to &lt;0.5%, and the similarity between clusters is small, showing a long-tail and diverse distribution. Due to space limit, we present the similarity matrix and cluster hierarchy of top-64 clusters in Figure <ref type="figure">11</ref> and 12 in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Can Arena Prompts Distinguish Models?</head><p>Next, we study how effective are these topic clusters in distinguishing models strengths. Constructing challenging prompts has become increasingly difficult due to LLMs' fast growing capabilities. For example, open models such as Llama-2-70b-chat can likely answer inquiries about movie or travel recommendation as good as GPT-4, but not in other domains such as reasoning or coding. To demonstrate, we sample 30 prompts from seven topic clusters and compare the performance of Llama-2-70b-chat and GPT-4.</p><p>To control variables, we factor out user votes and consider LLM-as-judge <ref type="bibr">(Zheng et al., 2023b)</ref> to evaluate model response. Results are shown in Table <ref type="table" target="#tab_2">2</ref>, where we see GPT-4 has significantly higher win-rate (up to 97%) in clusters that require coding and reasoning skills. On the other hand, for clusters with less problem-solving tasks, GPT-4 win-rate drops to below 60%. We show examples in Appendix D.1. This result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models.</p><p>Building Challenging Benchmark. To further demonstrate the prompt quality, we show it is possible to construct a challenging benchmark with crowd-sourced user prompts. To ensure both topic coverage and quality, we first run the topic modeling pipeline and follow a similar procedure in <ref type="bibr">Zheng et al. (2023a)</ref> to select challenging questions sampled from each topic cluster. Examples prompts and evaluation procedures can be found in the Appendix D.2 and Appendix D.3, respectively. We observe the selected prompts are highly effective in differentiating models. In Figure <ref type="figure" target="#fig_2">4</ref>, we compare Arena bench against a widely used LLM benchmark, MT-Bench <ref type="bibr">(Zheng et al., 2023b)</ref>. We can see that Arena Bench effectively reveals a significant gap in performance between proprietary and the strongest open models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Validating Vote Quality</head><p>To assess the quality of crowdsourced votes, we randomly selected 160 battles between GPT-4-Turbo and Llama-2-13B, as well as GPT-4-Turbo and GPT-3.5-Turbo-0613. We then asked experts <ref type="foot" target="#foot_3">4</ref> to label their preference per comparison. The experts were given the prompts and answers blindly, and asked to carefully fact-check model's answer with external resources like search engine. Manually labeling each  data point took on average 3-5 minutes. For reference, we also use GPT-4 as a judge for pairwise comparisons. The agreement rate between crowd-users, experts, and GPT-4judge are presented in Table <ref type="table" target="#tab_3">3</ref>. The corresponsing win-rate are shown in Table <ref type="table" target="#tab_4">4</ref>.</p><p>To summarize, we observe high agreement rates (72% to 83%) between Arena crowd-user and experts in both setup. Note that agreement rates between two experts are around similar levels (79.4% and 89.8%). As for the 10%-20% disagreement between experts, it is mostly due to some user prompts don't have a ground truth answer. Depending on the preference of the evaluator, sometimes both answers can be argued as being better than the other one, such as the examples in Appendix D.4. The gap between crowdvs-expert agreement rate and expert-vs-expert agreement rate (5%-10%) is mostly attributed to crowd user making mistakes or overlooking factual errors in model's response.</p><p>Overall, the agreement rates presented in Table <ref type="table" target="#tab_3">3</ref> validate the decent quality of crowd-sourced votes in Chatbot Arena. 0.0 0.5 1.0 1.5 2.0 2.5</p><p>zephyr-7b-beta (#19-37) wizardlm-13b (#19-37) llama2-70b-steerlm-chat (#17-37) solar-10.7b-instruct-v1.0 (#18-36) dolphin-2.2.1-mistral-7b (#17-37) pplx-70b-online (#17-31) gpt-3.5-turbo-1106 (#15-29) openchat-3.5 (#14-29) llama-2-70b-chat (#14-28) openhermes-2. <ref type="bibr">5-mistral-7b (#12-28)</ref> vicuna-33b (#7-25) starling-lm-7b-alpha (#7-26) gpt-3.5-turbo-0314</p><formula xml:id="formula_18">(#7-22) wizardlm-70b (#7-22) tulu-2-dpo-70b (#6-21) yi-34b-chat (#6-19) claude-instant-1 (#6-19)</formula><p>gemini-pro (#6-18) gpt-3.5-turbo-0613 (#6-18) claude-2.1 (#6-18) mixtral-8x7b-instruct-v0.1 <ref type="bibr">(#4-18)</ref> gemini-pro-dev-api (#3-18) claude-2.0 (#3-14) claude-1 (#3-8) mistral-medium <ref type="bibr">(#3-8)</ref> gpt-4-0613 (#3-7) gpt-4-0314 (#2) gpt-4-turbo (#1) corrected uncorrected <ref type="bibr">Figure 5</ref>. Intervals for the BT coefficients with and without multiplicity correction. The multiplicity correction, in this case a chi-square CLT interval, is technically required for the purpose of calculating the ranking, because it ensures all scores are simultaneously contained in their intervals (and the ranking is a function of all the scores). However, it induces extra conservatism, so we report both intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Ranking system</head><p>Computing the rank on real data. In this section, we report results from our experiments on approximate ranking. For this experiment, we ran a replay of T = 213, 576 historical votes from our online platform and calculate the BT coefficients using our earlier-described estimation algorithm with confidence intervals; see Figure <ref type="figure">5</ref> for these intervals (with and without multiplicity correction; the formal notion of approximate ranking technically requires multiplicity correction, but it makes the intervals looser).</p><p>Evaluating the coverage of the intervals. A natural followup question is whether or not the intervals are doing their job correctly: whether they cover the true BT coefficients with probability at least (and almost exactly) 1 -α. Of course, this cannot be evaluated on real data, so we run a simulation. A vector of BT coefficients is drawn, with each coordinate sampled i.i.d. from a distribution beta(1/γ, 1/γ); we take γ = 2 in Figure <ref type="figure" target="#fig_3">6</ref> (and we vary γ in Appendix A). Given these coefficients, a dataset is synthesized, and the coverage and average width are computed for each of 20 trials. The results can be seen in Figure <ref type="figure" target="#fig_3">6</ref> for the uncorrected intervals The coverage of the intervals behaves as expected, centering around 1 -α, regardless of the number of models. Meanwhile, the more models are included, the larger the intervals become.</p><p>Evaluating the active sampling rule. Next, we discuss the evaluation of our active sampling rule as Equation ( <ref type="formula">9</ref>) for win matrix estimation. We evaluate this sampling rule by taking the best fit BT coefficients to our 213,576 point sized holdout set, and then sampling from that distribution using our active sampling algorithm. The results are displayed in Figure <ref type="figure">7</ref>. It is hard to tell by looking at plots, but the improvement is substantial: To estimate θ * to a precision of 0.2, random needs 6,800 samples and adaptive needs 4,400 samples; meanwhile to estimate the score to a precision of 0.3, random needs 17,200 samples and adaptive needs 16,400 samples. Thus, the random baseline requires 54% and 5% more data to achieve the same level of precision, respectively. One can see from the plots in Figure <ref type="figure">7</ref> that these results are not cherry-picked: the sample-efficiency of our method is better at all values on the horizontal axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Anomalous Users Detection</head><p>We evaluate the outlier detection method in Section 5.1. We construct the evaluation set by manually identifying 25 anomalous users whose inputs are highly repetitive or meaningless (e.g., asking "hi" for 100 times or inputting garbled texts). We randomly sample 25 normal users with 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 average width ( ) at least 50 votes, and inspect their input prompts to ensure no abnormal behaviors. As mentioned in Section 5.1, per user we compute five M j and identify the user as anomalous if M j ≥ χ 2 2j,1-α/5 . We present results of two different α (i.e., the significance leval) in Table <ref type="table" target="#tab_5">5</ref>. We find the detection method effective (e.g., reaching 90% true positive and 60-70% true negative rate). We inspect the false negative errors and find those are from users do not always behave abnormally, making them harder to detect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>Limitations. Although our user base is extensive, we anticipate that it will primarily consist of LLM hobbyists and researchers who are eager to experiment with and evaluate the latest LLMs. This inclination may result in a biased distribution of users. Additionally, despite the wide array of topics encompassed by the prompts discussed in previous sections, the data predominantly comes from our online chat interface. This source might not accurately reflect the real-world usage of LLMs in production environments or specialized domains, potentially leading to a skewed prompt distribution. Moreover, our study concentrates on assessing the helpfulness of LLMs but overlooks their safety aspects. We recognize the possibility and necessity of a parallel mechanism to evaluate the safety of these models.</p><p>Future Directions. In our future work, we plan to develop comprehensive topic leaderboards and establish a dedicated section for multimodal and agent-based LLMs in more dynamic, gamified settings, catering to more complex tasks. We also believe our approach to detecting harmful users could be improved and made more formally rigorous by using the theory of nonnegative supermartingales and Evalues <ref type="bibr" target="#b19">(Howard et al., 2020;</ref><ref type="bibr" target="#b45">Waudby-Smith &amp; Ramdas, 2020;</ref><ref type="bibr" target="#b43">Vovk &amp; Wang, 2021;</ref><ref type="bibr" target="#b37">Ramdas et al., 2023)</ref>; this would deal with the dependence, but the variants we tried did not perform well in terms of power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this paper, we present Chatbot Arena, an open platform for evaluating LLMs through crowdsourced, pairwise human preferences. We conduct an in-depth analysis of the crowdsourced user prompts and preference votes to validate the diversity and quality. We develop an efficient model sampling and ranking algorithm. Our dataset including 100K pairwise preference votes will be released for future research.   The sandwich intervals, though larger in small samples, are more stable, and in large samples, they actually become smaller. We use the multiplicity corrected version of both intervals, so they both have a coverage of 1. (Coverage here is calculated with respect to the BT coefficient solution on the full dataset, so it is not as meaningful as in the simulation plot below.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Confidence Interval Simulation Study</head><p>We conduct a simulation study to evaluate the bootstrap confidence intervals versus the sandwich estimator. To a large extent, both intervals are the same-indeed, their intervals are often identical to the naked eye. Nonetheless, in our experiments, there are some differences. First, in Figure <ref type="figure" target="#fig_8">13</ref>, we conduct a replay study using the same 213576 data points mentioned in the main text.</p><p>We also do a suite of experiments in simulation using the same beta generating process as in the main text, with γ = 2.</p><p>The result is shown in Figure <ref type="figure" target="#fig_6">14</ref>; results are similar across many choices of the parameter γ and the model strength, which indicates that both intervals will have good coverage and width in the practical conditions we would expose them to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Nonparametric Bradley-Terry Model</head><p>Nonparametric Bradley-Terry. We next consider a nonparametric extension of the Bradley-Terry (BT) model <ref type="bibr" target="#b2">(Bradley &amp; Terry, 1952)</ref> to the case where the ranking is not necessarily transitive. Let G(m) denote the set of all paths to the model m, i.e.,</p><formula xml:id="formula_19">G(m) = g ∈ B M -1 : g i,1 ̸ = g j,1 , ∀i ̸ = j, and g M -1,2 = m ,<label>(11)</label></formula><p>where B = A ∪ {(a 2 , a 1 ) : a ∈ A}. Each element of G(m) is a chain of model pairings that leads to m; for example, if m = 5 and M = 6, one element of G(m) is ((1, 2), (2, 4), <ref type="bibr" target="#b54">(4,</ref><ref type="bibr" target="#b53">3)</ref>, <ref type="bibr" target="#b53">(3,</ref><ref type="bibr">6)</ref>, (6, 5)). Our score function is given by the average path-sum of the log odds of the second model winning, over the entirety of G(m):</p><formula xml:id="formula_20">s(θ) m = 1 |G(m)| g∈G(m) log θ ′ ((1, g 1,1 )) 1 -θ ′ ((1, g 1,1 )) + a∈g log θ ′ (a) 1 -θ ′ (a) ,<label>(12)</label></formula><p>where θ ′ (a) = θ(a)1 {a ∈ A} + (1 -θ((a 2 , a 1 )))1 {a / ∈ A}, with the convention that θ((m, m)) = 1/2 for all m. Note that for any g ∈ G(m) where a ∈ g and m / ∈ a, we also have some g ′ ∈ G(m) such that (a 2 , a 1 ) ∈ g. Meanwhile, if a ∈ g and m ∈ a, then a = (m ′ , m) for some m ′ . Thus, we can compute</p><formula xml:id="formula_21">s(θ) m = a∈A m / ∈a 1 2 log θ ′ (a) 1 -θ ′ (a) + log θ ′ ((a 2 , a 1 )) 1 -θ ′ ((a 2 , a 1 )) + m ′ ∈[M ]\{m} log θ ′ ((m ′ , m)) 1 -θ ′ ((m ′ , m)) + θ ′ ((1, m ′ )) 1 -θ ′ ((1, m ′ )) (13) = a∈A m / ∈a 1 2 log θ(a) 1 -θ(a) + log 1 -θ(a) θ(a) + m ′ ∈[M ]\{m} log θ ′ ((m ′ , m)) 1 -θ ′ ((m ′ , m)) + θ ′ ((1, m ′ )) 1 -θ ′ ((1, m ′ )) (14) = m ′ ∈[M ]\{m} log θ ′ ((m ′ , m)) 1 -θ ′ ((m ′ , m)) + θ ′ ((1, m ′ )) 1 -θ ′ ((1, m ′ )) (15) = m ′ ∈[M ]\{m} (1 -21 {m ′ &gt; m}) log θ((m ′ , m)) 1 -θ((m ′ , m)) + θ((1, m ′ )) 1 -θ((1, m ′ )) . (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>This score is always well-defined, and is a simple, smooth function of θ. Its derivative is, for all a ∈ A, </p><formula xml:id="formula_23">∂ ∂θ(a) s(θ) m = 1 {a 2 = m} (1 -21 {a 1 &gt; m}) 1 θ(a)(1 -θ(a)) + 1 {a 1 = 1, a 2 ̸ = m} 1 θ(a)(1 -θ(a)) .<label>(17)</label></formula><p>for some unknown parameters ξ 1 , . . . , ξ M -the Bradley-Terry coefficients. The basic goal of the Bradley-Terry model is to estimate these parameters from the observed outcomes. In our setting, however, we use the outcomes to get a CLT on θ, and then can immediately recover the coefficients. Taking without loss of generality ξ 1 = 0, we have that</p><formula xml:id="formula_25">log θ((1, m ′ )) 1 -θ((1, m ′ )) + log θ((m ′ , m)) 1 -θ((m ′ , m)) = log θ((1, m ′ )) θ((m ′ , 1)) + log θ((m ′ , m)) θ((m, m ′ ))<label>(19)</label></formula><p>= log e ξ m ′ (e ξ m ′ + 1) e ξ m ′ + 1 + log e ξm (e ξ m ′ + e ξm ) e ξ m ′ (e ξ m ′ + e ξm ) (20)</p><formula xml:id="formula_26">= ξ m ′ + ξ m -ξ m ′ = ξ m (21)</formula><p>Thus, all the sums over paths in ( <ref type="formula" target="#formula_20">12</ref>) are equal to ξ m -ξ g1,1 .</p><formula xml:id="formula_27">log θ ′ ((1, g 1,1 )) 1 -θ ′ ((1, g 1,1 )) + a∈g log θ ′ (a) 1 -θ ′ (a) (22) =ξ g1,1 + ξ g1,2 -ξ g1,1 + ξ g2,2 -ξ g2,1 + • • • + ξ g M -1,2 -ξ g M -1,1 (23) =ξ g M -1,2 = ξ m .<label>(24)</label></formula><p>Thus, if the parametric BT model is well-specified, the nonparametric version will exactly recover the Bradley-Terry coefficients. However, our nonparametric analogue of the BT model has major advantages over the original: it will retain statistical validity even if H t is not binary, if the win rate is non-transitive, and if the logistic model assumed by the BT model is misspecified. In practice, the nonparametric BT coefficient can be easily computed by ( <ref type="formula" target="#formula_21">16</ref>).</p><p>Step 6: Testing and refining the game * Test the game thoroughly to ensure that it works as expected, with no bugs or errors. * Refine the game mechanics and graphics as needed, based on player feedback or personal preference. * Consider adding additional features, such as keeping track of the player's score or implementing a time limit for each guess.</p><p>Step Example 2: An example user prompt from the cluster "Python Game Programming Challenge". This prompt distinctively separates GPT-4-0613 performance from Llama-2-70b-chat. Due to limited space, the code from each assistant's response is omitted.</p><p>&lt;|User Prompt|&gt; Write a python function that prints a christmas tree in the form of a triangle of asterisks, where the only input is size.  France and it's a great place to buy local products. 9. Enjoy a meal at a local restaurant: Lille is known for its delicious cuisine, so make sure to try some local dishes. 10. Visit the Lille Zoo: This is a small but well-kept zoo that houses about 70 different species. 11. Take a romantic walk through the Jardin des Plantes de Lille: This botanical garden is a peaceful place to enjoy nature. 12. Visit the Musee de l'Hospice Comtesse: This museum is located in a former hospital and it's home to a collection of art and artifacts from the Middle Ages. 13. Enjoy a performance at the Lille Opera: Check the schedule and book tickets for a romantic night out. 14. Take a day trip to the nearby city of Bruges: This charming Belgian city is just a short train ride away. 15. Visit the Le Palais Rihour: This palace houses the Lille Tourist Office and it's also home to the Salle du Conclave, which has a beautiful Gothic fireplace. Remember to check the opening times and any COVID-19 restrictions before you visit. Enjoy your trip! &lt;|Llama-2-70b-chat's Answer|&gt; Sure! Lille is a beautiful city in northern France that offers a variety of activities for couples to enjoy. Here are some ideas for things you could do with your girlfriend in Lille: 1. Explore the city's charming streets and historic landmarks, such as the Grand Place, the Vieille Bourse, and the Cathedral of Lille. 2. Visit one of the many museums, such as the Palais des Beaux-Arts de Lille , the Musee d'Art Moderne, d'Art Contemporain et d'Art Brut, or the Musee de l'Hospice Comtesse. 3. Take a stroll along the canal and enjoy the views of the city from the waterfront. 4. Visit the famous Lille Christmas market, which takes place every year in December and features festive stalls, food, and drinks. 5. Go shopping at the city's many boutiques and galleries, or at the large shopping centers such as the Euralille or the Lille Europe. 6. Enjoy a romantic dinner at one of Lille's many excellent restaurants, such as the Michelin-starred Le Comptoir du Relais or the cozy Le Bistrot d'Alsace. 7. Take a day trip to the nearby city of Bruges, Belgium, which is known for its picturesque canals and medieval architecture. 8. Go to a show or concert at the Lille Opera or the Le Zenith concert hall. 9. Visit the nearby town of Roubaix, which is famous for its textile industry and its beautiful Art Deco architecture. 10. Take a walk in the beautiful Parc de la Citadelle, which offers a peaceful escape from the city and features a pond, a botanical garden, and a zoo. 11. Go on a bike ride or a hike in the surrounding countryside, such as the Parc Naturel Regional Nord-Pas de Calais. 12. Visit the famous Lille flea market, which takes place every Sunday and offers a wide range of antiques, vintage items, and second-hand goods. 13. Take a cooking class together and learn how to make some of the region's delicious specialties, such as carbonnade flamande (beef stew) or tarte au maroilles (cheese tart). 14. Go on a wine tasting tour and discover the region's excellent wines, such as the Chablis or the Champagne. 15. Take a hot air balloon ride over the city and enjoy a bird's-eye view of Lille's beautiful architecture and surroundings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;|GPT</head><p>These are just a few ideas to get you started. Lille is a vibrant and romantic city that offers many opportunities for couples to have a great time together. Bon voyage !</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;|GPT-4-Turbo Verdict|&gt; Tie</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Arena Bench Prompt</head><p>Example 1: An prompt from Arena Bench requiring a very high level of complex problem-solving skill and reasoning while adhering to real-world application.</p><p>Create a flutter app for habit tracking that tracks daily habits for a user. The user should be able create multiple daily tasks that he wants to track. And he should be able to group the habits into a set of groups. One the user completes a task, he is rewarded a number of points per task. The app should have a page that summarize the total score per group. And the aggregate score of all the groups of habits. This flutter app needs to be compilable for both andriod and iOS.</p><p>Example 2: An prompt from Arena Bench requiring a very high level of complex problem-solving skill and reasoning while adhering to real-world application.</p><p>I want to set up a remote raspberry pi zero, powered by a solar panel with simple wiring. I want to power a small 2W pump, a simple electet microphone, a custom python script running on the raspberry pi that is used to classify audio detected by the microphone. What components will I need to optimise for cost and minimise any electrical work (e.g. soldering)? What size solar panel will I need to power this whole system? D.4. Arena Battles with No Ground Truth Example 1: An example of an Arena Battle between GPT-4-Turbo and Llama-2-13B-chat, where depending on different preferences, either answer can be argued as better than the other. While both answers are correct, GPT-4-Turbo's answer is concise and appropriate to the format of multiple choice question. On the other hand, Llama-2-13B-chat provides helpful explanation for the multiple choice answer. Plants, like other organisms, have different life cycle stages, and they can exist in two different forms: haploid and diploid. The haploid form is a single set of chromosomes, while the diploid form is two sets of chromosomes. As the plant changes from one form to another, this is called an alternation of generations. This process involves the alternation of haploid and diploid phases, and it is a fundamental aspect of the life cycle of plants.</p><p>Alternative life cycle and succession are not correct descriptions of this process, as they refer to different concepts. Alternative life cycle refers to the different stages of life an organism may go through, such as larva, pupa, and adult in insects, while succession refers to the process of one species replacing another in a particular environment or niche.</p><p>Example 2: An example of an Arena Battle between GPT-4-Turbo and Llama-2-13B-chat, where depending on different preferences, either answer can be argued as better than the other.</p><p>&lt;|User Prompt|&gt; What are critical elements of a Product Mindset for startup founders?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;|GPT-4-Turbo's Answer|&gt;</head><p>Adopting a Product Mindset is crucial for startup founders as it helps them focus on creating products that deliver value to customers while achieving business objectives. Here are some critical elements of a Product Mindset for startup founders:</p><p>1. Customer Centricity:</p><p>-Understand and empathize with your customers' needs, problems, and motivations.</p><p>-Continuously gather and incorporate customer feedback into the product development process.</p><p>-Prioritize features and improvements that directly address customer pain points and enhance their experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Value Proposition:</head><p>-Clearly define the unique value your product offers to the market.</p><p>-Align your product's features and capabilities with its value proposition to ensure it meets the expectations of your target audience.</p><p>3. Problem-Solving Focus:</p><p>-Identify and focus on solving real problems for users rather than just creating features for the sake of innovation.</p><p>-Validate assumptions and hypotheses through experimentation and user testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Iterative Development:</head><p>-Embrace an agile development approach, where you build, measure, and learn in iterative cycles.</p><p>-Be open to pivoting or making significant changes based on feedback and market response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Data-Driven Decision Making:</head><p>-Use data to inform decisions about product development, marketing, and growth strategies.</p><p>-Establish key performance indicators (KPIs) to measure the success of the product and track progress over time.</p><p>...</p><p>By internalizing these elements, startup founders can develop a strong Product Mindset that guides their decisions and strategies, ultimately leading to a product that customers love and that contributes to the startup's success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;|Llama-2-13B-chat's Answer|&gt;</head><p>A product mindset is a way of thinking and approaching product development that is critical for startup founders. Here are some essential elements of a product mindset for startup founders:</p><p>1. Customer-centricity: A product mindset starts with understanding the customer's needs, desires, and pain points. Founders must be obsessed with their customers and be willing to put in the effort to understand their needs.</p><p>2. Iteration and experimentation: A product mindset is all about experimentation and iteration. Founders must be willing to try new things, test hypotheses, and iterate on their product based on customer feedback.</p><p>3. Focus on value: A product mindset is focused on delivering value to the customer. Founders must be able to identify and prioritize the most important features and functionality that deliver the most value to their customers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Collaboration:</head><p>A product mindset requires collaboration between cross-functional teams, including engineering, design, and product management. Founders must be able to work effectively with these teams to develop a product that meets customer needs. 5. Data-driven decision making: A product mindset is data-driven, and founders must be comfortable using data to make informed decisions about their product. This includes analyzing customer feedback, usage data, and other metrics to inform product development.</p><p>...</p><p>By embracing these elements of a product mindset, startup founders can develop a product that meets customer needs, delivers value, and sets their company up for long-term success.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Win-rate (left) and battle count (right) between a subset of models in Chatbot Arena.</figDesc><graphic coords="4,136.31,248.85,118.20,118.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>MedicalFigure 3 .</head><label>3</label><figDesc>Figure 3. Similarity matrix of top-16 topic clusters. The number followed by the topic label represents the cluster size in percentage. Note that similarity is computed by cluster's centroid embeddings, hence diagonals are always one.</figDesc><graphic coords="6,155.36,79.73,100.39,100.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Model's performance between Arena Bench and MT-Bench, showing an increased gap between open and proprietary models. Both uses GPT-4 as judge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Intervals for the BT coefficients as a function of the number of samples and the number of models M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Screenshot of Chatbot Arena.</figDesc><graphic coords="12,55.44,67.06,486.00,371.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .Figure 11 .Figure 13 .</head><label>101113</label><figDesc>Figure 10. The number of votes per model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Synthetic experiment. Coefficients are drawn from the BT-coefficient distribution x on the left. Coverage of the uncorrected intervals is shown in the middle. Line plots of set width are shown on the right, and they almost perfectly match.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>How is the BT score related to the original Bradley-Terry model? In the original Bradley-Terry model, H t ∈ {0, 1}, and the probability of model m beating model m ′ is assumed to be given by θ((m ′ , m)) =e ξm e ξm + e ξ m ′ ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Example 3 :</head><label>3</label><figDesc>An example user prompt from the cluster "Movie Recommendations &amp; Rating". Both assistant retrieve the correct movie information and gave an almost identical explanation. Therefore, this prompt cannot distinguish GPT-4-0613's performance from Llama-2-70b-chat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>All plants have both a haploid and a diploid form. As the plant changes from one form to another, this is called: Multiple choice question. alternative life cycle succession an alternation of generations &lt;|GPT-4-Turbo's Answer|&gt; an alternation of generations &lt;|Llama-2-13B-chat's Answer|&gt; The correct answer is: an alternation of generations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of human preference datasets, including Anthropic HH<ref type="bibr" target="#b0">(Bai et al., 2022)</ref>, OpenAssistant Conversations<ref type="bibr" target="#b26">(Köpf et al., 2023)</ref>, and Chatbot Arena (as of 2024/1/21). The tokens are counted by Llama2's tokenizer. "Conv" = Conversation. "Lang" = Language.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6"># Convs # Models # Users # Langs</cell><cell>Avg. # Turns Avg. # Tokens Avg. # Tokens per Sample per Prompt per Response</cell></row><row><cell></cell><cell>Anthropic HH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">338,704</cell><cell></cell><cell></cell><cell>-</cell><cell>143</cell><cell>1</cell><cell>2.3</cell><cell>18.9</cell><cell>78.9</cell></row><row><cell></cell><cell>OpenAssistant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">66,497</cell><cell></cell><cell></cell><cell>-</cell><cell>13,500</cell><cell>35</cell><cell>-</cell><cell>36.9</cell><cell>214.2</cell></row><row><cell></cell><cell cols="9">Chatbot Arena (20240121) 243,329</cell><cell></cell><cell></cell><cell>50</cell><cell>90,051</cell><cell>149</cell><cell>1.3</cell><cell>94.9</cell><cell>269.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Model B</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>gpt-4-turbo</cell><cell>gpt-4-0613</cell><cell>mistral-medium</cell><cell>mixtral-8x7b-instruct-v0.1</cell><cell>gemini-pro-dev-api</cell><cell>claude-2.1</cell><cell>gpt-3.5-turbo-0613</cell><cell>claude-instant-1</cell><cell>llama-2-70b-chat</cell><cell>llama-2-13b-chat</cell><cell>llama-2-7b-chat</cell><cell>mistral-7b-instruct</cell></row><row><cell></cell><cell>gpt-4-turbo</cell><cell cols="11">0.00 0.68 0.69 0.75 0.71 0.76 0.77 0.75 0.76 0.79 0.86 0.90</cell><cell>0.9</cell><cell>0 2564 1189 1192 858 3053 1991 270 141 157 106 144</cell></row><row><cell></cell><cell>gpt-4-0613</cell><cell cols="11">0.32 0.00 0.50 0.59 0.61 0.59 0.59 0.59 0.62 0.72 0.70 0.80</cell><cell>0.8</cell><cell>2564 0</cell><cell>566 263 756 2227 1025 355 414 409 264 197</cell></row><row><cell></cell><cell>mistral-medium</cell><cell cols="11">0.31 0.50 0.00 0.54 0.56 0.50 0.57 0.52 0.69 0.73 0.60 0.87</cell><cell>1189 566</cell><cell>0</cell><cell>775 371 382 773 103 45</cell><cell>51</cell><cell>40</cell><cell>52</cell></row><row><cell></cell><cell>mixtral-8x7b-instruct-v0.1</cell><cell cols="11">0.25 0.41 0.46 0.00 0.54 0.48 0.51 0.56 0.58 0.53 0.73 0.84</cell><cell>0.7</cell><cell>1192 263 775</cell><cell>0</cell><cell>71 744 869 136 862 66</cell><cell>45</cell><cell>61</cell></row><row><cell>Model A</cell><cell>claude-instant-1 gpt-3.5-turbo-0613 claude-2.1 gemini-pro-dev-api</cell><cell cols="11">0.29 0.39 0.44 0.46 0.00 0.42 0.54 0.55 0.58 0.63 0.67 0.76 0.24 0.41 0.50 0.52 0.58 0.00 0.49 0.55 0.58 0.61 0.64 0.73 0.23 0.41 0.43 0.49 0.46 0.51 0.00 0.56 0.58 0.63 0.72 0.71 0.25 0.41 0.48 0.44 0.45 0.45 0.44 0.00 0.54 0.68 0.65 0.62</cell><cell>0.4 0.5 0.6</cell><cell>858 756 371 71 3053 2227 382 744 74 0 1991 1025 773 869 564 351 74 564 53 0 351 650 113 117 75 114 31 30 30 37 0 842 572 388 283 155 270 355 103 136 53 650 842 0 459 241 202 101</cell></row><row><cell></cell><cell>llama-2-13b-chat llama-2-70b-chat</cell><cell cols="11">0.24 0.38 0.31 0.42 0.42 0.42 0.42 0.46 0.00 0.61 0.58 0.61 0.21 0.28 0.27 0.47 0.37 0.39 0.37 0.32 0.39 0.00 0.53 0.57</cell><cell>0.3</cell><cell>141 414 45 862 31 113 572 459 157 409 51 66 30 117 388 241 383 0</cell><cell>383 134 369 0 251 621</cell></row><row><cell></cell><cell>llama-2-7b-chat</cell><cell cols="11">0.14 0.30 0.40 0.27 0.33 0.36 0.28 0.35 0.42 0.47 0.00 0.52</cell><cell>0.2</cell><cell>106 264 40</cell><cell>45</cell><cell>30</cell><cell>75 283 202 134 251</cell><cell>0</cell><cell>521</cell></row><row><cell></cell><cell>mistral-7b-instruct</cell><cell cols="11">0.10 0.20 0.13 0.16 0.24 0.27 0.29 0.38 0.39 0.43 0.48 0.00</cell><cell>0.1</cell><cell>144 197 52</cell><cell>61</cell><cell>37 114 155 101 369 621 521</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>GPT-4-0613's win-rate against Llama-2-70b-chat on 30 sample prompts from various topic clusters. We use GPT-4-turbo as judge to evaluate model responses in pairwise comparison.</figDesc><table><row><cell>Topic Cluster</cell><cell>Win-rate</cell><cell>Size</cell></row><row><cell>Python Game Programming Challenge</cell><cell cols="2">96.7% 0.2%</cell></row><row><cell>C/C++ Process Multi-Threading</cell><cell cols="2">86.7% 0.3%</cell></row><row><cell>SQL Query Database Assistance</cell><cell cols="2">73.3% 0.2%</cell></row><row><cell>Poetry Writing Prompts</cell><cell cols="2">66.7% 1.1%</cell></row><row><cell>Python Coding Basics</cell><cell cols="2">65.0% 0.2%</cell></row><row><cell>Linguistic Analysis &amp; Wordplay</cell><cell cols="2">58.3% 0.7%</cell></row><row><cell>Travel Itinerary Planning</cell><cell cols="2">58.3% 0.4%</cell></row><row><cell>Movie Recommendations &amp; Ratings</cell><cell cols="2">53.3% 0.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Pairwise agreement rate between crowd-user, gpt-4 judge, and experts on pairwise battles. The top part of the table is between GPT-4-Turbo and Llama-2-13b-chat. The bottom is between GPT-4-Turbo and GPT-3.5-Turbo-0613.</figDesc><table><row><cell>Llama-2-13b</cell><cell cols="2">Expert 1 Expert 2 GPT-4</cell></row><row><cell>Crowd</cell><cell>72.8%</cell><cell>77.8% 75.6%</cell></row><row><cell>Expert 1</cell><cell>-</cell><cell>89.8% 81.0%</cell></row><row><cell>Expert 2</cell><cell>-</cell><cell>-78.5%</cell></row><row><cell cols="3">GPT-3.5-Turbo Expert 1 Expert 2 GPT-4</cell></row><row><cell>Crowd</cell><cell>73.8%</cell><cell>83.1% 75.6%</cell></row><row><cell>Expert 1</cell><cell>-</cell><cell>79.4% 76.3%</cell></row><row><cell>Expert 2</cell><cell>-</cell><cell>-79.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>GPT-4-Turbo's win-rate across crowd-user, gpt-4 judge, and experts on pairwise battles against Llama-2-13b and GPT-3.5-</figDesc><table><row><cell>Turbo-0613.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell cols="3">Arena User Expert 1 Expert 2 GPT-4</cell></row><row><cell>Llama-2-13b</cell><cell>81.2%</cell><cell>89.4%</cell><cell>86.9% 78.8%</cell></row><row><cell>GPT-3.5-Turbo</cell><cell>76.3%</cell><cell>82.5%</cell><cell>89.4% 79.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Confusion matrix of different α. "Pred." means predicted. Positive means anomalous and negative means normal.</figDesc><table><row><cell>20000 30000</cell><cell></cell><cell></cell><cell cols="2">random pairwise adaptive</cell></row><row><cell>n</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>n</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.25 0</cell><cell>0.30</cell><cell>0.35 average width (s) 0.40</cell><cell>0.45</cell><cell>0.50</cell></row><row><cell cols="5">Figure 7. Interval widths on the win matrix (upper figure) and on</cell></row><row><cell cols="5">the BT coefficients (lower figure) as a function of the number of</cell></row><row><cell cols="5">samples, for random sampling and also adaptive sampling. Im-</cell></row><row><cell cols="5">provements from adaptive sampling can be seen in both cases,</cell></row><row><cell cols="4">although they are more subtle on the scale of the score.</cell><cell></cell></row><row><cell>α = 0.1</cell><cell></cell><cell cols="3">Pred. Positive Pred. Negative</cell></row><row><cell cols="2">Actual Positive</cell><cell>13/14</cell><cell cols="2">12/36</cell></row><row><cell cols="2">Actual Negative</cell><cell>1/14</cell><cell cols="2">24/36</cell></row><row><cell>α = 0.3</cell><cell></cell><cell cols="3">Pred. Positive Pred. Negative</cell></row><row><cell cols="2">Actual Positive</cell><cell>21/29</cell><cell></cell><cell>4/21</cell></row><row><cell cols="2">Actual Negative</cell><cell>8/29</cell><cell cols="2">17/21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Below is the terminal output of the snowman game programmed by GPT-4-0613, demonstrating its correctness. It is clear that GPT-4-0613's response is more helpful and relevant to the user's request.</figDesc><table><row><cell></cell><cell>7: Packaging and distributing the game</cell></row><row><cell cols="2">* Package the game as a standalone executable file using a tool like</cell></row><row><cell cols="2">PyInstaller.</cell></row><row><cell cols="2">* Distribute the game to friends, family, or online communities, and gather</cell></row><row><cell cols="2">feedback for future improvements.</cell></row><row><cell cols="2">This is a high-level overview of the steps involved in creating a snowman</cell></row><row><cell cols="2">game similar to Hangman. Of course, the details will depend on your</cell></row><row><cell cols="2">specific implementation and the tools you choose to use. Good luck with</cell></row><row><cell cols="2">your project!</cell></row><row><cell cols="2">&lt;|GPT-4-Turbo's Judgment Verdict|&gt;</cell></row><row><cell cols="2">GPT-4-0613 Wins</cell></row><row><cell cols="2">Program Output: &gt;&gt;&gt; python snowman.py</cell></row><row><cell cols="2">Choose difficulty (easy, medium, hard): easy</cell></row><row><cell cols="2">The word contains 6 letters.</cell></row><row><cell cols="2">Good job! The word contains that letter.</cell></row><row><cell cols="2">_ A _ _ _ _</cell></row><row><cell cols="2">Guess a letter: e</cell></row><row><cell cols="2">Good job! The word contains that letter.</cell></row><row><cell cols="2">_ A _ _ _ E</cell></row><row><cell cols="2">Guess a letter: d</cell></row><row><cell cols="2">_____</cell></row><row><cell>(</cell><cell>)</cell></row><row><cell cols="2">Sorry, the word does not contain that letter.</cell></row><row><cell cols="2">_ A _ _ _ E</cell></row><row><cell cols="2">Guess a letter: g</cell></row><row><cell cols="2">_____</cell></row><row><cell>(</cell><cell>)</cell></row><row><cell>( (</cell><cell>) )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>&lt;|GPT-4-0613's Answer|&gt; The</head><label></label><figDesc>Mask of Fu Manchu was released in 1932, and Blind Shaft was released in 2003. Therefore, The Mask of Fu Manchu came out first. An example user prompt from the cluster "Travel Itinerary Planning". Both assistant provided an equally helpful itinerary. Therefore, this prompt cannot distinguish GPT-4-0613's performance from Llama-2-70b-chat. Visit the Palais des Beaux-Arts: This is one of France's largest museums and it's right in the heart of Lille. It houses collections of European paintings, a medieval and Renaissance collection, ceramics, French sculptures and more. 2. Explore the Vieux Lille: Stroll through the charming old town with its Flemish influenced architecture, chic boutiques, and delightful eateries . 3. Visit the Lille Cathedral (Basilique-cathedrale Notre-Dame-de-la-Treille) : This beautiful cathedral is a mix of modern and traditional architecture. 4. Enjoy a picnic at the Citadel Park: This is a great place to relax, have a picnic, and enjoy the outdoors. 5. Visit the LaM (Lille Metropole Musee d'art moderne, d'art contemporain et d'art brut): This museum is home to a unique collection of modern and contemporary art. 6. Take a boat tour on the Deule River: This is a romantic way to see the city and its surroundings. 7. Visit the Maison Natale de Charles de Gaulle: This is the birthplace of Charles de Gaulle and it's now a museum dedicated to his life and work. 8. Explore the Marche de Wazemmes: This is one of the largest markets in</figDesc><table><row><cell>Example 4: &lt;|User Prompt|&gt;</cell></row><row><cell>Can you make a list of things I could do with my girlfriend in Lille,</cell></row><row><cell>France?</cell></row><row><cell>&lt;|GPT-4-0613's Answer|&gt;</cell></row><row><cell>Absolutely, here are some suggestions for activities you and your girlfriend</cell></row><row><cell>can enjoy in Lille, France:</cell></row><row><cell>1.</cell></row><row><cell>&lt;|User Prompt|&gt;</cell></row><row><cell>Which film came out first, Blind Shaft or The Mask Of Fu Manchu?</cell></row><row><cell>&lt;|Llama-2-70b-chat's Answer|&gt;</cell></row><row><cell>The Mask Of Fu Manchu came out first. It was released in 1932, while Blind</cell></row><row><cell>Shaft was released in 2003.</cell></row><row><cell>&lt;|GPT-4-Turbo Verdict|&gt;</cell></row><row><cell>Tie</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://chat.lmsys.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The number was estimated by Google Analytics as of March 2024. Note that user visit may not convert to votes as our website also offers "direct chat" mode.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/MaartenGr/BERTopic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The laborers are graduate students at UC Berkeley.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This project is supported by sponsorship from <rs type="funder">Kaggle</rs>, <rs type="funder">MBZUAI</rs>, <rs type="grantNumber">a16z</rs>, <rs type="funder">Together AI</rs>, <rs type="funder">Anyscale</rs>, and <rs type="funder">HuggingFace</rs>. This project is also partly supported by <rs type="funder">Accenture</rs>, <rs type="funder">AMD</rs>, <rs type="funder">Google</rs>, <rs type="funder">IBM</rs>, <rs type="funder">Intel</rs>, <rs type="funder">Microsoft</rs>, <rs type="funder">Samsung SDS</rs>, SAP, <rs type="funder">Uber</rs>, and <rs type="funder">VMware</rs>. The authors would like to thank <rs type="person">Siyuan Zhuang</rs> for insightful discussion and <rs type="person">Tijana Zrnić</rs> for helpful feedback on the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9HzYjn6">
					<idno type="grant-number">a16z</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Valid P-Value</head><p>Consider the p-value</p><p>We will prove that this p-value is valid, i.e., that P(p i ≤ t) ≤ t, under the null hypothesis that the vector</p><p>is exchangeable, where ∥ denotes the concatenation operator. First, notice that we can equivalently write</p><p>We also have that</p><p>Then, since the data points are exchangeable, we have that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sample Prompts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Prompt Examples From Different Clusters</head><p>Example 1: An example user prompt from the cluster "Python Game Programming Challenge". This prompt distinctively separates GPT-4-0613 performance from Llama-2-70b-chat. The user request "higher quality code" from each assistant. While GPT-4-Turbo provides a correct implementation along with helpful explanation, Llama-2-70b-chat fail to provide any program and provided a less helpful response overall.</p><p>&lt;|User Prompt|&gt; Develop a snowman game similar to the Hangman game but draws a snowman figure standing on the snow instead of a hangman. It also downloads (or uses library) the complete English dictionary to select words for three levels of difficulty. Think step-by-step, take a deep breath and choose a higher quality code for each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;|GPT-4-0613's Answer|&gt;</head><p>Sure, here's a step-by-step guide on how to create a Snowman game similar to Hangman. We'll use Python for this example.</p><p>Step 1: Import necessary libraries '''python import random import urllib.request '''</p><p>Step 2: Download the English dictionary '''python url = "<ref type="url" target="https://raw.githubusercontent.com/dwyl/english-words/master/words.txt">https://raw.githubusercontent.com/dwyl/english-words/master/words.txt</ref> " urllib.request.urlretrieve(url, 'words.txt') '''</p><p>Step 3: Load the words from the dictionary and categorize them into three levels of difficulty print("Sorry, you didn't guess the word. The word was", word) '''</p><p>Step 7: Run the game '''python play_snowman() ''' This is a simple version of the game. You can enhance it by adding more features like scoring, time limit, etc. '''</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;|Llama-2-70b-chat's Answer|&gt;</head><p>Sure! Here's a step-by-step guide to developing a snowman game similar to Hangman, using the complete English dictionary for three levels of difficulty.</p><p>Step 1: Setting up the project</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Arena Bench System Prompt</head><p>The novel evaluation procedure is as follow: we prompt GPT-4-Turbo with the system prompt displayed below alongside a user prompt, a reference answer, and 2 assistant's answers. For reference answer, we present the user prompt with 3 assistants' answers, GPT-4-Turbo, GPT-4-0314, and Claude-1, to GPT-4-Turbo and ask GPT-4-Turbo to generate an answer to the prompt. To ensure consistent pairwise judgment, we set up GPT-3.5-Turbo-0301 as the baseline answer for all models to be compared against. To avoid positional bias, we conduct two judgments per prompt: the first judgment presents the baseline answer as Assistant A while the second judgment presents the baseline answer as Assistant B. In total, we conduct 700 pairwise comparisons between each model against GPT-3.5-Turbo-0301 across 350 user prompts to calculate a win-rate against the baseline. Then we project the win-rate on a scale from 0 to 10 by assigning wins with a score of 10, ties with a score of 5, and losses with a score of 0. Further, we assign a significant win or loss as 3 wins or 3 losses, respectively, and keeping the other verdicts as a single win, loss, or tie. Finally, we calculate the final score by averaging across the wins, losses, and ties.</p><p>&lt;|System Prompt|&gt; Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. Your job is to evaluate which assistant's answer is better.</p><p>When evaluating the assistants' answers, compare both assistants' answers.</p><p>You must identify and correct any mistakes or inaccurate information.</p><p>Then consider if the assistant's answers are helpful, relevant, and concise.</p><p>Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive.</p><p>Then consider the creativity and novelty of the assistant's answers when needed. Finally, identify any missing important information in the assistants' answers that would be beneficial to include when responding to the user prompt.</p><p>After providing your explanation, you must output only one of the following choices as your final verdict with a label:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Das-Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">uncovered: Robustness and best practices in language model evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boubdir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ermis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><surname>Elo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. the method of paired comparisons</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Preference-based rank elicitation using statistical models: The case of mallows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huellermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Szörényi</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v32/busa-fekete14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-24">22-24 Jun 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1071" to="1079" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Preference-based rank elicitation using statistical models: The case of mallows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huellermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Szörényi</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v32/busa-fekete14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-24">22-24 Jun 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1071" to="1079" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Evaluating large language models trained on code</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sequential Design of Experiments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chernoff</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-4380-9_27</idno>
		<ptr target="https://doi.org/10.1007/978-1-4612-4380-9_27" />
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="345" to="360" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can large language models be an alternative to human evaluations?</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.870</idno>
		<ptr target="https://aclanthology.org/2023.acl-long.870" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15607" to="15631" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boosting language models with high-quality feedback</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Ultrafeedback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bootstrap confidence intervals</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Diciccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="228" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Probability: theory and examples</title>
		<author>
			<persName><forename type="first">R</forename><surname>Durrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The proposed uscf rating system, its development, theory, and applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Elo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chess Life</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="242" to="247" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Statistical methods for research workers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1928">1928</date>
			<publisher>Oliver and Boyd</publisher>
		</imprint>
	</monogr>
	<note>Number 5</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the so-called &quot;huber sandwich estimator&quot;&apos; and &quot;robust standard errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="299" to="302" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gemini: a family of highly capable multimodal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gemini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A dialogue model for academic research</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gudibande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Koala</surname></persName>
		</author>
		<ptr target="https://bair.berkeley.edu/blog/2023/04/03/koala/" />
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
	<note>Blog post</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Grootendorst</surname></persName>
		</author>
		<author>
			<persName><surname>Bertopic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05794</idno>
		<title level="m">Neural topic modeling with a class-based tf-idf procedure</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sekhon</surname></persName>
		</author>
		<title level="m">Time-uniform chernoff bounds via nonnegative supermartingales</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02143</idno>
		<title level="m">Competition-level problems are effective llm evaluators</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The behavior of maximum likelihood estimates under nonstandard conditions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MM algorithms for generalized Bradley-Terry models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1079120141</idno>
		<ptr target="https://doi.org/10.1214/aos/1079120141" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="384" to="406" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online active model selection for pre-trained classifiers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Gürel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karlaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The perils of using Mechanical Turk to evaluate open-ended text generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.97</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="1265" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking benchmarking in nlp</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ringshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4110" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Von Rütte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-R</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barhoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagyfi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07327</idno>
		<title level="m">Openassistant conversationsdemocratizing large language model alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning (ICML 2000)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</editor>
		<meeting>the 17th International Conference on Machine Learning (ICML 2000)<address><addrLine>CA</addrLine></address></meeting>
		<imprint>
			<publisher>Stanford</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/alpaca_eval" />
		<title level="m">Alpacaeval: An automatic evaluator of instruction-following models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dal Lago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">6624</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-emnlp.311</idno>
		<ptr target="https://aclanthology.org/2023.findings-emnlp.311" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2020">2020. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Proving test set contamination in black box language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17623</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gametheoretic statistics and safe anytime-valid inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grünwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="576" to="601" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ties in paired-comparison experiments: A generalization of the bradley-terry model</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Kupper</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1967.10482901</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">317</biblScope>
			<biblScope unit="page" from="194" to="204" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A M</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garriga-Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Online rank elicitation for plackettluce: A dueling bandits approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Szörényi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2015/file/7eacb532570ff6858afd2723755ff790-Paper.pdf" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and finetuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">E-values: Calibration, combination and applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1736" to="1754" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-instruct: Aligning language models with self-generated instructions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.754</idno>
		<ptr target="https://aclanthology.org/2023.acl-long.754" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13484" to="13508" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Estimating means of bounded random variables by betting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Waudby-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09686</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of misspecified models</title>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the econometric society</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.04850</idno>
		<title level="m">Rethinking benchmark and contamination for language models with rephrased samples</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hellaswag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Lmsys-chat-1m: A large-scale real-world llm conversation dataset</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Judging LLM-as-a-judge with MT-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=uccHPGDlao" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06364</idno>
		<title level="m">A human-centric benchmark for evaluating foundation models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Starling-7b: Improving llm helpfulness &amp; harmlessness with rlaif</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-11">November 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><surname>Tie</surname></persName>
		</author>
		<title level="m">relatively the same</title>
		<imprint/>
	</monogr>
	<note>A=B</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Assistant B is slightly better</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m">Assistant B is significantly better</title>
		<imprint/>
	</monogr>
	<note>Example output: &quot;My final verdict is tie:. A=B</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
