<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Capabilities of Gemini Models in Medicine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-01">1 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Khaled</forename><surname>Saab •</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical Lead</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei-Hung</forename><surname>Weng •</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Stutz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Strother</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunjong</forename><surname>Park</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elahe</forename><surname>Vedadi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juanma</forename><forename type="middle">Zambrano</forename><surname>Chaves</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Szu-Yeu</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mike</forename><surname>Schaekermann</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cathy</forename><surname>Cheung</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anil</forename><surname>Palepu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tomer</forename><surname>Golany</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luyang</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nenad</forename><surname>Tomasev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Freyberg</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Lau</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Kemp</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Lai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kimberly</forename><surname>Kanada</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siwai</forename><surname>Man</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kavita</forename><surname>Kulkarni</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruoxi</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Caine</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Natasha</forename><surname>Latysheva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Mansfield</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jesper</forename><surname>Anderson</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bradley</forename><surname>Green</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renee</forename><surname>Wong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Chou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Manyika</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dale</forename><surname>Webster</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joelle</forename><surname>Barral</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Semturs</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Sara Mahdavi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juraj</forename><surname>Gottweis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Senior Lead</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Co-First</surname></persName>
						</author>
						<author>
							<persName><surname>Core</surname></persName>
						</author>
						<title level="a" type="main">Capabilities of Gemini Models in Medicine</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-01">1 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">09EF7DAB3280120C38D866ADE0D84AB3</idno>
					<idno type="arXiv">arXiv:2404.18416v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-24T14:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Google DeepMind, 3 Google Cloud, 4 Verily Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with their strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini 1.0 and Gemini 1.5, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly integrate the use of web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks spanning text, multimodal and long-context applications, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy, outperforming our prior best Med-PaLM 2 by 4.6%. Our search-based strategy generalizes with SoTA performance on complex diagnostic challenges from the New England Journal of Medicine (NEJM) and the GeneTuring benchmark. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health &amp; medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization and referral letter generation, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for the promise of Med-Gemini in many areas of medicine, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Med-Gemini, a family of highly capable, multimodal medical models built upon Gemini. We enhance our models' clinical reasoning capabilities through self-training and web search integration, while improving multimodal performance via fine-tuning and customized encoders. Med-Gemini models achieve state-of-the-art (SoTA) performance on 10 out of 14 medical benchmarks that span text, multimodal, and long-context applications, and surpass the GPT-4 model family on every benchmark where a direct comparison could be made. The bar chart shows the relative percentage gains from our models over prior SoTA across the benchmarks. In particular, on the MedQA (USMLE) benchmark, we attain a new SoTA surpassing our prior best (Med-PaLM 2) by a significant margin of 4.6%. Moreover, re-annotation of the dataset with expert clinicians reveals that 7.4% of questions are deemed unfit for evaluation as they either lack key information, have incorrect answers, or support multiple plausible interpretations. We account for these data quality issues to characterize more precisely the performance of our model. Med-Gemini models excel in multimodal and long-context capabilities as evidenced by their SoTA performance on several benchmarks including needle-in-a-haystack retrieval from long, de-identified health records, and medical video question answering benchmarks.</p><p>Moving beyond benchmarks, we also demonstrate the real-world potential of Med-Gemini through quantitative evaluation on medical summarization, referral letter generation, and medical simplification tasks where our models outperform human experts, in addition to qualitative examples of multimodal medical dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Medicine is a multifaceted endeavor. A clinician's day-to-day work involves patient consultations, where clear communication of diagnoses, treatment plans, and empathy are essential for building trust.</p><p>Complex cases necessitate deeper understanding of the patient's history within the electronic medical record, along with multimodal reasoning from medical images and other diagnostics. To guide their decisions under uncertainty, clinicians must stay abreast of the latest medical information from a wide variety of authoritative sources that can range from research publications to procedural videos. The art of care delivery hinges on a clinician's ability to perform advanced clinical reasoning, synthesize complex information from diverse and multimodal sources, and collaborate effectively with other clinicians to help people in their care journeys. Although artificial intelligence (AI) systems can assist individual medical tasks <ref type="bibr" target="#b103">(Rajpurkar et al., 2022)</ref> and demonstrate early promise towards multimodal multi-task "generalist" medical uses <ref type="bibr">(Moor et al., 2023a;</ref><ref type="bibr">Tu et al., 2024a)</ref>, the development of more sophisticated reasoning, multimodal, and long-context understanding capabilities would enable significantly more intuitive and helpful assistive tools for clinicians and patients alike.</p><p>The advent of large language models (LLMs) and large multimodal models (LMMs), like GPT-4 <ref type="bibr" target="#b1">(Achiam et al., 2023)</ref>, PaLM <ref type="bibr" target="#b15">(Chowdhery et al., 2023)</ref> and Gemini <ref type="bibr" target="#b34">(Gemini Team, Google, 2023)</ref>, showed that such models effectively encode clinical knowledge and can perform impressively in medical question answering benchmarks, even for complex cases and scenarios requiring specialized knowledge <ref type="bibr" target="#b4">(Antaki et al., 2023;</ref><ref type="bibr" target="#b26">Eriksen et al., 2023;</ref><ref type="bibr" target="#b60">Kanjee et al., 2023)</ref>. However, performance on such tasks is far from indicative of real-world utility. The unique nature of medical data and the critical need for safety demand specialized prompting <ref type="bibr" target="#b85">(Nori et al., 2023)</ref>, fine-tuning, or potentially both along with careful alignment of these models <ref type="bibr" target="#b93">(Ouyang et al., 2022)</ref>.</p><p>Medically fine-tuned LLMs <ref type="bibr" target="#b76">(Luo et al., 2022;</ref><ref type="bibr">Singhal et al., 2023a;</ref><ref type="bibr" target="#b119">Toma et al., 2023)</ref> can also provide high-quality long-form answers to nuanced and open-ended medical questions asked by millions of internet users, with Med-PaLM 2 surpassing physicians on axes such as factuality, reasoning, harm, and bias <ref type="bibr">(Singhal et al., 2023b)</ref>. The potential extends beyond question answering. LMMs <ref type="bibr" target="#b68">(Li et al., 2024;</ref><ref type="bibr">Moor et al., 2023b</ref>) such as Flamingo-CXR and Med-PaLM M are comparable with radiologists in controlled settings for generating radiology reports <ref type="bibr">(Huang et al., 2023;</ref><ref type="bibr">Tanno et al., 2024;</ref><ref type="bibr">Tu et al., 2024a)</ref>. In the more challenging setting of text-based diagnostic consultations with patient actors, the Articulate Medical Intelligence Explorer (AMIE) model outperformed primary care physicians on several evaluation axes for diagnostic dialogue <ref type="bibr">(Tu et al., 2024b)</ref>. Despite these promising results, there are considerable opportunities for improvement in performance. LLMs demonstrate suboptimal clinical reasoning under uncertainty, with confabulations and bias remaining key challenges <ref type="bibr" target="#b92">(Omiye et al., 2023;</ref><ref type="bibr" target="#b124">Umapathi et al., 2023)</ref>. The use of tools and up-to-date medical information <ref type="bibr" target="#b144">(Zakka et al., 2024)</ref> to accomplish medical tasks remains a challenge for LLMs, alongside effective collaboration with clinicians <ref type="bibr" target="#b79">(McDuff et al., 2023)</ref>. Additionally, their ability to handle complex multimodal medical data (for example, integrating images, videos, and de-identified health records over time) is currently limited <ref type="bibr">(Tu et al., 2024a)</ref>. Although these capabilities are particularly meaningful in medical applications, improvements in performance might be relevant beyond the medical domain. Tasks and benchmarks developed to measure and accelerate the progress of medical LLMs will be broadly impactful.</p><p>The Gemini models, as detailed in the Gemini 1.0 and 1.5 technical reports <ref type="bibr" target="#b34">(Gemini Team, Google, 2023</ref><ref type="bibr" target="#b81">, 2024)</ref>, are a new generation of highly capable multimodal models with novel foundational capabilities that have the potential to address some of these key challenges for medical AI. The models are transformer decoder models <ref type="bibr" target="#b8">(Brown et al., 2020;</ref><ref type="bibr" target="#b126">Vaswani et al., 2017)</ref> enhanced with innovations in architecture, optimization and training data, enabling them to exhibit strong capabilities across various modalities including images, audio, video, and text. The recent addition of the mixture-ofexperts architecture <ref type="bibr" target="#b28">(Fedus et al., 2022;</ref><ref type="bibr" target="#b110">Shazeer et al., 2017)</ref> allows the Gemini models to efficiently scale and reason over significantly longer and more complex data at inference time.</p><p>Building on the strengths of the Gemini models, we present Med-Gemini, a family of models fine-tuned and specialized for medicine. The notion of generalist medical AI models has received considerable attention with impressive demonstrations of the possibilities for such systems <ref type="bibr">(Tu et al., 2024a)</ref>. However, while the generalist approach is an meaningful research direction for medicine, real world considerations present trade-offs and requirements for task-specific optimizations which are at odds with each other. In this work, we do not attempt to build a generalist medical AI system. Rather, we introduce a family of models, each optimized for different capabilities and application-specific scenarios, considering factors such as training data, compute availability, and inference latency.</p><p>Med-Gemini inherits Gemini's foundational capabilities in language and conversations, multimodal understanding, and long-context reasoning. For language-based tasks, we enhance the models' ability to use web search through self-training and introduce an inference time uncertainty-guided search strategy within an agent framework. This combination enables the model to provide more factually accurate, reliable, and nuanced results for complex clinical reasoning tasks. This leads to the state-ofthe-art (SoTA) performance of 91.1% accuracy on MedQA (USMLE) <ref type="bibr" target="#b55">(Jin et al., 2021)</ref> surpassing prior Med-PaLM 2 models by 4.6%. We further conduct a careful examination of the MedQA (USMLE) data quality through relabelling with multiple independent expert clinicians, identifying unanswerable questions due to missing information and errors, enabling reliable analysis and characterization of our SoTA performance. The uncertainty-guided search strategy generalizes and leads to SoTA performance on the New England Journal of Medicine (NEJM) clinico-pathological conference (CPC) cases <ref type="bibr" target="#b60">(Kanjee et al., 2023;</ref><ref type="bibr" target="#b79">McDuff et al., 2023)</ref> and the GeneTuring benchmark <ref type="bibr" target="#b47">(Hou and Ji, 2023)</ref>. Beyond their strong performance on such benchmarks, our models suggest real-world utility by performing favorably when compared to human physicians on tasks such as medical note summarization and clinical referral letter generation.</p><p>As Gemini models are trained to accommodate textual input interleaved with a wide variety of other data modalities, they are known to excel in multimodal tasks. This confers impressive out-of-thebox SoTA performance on some multimodal medical benchmarks such as the NEJM Image Challenge. However, their performance can be further improved when dealing with specialized medical modalities not heavily represented in their pretraining data. We address this through multimodal fine-tuning and demonstrate the models' adaptability to novel medical modalities using customized encoders leading to SoTA performance on benchmarks such as Path-VQA <ref type="bibr" target="#b45">(He et al., 2020)</ref> and ECG-QA <ref type="bibr">(Oh et al., 2023)</ref> among others. We qualitatively showcase our models' capabilities for clinically-meaningful multimodal conversation on a variety of both in-distribution and out-of-distribution data modalities.</p><p>Finally, the long-context capabilities of Gemini models open many exciting possibilities for application in medicine, given how frequently a clinically-meaningful decision requires parsing of large amounts of data with significant risks of "information overload" <ref type="bibr" target="#b108">(Sbaffi et al., 2020)</ref>. Our Med-Gemini models configured for long-context processing are able to seamlessly analyze complicated and longform modalities such as de-identified electronic health records (EHRs) and videos. We demonstrate the effectiveness of these capabilities with impressive performance on the "needle-in-a-haystack" long EHR understanding <ref type="bibr">(Johnson et al., 2019a)</ref>, medical instructional video question answering <ref type="bibr" target="#b42">(Gupta and Demner-Fushman, 2022)</ref>, surgical action recognition from video <ref type="bibr" target="#b38">(Goodman et al., 2021)</ref>, and the Critical View of Safety (CVS) assessment of surgical video <ref type="bibr" target="#b116">(Strasberg and Brunt, 2010)</ref> tasks.</p><p>The advances of Med-Gemini have great promise, but it remains crucial to carefully consider the nuances of the medical field, acknowledge the role of AI systems as assistive tools for expert clinicians, and conduct rigorous validation before real-world deployments at scale.</p><p>Our key contributions are summarized below:</p><p>• Med-Gemini, our new family of multimodal medical models: We introduce a new family of highly capable multimodal medical models, built upon Gemini. Med-Gemini demonstrates important advancements in clinical reasoning, multimodal, and long-context capabilities. They are further fine-tuned to make use of web search for current information and can be customized to novel medical modalities through the use of modality-specific encoders. • Comprehensive benchmarking: We evaluate Med-Gemini's capabilities on a suite of 25 tasks across 14 medical benchmarks, encompassing text, multimodal, and long-context applications.</p><p>To the best of our knowledge, this is the most comprehensive benchmarking of multimodal medical models to date. • SoTA results on clinical language tasks: Med-Gemini optimized for clinical reasoning reaches a SoTA performance of 91.1% on MedQA (USMLE) using a novel uncertainty-guided search strategy. We quantify and characterize our performance improvements through a careful re-annotation of the MedQA dataset with clinical experts, finding these improvements to be meaningful. We further demonstrate the effectiveness of the search strategy through SoTA performance on NEJM CPC and GeneTuring benchmarks. • Multimodal and long-context capabilities: Med-Gemini attains SoTA performance on 5 out of 7 multimodal medical benchmarks evaluated in this study. We demonstrate the effectiveness of multimodal medical fine-tuning and the ability to customize to novel medical modalities such as electrocardiograms (ECGs) using specialized encoder layers. Med-Gemini also exhibits strong long-context reasoning capabilities, attaining SoTA on challenging benchmarks such as "needle-in-the-haystack" tasks in lengthy electronic health records or benchmarks for medical video understanding. In addition, in forthcoming work, we will also rigorously explore the capabilities of Gemini in radiology report generation. • Real-world utility of Med-Gemini: Beyond performance on popular medical benchmarks, we preview the potential real-world utility of Med-Gemini through quantitative evaluations on tasks such as medical note summarization, clinical referral letter generation, and EHR question answering. We further showcase qualitative examples in multimodal diagnostic dialogues and applications of the models' long-context capabilities for medical education, clinician-facing tools, and biomedical research. We note that such uses (particularly in safety-critical areas like diagnosis) would require considerable further research and development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>As introduced in the Gemini technical reports <ref type="bibr" target="#b34">(Gemini Team, Google, 2023</ref><ref type="bibr" target="#b81">, 2024)</ref>, the Gemini ecosystem encompasses a suite of models varying in size, modality encoders, and architectures, trained on a wide variety of high quality data across many modalities. The Gemini models exhibit state-of-the-art results across a diverse array of language, reasoning, coding, multilingual, image, and video benchmarks. Notably, the Gemini 1.0 Ultra model excels in language-based tasks that require complex reasoning, and the Gemini 1.5 Pro model adds the ability to efficiently handle and make use of long-context inputs spanning millions of tokens and/or multimodal inputs such as hours of video or tens of hours of audio. Gemini 1.0 Nano is the smallest model variant in the Gemini model family that can run efficiently on-device.</p><p>We develop our Med-Gemini models by building on the Gemini family, focusing on the following capabilities and methods:</p><p>1. Advanced reasoning via self-training and web search integration: For language tasks that require less complex reasoning, such as summarizing medical notes and creating referral letters, we introduce Med-Gemini-M 1.0 by fine-tuning the Gemini 1.0 Pro model. For other tasks that require more advanced reasoning, we introduce Med-Gemini-L 1.0 by fine-tuning the Gemini 1.0 Ultra model using a self-training method to enable the models to efficiently use web search. We develop a novel uncertainty-guided search strategy at inference time to improve performance on complex clinical reasoning tasks. 2. Multimodal understanding via fine-tuning and customized encoders: The Gemini models are natively multimodal and have demonstrated impressive zero-shot performance on many multimodal benchmarks. However, the unique nature and heterogeneity of some medical modalities require fine-tuning to achieve the best possible performance. We introduce Med-Gemini-M 1.5 by performing fine-tuning with Gemini 1.5 Pro on a suite of multimodal medical datasets. We introduce Med-Gemini-S 1.0 and demonstrate the Gemini models' capability to adapt to novel medical modalities using specialized encoders with the Gemini 1.0 Nano model. 3. Long-context processing with chain-of-reasoning: For the long-context processing tasks, we re-use Med-Gemini-M 1.5 with a long-context configuration. In addition, we also develop a novel inference-time chain-of-reasoning technique inspired by <ref type="bibr">Tu et al. (2024b)</ref> to enable better understanding of long EHRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Advanced reasoning via self-training and web search integration</head><p>Clinical reasoning is a fundamental skill that underpins successful care. Although it is a broad field with many definitions, clinical reasoning can be conceptualized as an iterative process by which a physician integrates their own clinical knowledge with initial patient information to form a case representation. This representation is then used to guide the iterative acquisition of additional information until a confidence threshold is reached to support a final diagnosis with plans for treatment and management <ref type="bibr" target="#b41">(Gruppen, 2017)</ref>. During this process, a physician may reason across many diverse inputs, such as patient symptoms, medical and socio-economic history, investigations and lab tests, prior responses to treatments and other wider factors such as epidemiological data. Moreover, many of these inputs have a time component, such as a series of evolving symptoms, lab measurements over time, or the various temporal data that is collected for monitoring health, such as electrocardiograms (ECGs). Medical knowledge is highly non-stationary, with reducing "doubling times" in the volume of medical information driven by the rapid pace of research <ref type="bibr" target="#b22">(Densen, 2011;</ref><ref type="bibr" target="#b40">Grandage et al., 2002)</ref>. To ensure that their outputs reflect the latest information in this domain, LLMs might ideally not only possess strong reasoning capabilities but also be able to integrate up-to-date information, for example, from authoritative web sources. This grounding in external knowledge has the potential to reduce uncertainty in the model's responses, but requires an informed approach to information retrieval itself. The key challenge we aim to tackle with our medical fine-tuning of Gemini 1.0 Ultra is to improve the model's ability to make the most helpful web search queries and integrate their results in the reasoning process to generate accurate answers. The resulting model is Med-Gemini-L 1.0.</p><p>Instruction fine-tuning has been shown to improve the clinical reasoning ability of LLMs <ref type="bibr">(Singhal et al., 2023b)</ref>. A prevalent instruction-tuning dataset is MedQA <ref type="bibr" target="#b55">(Jin et al., 2021)</ref>, which consists of multiple-choice questions representative of US Medical License Exam (USMLE) questions, that are designed to assess medical knowledge and reasoning across diverse scenarios with a large number of variables of interest <ref type="bibr" target="#b55">(Jin et al., 2021)</ref>. However, MedQA only provides a multiple-choice ground truth, and lacks expert demonstrations of the reasoning process necessary to train LLMs for clinical reasoning across diverse settings. As a result, LLMs fine-tuned on MedQA, such as Med-PaLM 2 <ref type="bibr">(Singhal et al., 2023b)</ref>, still exhibit significant reasoning shortcomings. This, coupled with the lack of access to web search in such systems, results in factuality errors that compound in downstream reasoning steps or lead to models adopting premature conclusions without considering all possible reasoning pathways.  Fine-tuning datasets for language-based tasks Collecting expert demonstrations of clinical reasoning, including how experts make informed use of knowledge retrieval tools such as web search, is both time-consuming and difficult to scale. To overcome this, we generate two novel datasets with self-training as described below: MedQA-R (Reasoning), which extends MedQA with synthetically generated reasoning explanations, or "Chain-of-Thoughts" (CoTs), and MedQA-RS (Reasoning and Search), which extends MedQA-R with instructions to use web search results as additional context to improve answer accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Med-Gemini</head><p>To add further variety to the fine-tuning data mixture of Med-Gemini-L 1.0, we also add a long-form question answering dataset, which consists of 260 expert-crafted long-form responses to questions from HealthSearchQA, LiveQA, and MedicationQA in the MultiMedQA benchmark <ref type="bibr">(Singhal et al., 2023a)</ref>, along with a medical summarization dataset, consisting of 65 clinician-written summaries of medical notes from MIMIC-III <ref type="bibr" target="#b57">(Johnson et al., 2016)</ref>. We provide an overview of the datasets for language-based instruction fine-tuning datasets in Table <ref type="table" target="#tab_9">C1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-training with search</head><p>Inspired by the recent success of self-training for synthetic data generation <ref type="bibr">(Tu et al., 2024b)</ref>, we implement an iterative data-generation framework targeted at curating highquality synthetic examples of clinical reasoning with web search use.</p><p>As depicted in the left panel of Figure <ref type="figure" target="#fig_1">2</ref>, we generate two reasoning paths, or CoTs, per training question: one without access to external information from search, and one that integrates search results as additional context during the CoT generation. Our self-training with search framework consists of the following key ingredients:</p><p>• Web search: For each question, we prompt Med-Gemini-L 1.0 to generate search queries whose results would help answer the medical question. We then pass the search queries to a web search API and retrieve search results.</p><p>• In-context demonstrations: For each type of reasoning response path, we hand-curate five expert demonstrations as seed with accurate clinical reasoning, explaining why the ground-truth answer is the best suited versus other potentially valid answers. For question examples with search results, the demonstrations explicitly refer to, and quote, the helpful information in the search results to best answer the question. • Generating CoTs: We prompt Med-Gemini-L 1.0 to generate CoTs using the in-context seed demonstrations over the train set. Before fine-tuning the model on the generated CoTs, we filter out the ones that lead to erroneous predictions. • Fine-tuning loop: After fine-tuning Med-Gemini-L 1.0 on the generated CoTs, the model's ability to follow the reasoning style and search integration of expert demonstrations improves.</p><p>We then use the improved model to re-generate the CoTs, and iteratively repeat this self-training process until the model's performance saturates.</p><p>Below we provide a MedQA-RS example of an input prompt, along with the retrieved search results and an example of a generated CoT, which is then used to further fine-tune Med-Gemini-L 1.0. For brevity, we only display one representative search result in the example below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction</head><p>You are a medical expert answering a multiple choice question about medical knowledge.</p><p>To help you answer the question, you are given access to search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>A 20-year-old woman presents with menorrhagia for the past several years. She says that her menses "have always been heavy", and she has experienced easy bruising for as long as she can remember. Family history is significant for her mother, who had similar problems with bruising easily. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty-guided search at inference</head><p>We design a novel, uncertainty-guided and iterative search process to improve Med-Gemini-L 1.0's generations at inference time. As displayed in the right panel of Figure <ref type="figure" target="#fig_1">2</ref>, each iteration consists of four steps: multiple reasoning path generation, uncertainty-based search invocation, uncertainty-guided search query generation, and finally search retrieval for prompt augmentation. Note that while uncertainty-guided search at inference could potentially benefit multimodal settings, we only apply this approach to text-only benchmarks and leave multimodal exploration for future work.</p><p>1. Multiple reasoning path generation: Given an input context prompt with a medical question, we generate multiple reasoning paths from Med-Gemini-L 1.0. For the first iteration, the prompt only consists of the instruction and question. For subsequent iterations, the prompt also includes search results from step (4) below. 2. Uncertainty-based search invocation: Given the multiple reasoning paths from step (1), we define an uncertainty measure based on the Shannon entropy of the answer choice distribution. Specifically, we calculate the probability of each answer choice by dividing its occurrence by the total number of responses, and apply the entropy based on the answer choice probabilities <ref type="bibr" target="#b46">(Horvitz et al., 1984)</ref>. High entropy (model responses are more uniform across the different answer choices) indicates a high epistemic uncertainty. If the uncertainty for a question is higher than a defined threshold, we perform the uncertainty-guided search process in steps (3) and (4); otherwise, the majority vote answer is returned as the final answer. 3. Uncertainty-guided search query generation: Given conflicting responses from step (1), we prompt Med-Gemini-L 1.0 to generate three search queries whose results would help resolve the conflict. Our motivation of conditioning on previously generated but conflicting responses is to retrieve search results that are directly targeted at resolving the model's uncertainty to the question. 4. Search retrieval: The generated queries are then submitted to a web search engine, and the retrieved results are incorporated into Med-Gemini-L 1.0's input prompt for the next iteration, starting back at step (1). Augmenting the prompt with search results enables the model to refine its response by considering external relevant insights obtained from web search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multimodal understanding via fine-tuning and customized encoders</head><p>To specialize Gemini's multimodal reasoning and conversational capabilities to the medical domain, we perform instruction fine-tuning of Gemini over a collection of domain-specific multimodal tasks following a similar procedure in prior works by <ref type="bibr">Tu et al. (2024a)</ref>, <ref type="bibr" target="#b142">Yu et al. (2022), and</ref><ref type="bibr" target="#b2">Alayrac et al. (2022)</ref>. We use eight multimodal tasks across six datasets as shown in Table <ref type="table" target="#tab_19">D1</ref>. A detailed description of the datasets is provided in the Appendix D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-to-text multimodal fine-tuning</head><p>We use four image-to-text datasets from MultiMedBench <ref type="bibr">(Tanno et al., 2024;</ref><ref type="bibr">Tu et al., 2024a)</ref> including Slake-VQA <ref type="bibr" target="#b70">(Liu et al., 2021)</ref>, Path-VQA <ref type="bibr" target="#b45">(He et al., 2020)</ref>, MIMIC-CXR <ref type="bibr">(Johnson et al., 2019a,b)</ref>, PAD-UFES-20 <ref type="bibr" target="#b94">(Pacheco et al., 2020)</ref>, in addition to the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" target="#b96">(Pelka et al., 2018)</ref>. Slake-VQA and Path-</p><p>VQA include both open-ended and close-ended visual question answering tasks in radiology and pathology, respectively. ROCO contains radiology image captioning tasks spanning multiple imaging modalities including computed tomography (CT), ultrasound, X-ray [chest X-ray (CXR), fluoroscopy, mammography, angiography], positron emission tomography (PET) and magnetic resonance imaging (MRI). PAD-UFES-20 is a domain specific dataset with diagnostic labels and patient clinical information designed for dermatology image classification. MIMIC-CXR is a radiology dataset comprised of CXRs, their corresponding text reports, and a set of discrete labels that denote the presence of 13 abnormal radiological conditions derived using the CheXpert labeler <ref type="bibr" target="#b51">(Irvin et al., 2019</ref>) (e.g., pneumonia). We use this dataset to formulate CXR report generation and image classification tasks for fine-tuning. For each task, we fine-tune Gemini 1.5 Pro by providing task-specific instructions as shown in Figure <ref type="figure" target="#fig_0">D1</ref>.</p><p>The mixture ratio for each task is approximately proportional to the number of training samples in each dataset. The resulting model is Med-Gemini-M 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmenting health signal modalities with new modality encoders</head><p>We anticipate that integrating various health-related signals will significantly enhance medical models and treatment decisions. These signals include data from consumer wearables (e.g., long-term heart rate measurements, activity levels), genomic information, nutritional data (e.g., images of meals), and environmental factors (e.g., air quality measurements). As a proof-of-concept, we expand Med-Gemini's capability to process raw biomedical signals. Specifically, we develop Med-Gemini-S 1.0 by augmenting Gemini 1.0 Nano with a specialized encoder using a cross-attention mechanism based on Flamingo <ref type="bibr" target="#b2">(Alayrac et al., 2022)</ref> to answer questions directly taking a raw 12-channel electrocardiogram (ECG) waveform as input. We use a subset of labeled ECG examples from the ECG-QA dataset <ref type="bibr">(Oh et al., 2023)</ref> and formulate the task as close-ended question answering with the instruction shown in Figure <ref type="figure" target="#fig_0">D1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Long-context processing via instruction prompting and chain-of-reasoning</head><p>Many applications in medicine require the analysis of a large amount of information and the expertise to identify subtle details of the domain. As introduced before, Gemini models have breakthrough long-context capabilities. We assess medically-relevant long-context performance for Med-Gemini-M 1.5 by meaningfully processing large amounts of fine-grained information for two different medical applications: a "needle-in-a-haystack" retrieval task from lengthy EHR notes and records; and tasks requiring understanding of medical videos. We describe various prompting strategies and chain-ofreasoning to enable accurate recall and reasoning of information.</p><p>Chain-of-reasoning for long EHR understanding Searching and retrieving clinically-relevant information from long EHR notes and records is a common and important task in patient care but must be performed with high precision and recall to enhance clinician efficiency and reduce workload <ref type="bibr" target="#b30">(Ford et al., 2016;</ref><ref type="bibr" target="#b54">Jensen et al., 2012)</ref>. Clinicians frequently curate a summary of their patient's historical conditions, symptoms, or procedures (the "problem list"), which can be timeconsuming and challenging for individuals with lengthy medical records. Difficulty arises with multiple factors hindering effective information retrieval in EHRs.</p><p>Firstly, classic query expansion and matching mechanisms encounter limitations due to textual similarities between conditions with similar taxonomies and the diverse information models used in EHRs (e.g. "Miller" vs. "Miller Fisher syndrome", "Diabetic nephropathy" vs. "Diabetes mellitus").</p><p>Vocabulary inconsistency in and between EHR systems presents issues including variations in how medical terms are encoded, such as acronyms ("rx" vs. "prescription"), misspellings, or synonyms for the same condition. Secondly, EHRs often contain heterogeneous data structure such as a checkliststyle data template: "[ ] cough <ref type="bibr">[x]</ref> headache", where a mention does not always indicate the presence of a medical condition. Thirdly, the context of a mention influences its interpretation. For example, the mention of the same condition in a patient's "Family History" compared to their "Past Medical History" could have different interpretations and implications for the patient's care. Lastly, polysemous acronyms in medical notes can lead to misinterpretations. These challenges motivate the need for AI systems to address the task of context-aware retrieval of subtle or rare conditions, medications, or procedure mentions from long EHR records -a practical benchmark for evaluating the utility of Med-Gemini in medicine. We setup the long-context EHR understanding task based on our prior work <ref type="bibr" target="#b27">(Feder et al., 2022)</ref>, where we curate a set of long and challenging EHR cases from MIMIC-III <ref type="bibr" target="#b57">(Johnson et al., 2016)</ref>, and formulate a subtle medical problem (condition/symptom/procedure) search-retrieval task over a collection of EHR notes and records, mimicking a clinically-relevant "needle-in-a-haystack" (Gemini Team, Google, 2024) problem. Details of the dataset and task curation procedure are described in Appendix E.1 and Section 3.3.</p><p>To assess the long-context retrieval and reasoning capability of Med-Gemini-M 1.5, we aggregate the EHR notes across multiple visits from a single patient in each example and utilize the long-context window of the model with a two-step chain-of-reasoning approach (using only in-context learning). In the first step, we prompt Med-Gemini-M 1.5 to retrieve all mentions (snippets of evidence) related to the given problem (condition/symptom/procedure) with a one-shot demonstration. In the second step, we further prompt Med-Gemini-M 1.5 to determine the presence of the given problem entities based on the mentions retrieved. Details of the instruction prompts are shown in Figure <ref type="figure" target="#fig_9">8</ref> and Section 3.3.</p><p>We use our prior heuristic-based annotation-aggregation method <ref type="bibr" target="#b27">(Feder et al., 2022)</ref> as a baseline method for comparison with Med-Gemini-M 1.5. This heuristic-based method requires an extensive effort of manual feature engineering to determine the existence of a problem (condition/symptom/procedure) from a set of medical records. It is an ontology-dependent multiple-step process, which includes an annotation step that labels the problem in each EHR note, a rule-based selection step that selects mentions of problem entities with high confidence, and another rule-based aggregation step that aggregates all selected problem mentions to reach a final conclusion. Note that the manually crafted aggregation rules can only provide a limited coverage of all possible conditions, and therefore it requires additional engineering effort to expand coverage to new conditions.</p><p>To curate a "needle-in-a-haystack" evaluation benchmark, we select medical conditions from a collection of EHR records with only one evidence snippet found in the aggregation step. We note that a mention of a condition in the EHR does not always mean the patient has that condition. This task enables us to assess Med-Gemini-M 1.5's ability to identify rarely documented and subtle conditions, symptoms, and procedures and reason accurately and holistically regarding their existence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction prompting for medical video understanding</head><p>The understanding of surgical and procedural videos is a highly active research topic in medical AI. The advancing frontier of computer vision in semantic segmentation, object detection and tracking, and action classification has enabled new clinical applications such as surgical phase recognition, tool detection and tracking, and even surgical skill assessment <ref type="bibr" target="#b39">(Goodman et al., 2024)</ref>.</p><p>Limited model context windows have hindered the ability for vision-language models to capture long-range dependencies and complex relationships within videos. Gemini's long-context capability offers a potential breakthrough for medical video understanding. By processing a whole video input, Med-Gemini-M 1.5 is able to identify visual patterns and understand actions and relationships between events across extended time frames.</p><p>To enable Med-Gemini-M 1.5 to understand medical videos, we employ zero-shot prompting with task-specific instructions as shown in Figure <ref type="figure" target="#fig_10">10</ref>, Figure <ref type="figure">9</ref>, and Figure <ref type="figure" target="#fig_11">11</ref>. The goal is to enable the model to analyze the language query and video content, and perform the given task related to the input medical video-either localizing the relevant visual segment matching the query for the medical visual answer localization (MVAL) task <ref type="bibr" target="#b43">(Gupta et al., 2023)</ref>, or identifying the surgical view in the video frames for the Critical View of Safety (CVS) assessment task <ref type="bibr" target="#b106">(Ríos et al., 2023;</ref><ref type="bibr" target="#b116">Strasberg and Brunt, 2010)</ref>. More details on the medical video datasets and evaluation metrics are described in Appendix E.1 and Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>We present evaluation benchmarks spanning (1) text-based reasoning, (2) multimodal, and (3) long-context processing tasks, demonstrating Med-Gemini's performance across a wide range of capabilities in medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation of advanced reasoning on text-based tasks</head><p>We evaluate the medical reasoning capability of Med-Gemini-L 1.0 on three text benchmarks assessing clinical reasoning and the ability to retrieve information using web search to reduce uncertainty:  <ref type="bibr" target="#b47">(Hou and Ji, 2023)</ref>.</p><p>For MedQA, we follow the input-output format, and the evaluation method as described in Singhal et al. (2023a) using prediction accuracy as the metric. At inference, we go through four iterations of uncertainty-guided search. Additionally, we ask board-certified primary care physicians (PCPs) from the US to relabel the MedQA test set. This enables us to identify questions with missing information such as plots or figures, labeling errors, and other potentially ambiguous questions with multiple possible correct answers <ref type="bibr" target="#b117">(Stutz et al., 2023)</ref>. Overall, this allows us to better characterize our performance on MedQA (USMLE). More details on this rating task can be found in Appendix C.2. NEJM CPC evaluation is an open-ended diagnosis task. The input is a text-based, challenging clinico-pathological case (CPC) report, and the output is a differential diagnosis list, comprising 10 potential diagnoses. We use the top-1 and top-10 accuracy of identifying the correct diagnosis of the given challenging case, and use the same prompting procedures following <ref type="bibr" target="#b79">McDuff et al. (2023)</ref>. At inference, we go through one iteration of uncertainty-guided search.</p><p>GeneTuring consists of 12 modules, each containing 50 open or close-ended QA pairs. We use the prediction accuracy as the evaluation metric, where the evaluation method and scoring technique for each module follow the methods described in <ref type="bibr" target="#b47">Hou and Ji (2023)</ref>. In particular, we exclude from numerical evaluation, cases where the model outputs either do not directly answer or acknowledge limitations (i.e., abstained). At inference, we again go through only one iteration of uncertainty-guided search similar to NEJM CPC evaluation.</p><p>Beyond these benchmarks, we further evaluate Med-Gemini-M 1.0 on three challenging use cases that require long-form text generation. To this end, we conduct an expert evaluation where a panel of clinicians compare the responses of our model to those of other human experts via a side-by-side blinded preference comparison (more details are provided in Appendix C.4):</p><p>• Medical summarization: Generate an after-visit summary (AVS) given de-identified history and physical (H&amp;P) notes. An AVS is a structured report that patients receive at the end of a medical appointment to summarize and guide their care journeys. • Referral letter generation: Generate a referral letter to another healthcare provider given a de-identified outpatient medical note that contains a recommendation for a referral. • Medical simplification: Generate a plain language summary (PLS) given a technical abstract from a medical systematic review. A PLS should be written in plain English which can be understood by most readers without a university education <ref type="bibr" target="#b18">(Cochrane, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation of multimodal capabilities</head><p>We evaluate Med-Gemini on seven multimodal visual question answering (VQA) benchmarks. For in-distribution evaluation, we choose four medical specialty datasets used in the instruction fine-tuning of Med-Gemini: PAD-UFES-20 (dermatology), Slake-VQA (radiology in English and Chinese) and Path-VQA (pathology) for Med-Gemini M 1.5, and ECG-QA (cardiology) for Med-Gemini S 1.0.</p><p>We also include three cross-specialty benchmarks for measuring out-of-box performance of Med-Gemini: NEJM Image challenge, USMLE-MM (multimodal), and MMMU-HM (health and medicine) datasets. These datasets are not used in any training or fine-tuning process. For this, we focus our evaluation on the Med-Gemini-L 1.0 model without any multimodal finetuning.</p><p>Its worth noting that PAD-UFES-20, NEJM Image Challenge, USMLE-MM datasets, and most questions in MMMU-HM are close-ended VQA, i.e., multiple-choice question in a VQA setup. An overview of the selected datasets is presented in Table <ref type="table" target="#tab_4">D2</ref> and more details are in Appendix D.1 and D.2.</p><p>We report prediction accuracy for all the close-ended multiple-choice VQA tasks, including NEJM Image Challenge, USMLE-MM, and PAD-UFES-20 6-class skin condition classification. We also follow the evaluation setup in <ref type="bibr" target="#b143">Yue et al. (2023)</ref> to report accuracy for MMMU-HM. We use the exact-match accuracy for ECG-QA following <ref type="bibr">Oh et al. (2023)</ref>. For the open-ended VQA tasks (Slake-VQA and Path-VQA), we use the token-level F1 score following <ref type="bibr">Tu et al. (2024a)</ref>.</p><p>We further showcase Med-Gemini-M 1.5's multimodal capability in multimodal medical diagnostic dialogue in two specialities -dermatology and radiology <ref type="bibr">(Tu et al., 2024b)</ref> -with qualitative evaluation of the example dialogues by attending expert clinicians in these specialties. We note that these demonstrations indicate the "art of the possible", but that extensive further research and validation would be required before the consideration of deployment for a safety-critical use-case such as diagnostic assistance to a clinician.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation of long-context capabilities on video and EHR tasks</head><p>We consider three tasks to demonstrate Med-Gemini-M 1.5's ability to seamlessly understand and reason over long context medical information (Table <ref type="table" target="#tab_21">E1</ref>, details in Appendix E.1):</p><p>• Long unstructured EHR notes understanding • Medical instructional video QA • Critical view of safety (CVS) assessment of surgical video Long EHR understanding For the long-context EHR understanding task, we curate a MIMIC-III-Needle-in-a-Haystack task where the goal is to retrieve the relevant text spans of any mention of a given medical problem (condition/symptom/procedure) over a large collection of clinical notes in EHR and determine the existence of the condition by reasoning across the retrieved evidence. Specifically, we curate 200 examples where each example consists of a collection of de-identified EHR notes selected from 44 unique ICU patients with a long medical history based on the following criteria:</p><p>• Patients with long records: more than 100 medical notes (excluding structured EHR data). The length of each example ranges from 200,000 to 700,000 words. • In each example, the condition is mentioned only once across the collection of all EHR notes.</p><p>• Each sample has a single condition of interest.</p><p>The ground-truth label of each sample is a binary variable indicating whether a given problem entity of interest is present or not, obtained from the majority vote of three physician raters. Across the 200 test examples, the number of positive cases and negative cases are 121 and 79, respectively.</p><p>We compare Med-Gemini-M 1.5's one-shot in-context learning performance against the heuristicbased annotation-aggregation baseline method <ref type="bibr" target="#b27">(Feder et al., 2022)</ref> in terms of precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video understanding</head><p>We quantitatively evaluate Med-Gemini-M 1.5's long-context performance in the setting of video question-answering using three medical video tasks: two medical visual answer localization (MVAL) tasks using the Medical Instructional Video QA (MedVidQA) dataset <ref type="bibr" target="#b43">(Gupta et al., 2023)</ref>, and the critical view of safety (CVS) assessment task on the Cholec80-CVS dataset <ref type="bibr" target="#b106">(Ríos et al., 2023;</ref><ref type="bibr" target="#b123">Twinanda et al., 2016)</ref>.</p><p>The goal of MVAL is to identify specific video segments based on natural language descriptions (queries) given a video input. For MVAL, we benchmark the test set of MedVidQA for two video span prediction tasks, one using both the video input and subtitle text and the other one with only the video inputs. We follow <ref type="bibr" target="#b43">Gupta et al. (2023)</ref>; <ref type="bibr" target="#b67">Li et al. (2022)</ref> using Intersection over Union (IoU) at the threshold of 0.3, 0.5, 0.7, and mean IoU (mIoU) as the evaluation metrics for the video span prediction tasks. IoU and mIoU are used to measure how much of the ground truth span overlaps with the predicted span.</p><p>We evaluate Med-Gemini-M 1.5's long-context capabilities in assessing the achievement of the Critical View of Safety (CVS) method in laparoscopic cholecystectomy (a keyhole operation to remove the gallbladder) videos. The CVS <ref type="bibr" target="#b116">(Strasberg and Brunt, 2010</ref>) is a recommended protocol used for secure identification of the cystic duct and cystic artery to minimize the risk of Bile Duct Injury (BDI), a significant injury associated with consequential postoperative morbidity and mortality, reduced long-term survival and impact on quality of life <ref type="bibr" target="#b133">(Way et al., 2003)</ref>. We evaluate the CVS assessment task on the public Cholec80 dataset <ref type="bibr" target="#b123">(Twinanda et al., 2016)</ref> and Cholec80-CVS <ref type="bibr" target="#b106">(Ríos et al., 2023)</ref> video clip annotations. Specifically, for each surgical video in the Cholec80 dataset, the Cholec80-CVS dataset provides annotations for video clips within the full video, where at least one CVS criteria is met. Each of those video clips is annotated with a score of 0, 1 or 2 for each of the three CVS criteria. All frames contained in a given video clip are considered to share the same annotation. We evaluate the model's ability to predict which of the CVS criteria are met based on the whole video clip. We then compute the average accuracy of the answer against the Cholec80-CVS annotations across 572 annotated video clips. More details on the CVS task can be found in Appendix E.1. Furthermore, to show the real-world capability of Med-Gemini-M 1.5 in capturing surgical actions in procedural videos, we qualitatively evaluate the surgical action recognition task using examples from the Annotated Videos of Open Surgery (AVOS) dataset <ref type="bibr" target="#b38">(Goodman et al., 2021)</ref>, a video collection of open surgical procedures uploaded to the YouTube platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>As introduced previously, we evaluate Med-Gemini's advanced reasoning, multimodal, and longcontext capabilities across a wide range of medical benchmarks, both quantitatively and qualitatively. The array and diversity of tasks considered in this work is to the best of our knowledge, the most comprehensive for medical LLMs. Further, our evaluations of Med-Gemini go beyond benchmarking of model capabilities and extend to tasks reflecting the potential for real-world utility, such as medical summarization, multimodal conversations, and surgical video understanding. OOD: out-of-distribution dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Med-Gemini demonstrates advanced reasoning on text-based tasks</head><p>As shown in Table <ref type="table">1</ref>, Med-Gemini-L 1.0 scores 91.1% accuracy on MedQA (USMLE), a new SoTA, outperforming our previous Med-PaLM 2, by 4.5%, and the recent results augmenting GPT-4 with complex, specialized prompting -MedPrompt <ref type="bibr" target="#b85">(Nori et al., 2023)</ref> by 0.9%. In contrast to MedPrompt, our principled approach leverages general web search in an uncertainty-guided framework that can be easily to extended to more complex scenarios beyond MedQA.</p><p>As proof of generalization of our search integration, on the NEJM CPC complex diagnostic challenges benchmark, Med-Gemini-L 1.0 surpasses our previous SoTA AMIE model (which itself is better than GPT-4) <ref type="bibr" target="#b79">(McDuff et al., 2023)</ref> by 13.2% on the top-10 accuracy as shown in Figure <ref type="figure" target="#fig_2">3a</ref>.</p><p>The same search strategy is also effective for genomics knoweledge tasks as shown in Table <ref type="table">1</ref>. Med-Gemini-L 1.0 outperforms the SoTA models reported in <ref type="bibr" target="#b47">Hou and Ji (2023)</ref> on seven GeneTuring modules including Gene name extraction, Gene alias, Gene name conversion, Gene location, Proteincoding genes, Gene ontology and TF regulation. We also compare model abstention across the 12 modules in Figure <ref type="figure" target="#fig_2">3b</ref>. It is worth noting that GeneGPT <ref type="bibr" target="#b56">(Jin et al., 2024)</ref> achieves higher scores through specialized web APIs, while our comparison focuses on prior models from <ref type="bibr" target="#b47">Hou and Ji (2023)</ref> that utilize general web search similar to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation analysis</head><p>To understand the impact of self-training and uncertainty-guided search on performance, we compare Med-Gemini-L 1.0's performance with and without self-training, along with varying number of rounds of uncertainty-guided search for MedQA (USMLE). As shown in Figure <ref type="figure" target="#fig_3">4a</ref>, Med-Gemini-L 1.0's performance improves considerably with self-training (a gain of 3.2% in accuracy), and improves with each round of search from 87.2% up to 91.1%. Similarly, for the NEJM CPC benchmark, Figure <ref type="figure" target="#fig_2">3a</ref> shows a 4.0% improvement for top-10 accuracy when we add search at inference. In Appendix C.3, we additionally show performance on NEJM CPC stratified by four specialities. Revisiting MedQA (USMLE) labels MedQA (USMLE) is a popular benchmark for assessing the capabilities of LLMs in the medical domain. However, some MedQA test questions have missing information such as figures or lab results, and potentially outdated ground-truth answers. To address these concerns, we conduct a complete relabeling of the MedQA (USMLE) test set. Specifically, we recruit at least three US physicians to re-annotate each question, asking them to answer the question and evaluate the provided ground-truth answer. We also ask them to identify if there was any missing information in the questions. Following <ref type="bibr" target="#b117">Stutz et al. (2023)</ref>, we characterize the questions to exclude due to missing information or label errors by bootstrapping votes from committees of three raters per question. We additionally identify ambiguous questions as those allowing multiple correct answers (more details can be found in Appendix C.2).</p><p>Figure <ref type="figure" target="#fig_3">4b</ref> shows that, on average across bootstrapped committees, 3.8% of questions include missing information, following the unanimous vote of bootstrapped committees. Additionally, 2.9% likely include label errors. Another 0.7% are ambiguous. Excluding these questions is supported by high inter-rater agreement of 94%, 87.6%, and 94.6%, respectively. Importantly, Med-Gemini-L 1.0's mistakes can be attributed disproportionately to these questions; our entropy-based uncertainty score also tends to be higher on these question (t-test, 𝑝-value=0.033). Filtering both types improves accuracy from 91.1% to 91.8% ± 0.2%. Using majority instead of unanimous votes further improves accuracy to 92.9% ± 0.38% by discarding up to 20.9% of the uncertain questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Performance on long-form medical text generation</head><p>Med-Gemini-M 1.0 demonstrates the ability to generate long-form text for three challenging real-world use cases -after-visit clinical summaries, doctor referral letter generation and medical simplification. In side-by-side comparisons, Med-Gemini-M 1.0's responses are considered as good or better than expert responses more than half the time by clinician raters across the three tasks (Figure <ref type="figure" target="#fig_4">5</ref>). For more task details, see Appendix C.4. Notably for the referral letter generation task, the model generated letters are preferred or tied with experts across all the samples evaluated.  The tasks considered include generation of after-visit summaries, referral letters and simplified summaries of medical systematic reviews. Evaluation was performed by clinician raters. P-values are used to denote whether the rate at which Med-Gemini-M 1.0 is preferred or tied with experts is 0.5 (two-sided t-test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Med-Gemini demonstrates multimodal understanding across diverse tasks</head><p>Our Med-Gemini models surpass, or perform competitively, with the state-of-the-art methods across seven medical multimodal benchmarks (See Table <ref type="table" target="#tab_4">2</ref>). We provide representative input and output examples for the multimodal tasks in Figure <ref type="figure" target="#fig_0">D1</ref> for illustration.</p><p>In particular, Med-Gemini-L 1.0 reaches SoTA on three out-of-distribution close-ended VQA tasks-NEJM Image Challenge, multimodal USMLE sample questions (USMLE-MM), and the health &amp; medicine subset of MMMU (MMMU-HM), outperforming GPT-4V by 8.7%, 13.1%, and 2.6%, respectively. Meanwhile, Med-Gemini-M 1.5 outperforms our previous multimodal models, Med-PaLM M <ref type="bibr">(Tu et al., 2024a)</ref> on Path-VQA by 2.0% in token F1 score, and Med-Gemini-S 1.0 outperforms the previous SoTA for ECG-QA (GPT-4 with SE-WRN) by 6.1% on macro-averaged accuracy across ECG question types <ref type="bibr">(Oh et al., 2023)</ref>. Med-Gemini-M 1.5 also performs competitively on Slake-VQA and PAD-UFES-20 compared to the previous SoTA method (Med-PaLM M) but does not reach SoTA. * denotes the performance of Med-Gemini-L 1.0, † denotes the performance of Med-Gemini-M 1.5, and ‡ denotes the performance of Med-Gemini-S 1.0.</p><p>Note that we have evaluated PAD-UFES-20 on two different data split setups. We first evaluate on the Med-PaLM M split (the image-level split) for a direct, fair comparison against the previous SoTA method. In addition, we also report our model's performance on a new split, which is a split at the patient level (Table <ref type="table" target="#tab_4">2</ref>).</p><p>For USMLE-MM, our model achieves accuracies of 89.5%, 92.9%, 100.0% for USMLE step 1 questions (n=19), step 2 (n=14), and step 3 (n=13), respectively.</p><p>In aggregate across these seven benchmarks, Med-Gemini improve over GPT-4V by an average relative margin of 44.5%. Note that for the USMLE-MM, PADS-UFES-20 and Slake-VQA datasets, we report reproduced GPT-4V results using public APIs and the same prompt used for the corresponding Med-Gemini model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Preview of multimodal dialogue capabilities</head><p>To extend beyond multimodal benchmarks, we demonstrate the potential for future real-world utility of Med-Gemini through hypothetical multimodal medical dialogues across two specialities.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> illustrates an out-of-distribution setting where the dermatology image comes from a dataset <ref type="bibr" target="#b132">(Ward et al., 2024)</ref> not used in the multimodal fine-tuning mixture. The user first asks Med-Gemini-M 1.5 about itchy lumps on their legs and arms; our model then asks the user to share an image of the lumps; after the user provides the image of their suspicious lesion, the model asks a follow-up question and continues to provide a correct diagnosis of prurigo nodularis, and recommends next steps and potential treatment options.</p><p>In Figure <ref type="figure" target="#fig_8">7</ref>, we show a radiology dialogue example with the example image coming from the test set of the MIMIC-CXR dataset. Med-Gemini-M 1.5 demonstrates the ability to interact with a primary care provider to analyze a CXR, identify degenerative disk disease, discuss the differences between causation and correlation with a patient history of back-pain, suggest follow-up investigations to establish the cause of back pain and provide a report using non-technical language to facilitate patient understanding and communication. We observe some variability in Med-Gemini-M 1.5's responses depending on the prompt (for instance, for some prompts the report will not list mild degenerative changes especially if prompted to focus on some other anatomical feature). A full quantification of Med-Gemini-M 1.5's multimodal dialogue capability and variability is beyond the scope of this work, but nonetheless these qualitative examples illustrate Med-Gemini-M 1.5's ability to support conversations about medical knowledge grounded on multimodal sources, a potentially useful attribute for applications considering user-AI and clinician-AI interaction. Real-world exploration of these use-cases would require considerable further development and validation to build upon these signs of early promise.</p><p>For the past few weeks, I have noticed multiple itchy lumps on my legs and arms. The lumps are solid and don't contain fluid. They are very itchy and I have been waking up at night scratching them. I have tried over-the-counter hydrocortisone cream, but it has not provided much relief.</p><p>Here it is.</p><p>Have you recently been exposed to any new plants, animals, or medications?</p><p>No, I don't think so. This looks like prurigo nodularis. It is a skin condition that causes hard, itchy lumps to form on the skin. The itching can be intense, especially at night. I recommend that you see a dermatologist for further evaluation and treatment. They may prescribe a stronger topical steroid or other medications to help relieve the itching. In the meantime, you can try to avoid scratching the lumps, as this can make them worse Thanks! Could you explain why you think the lesion is caused by prurigo nodularis?</p><p>The lesion appears as a firm, dome-shaped nodule with a hyperkeratotic surface, which is consistent with prurigo nodularis. Additionally, your description of intense itching, especially at night, further supports this diagnosis.</p><p>However, a definitive diagnosis can only be made by a dermatologist through a physical examination and potentially a skin biopsy. Okay, will you advise me on how to treat it?</p><p>Unfortunately, there is no cure for prurigo nodularis, but there are several treatment options that can help manage the symptoms and improve quality of life, including:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topical medications:</head><p>-High-potency corticosteroids: These creams or ointments can help reduce inflammation and itching. -Capsaicin cream: This cream can help desensitize the skin and reduce itching. -Topical calcineurin inhibitors: These medications can help suppress the immune response and reduce inflammation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oral medications:</head><p>-Antihistamines: These medications can help reduce itching. -Gabapentin or pregabalin: These medications can help reduce nerve pain and itching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other treatments:</head><p>-Phototherapy: This treatment involves exposing the skin to ultraviolet light, which can help reduce inflammation and itching. The user interacts with the our multimodal model, Med-Gemini-M 1.5, acting as a patient based on a case from SCIN <ref type="bibr" target="#b132">(Ward et al., 2024)</ref>, an external dataset that is not included in the fine-tuning mixture. This system would not be appropriate for use in real-world diagnostic tasks without considerable further research and development. Nonetheless, this example shows indicatively the future initial promise of how the combination of the conversational capability inherited from the original Gemini model and the newly acquired multimodal medical knowledge from fine-tuning can potentially enable attributes of a rich multi-turn diagnostic dialogue. In this interaction, Med-Gemini-M 1.5 asks for a picture when it is not provided (multimodal information acquisition), arrives at the right diagnosis efficiently (open-ended diagnosis), explains the reasoning by integrating the relevant visual features and other gathered patient symptoms (explainability), answers questions about treatment options while deferring aptly to experts for the final decision. (b) Shows feedback collected from dermatologists to evaluate qualitative the contents of the diagnostic dialogue, specifically asking them to comment on both positive and negative aspects. What evidence do you see for degenerative changes?</p><p>Hello, I am a primary care physician and this is a chest x-ray for a patient of mine. The formal radiology report is still pending and I would like some help to understand the x-ray. Please write a radiology report for me:</p><p>My patient has a history of back pain. Might this explain the back pain?</p><p>While the degenerative changes seen on the x-ray could potentially contribute to your patient's back pain, it's important to note that correlation doesn't equal causation. Many people have degenerative changes in their spine without experiencing pain, and conversely, back pain can have numerous causes beyond just degenerative changes.</p><p>Therefore, while the x-ray findings are worth noting, they don't definitively explain your patient's back pain. A comprehensive evaluation, including a detailed history, physical examination, and potentially additional imaging or tests, would be necessary to determine the exact cause of their pain.</p><p>Please let me know if you have any other questions.</p><p>To help me communicate all of this to my patient, please rewrite the report without medical terminology:</p><p>Chest X-ray Results:</p><p>• Heart: Normal size.</p><p>• Lungs: Clear, no signs of infection or fluid buildup.</p><p>• Bones: Minor wear and tear in the lower spine, common with age.</p><p>Overall: No signs of serious problems in the chest. The wear and tear in the lower spine might be related to your back pain, but other factors could also be involved. We'll need to do more tests to figure out the exact cause of your pain. Thank you. That is useful while I wait for the radiologists report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth report</head><p>You're welcome! I'm glad I could help. If you have any further questions, feel free to ask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pros</head><p>• "Realistic hypothetical demonstration of interaction with a healthcare provider in a clinical imaging context." • "Demonstration of high-quality realistic report generation."</p><p>• "Showcases general medical knowledge and also provides adequate limitations (i.e., mentioning additional tests needed)."</p><p>Cons:</p><p>• "Small osteophytes are present in the image, but along the lateral margins (rather than anterior margins) of the vertebral bodies." • "For optimal clarity, the rewritten 'Chest X-ray Results' report, which mentions the 'lower spine' and avoids medical terminology, should specify the lower thoracic spine." In this interaction, Med-Gemini-M 1.5 demonstrates its ability to analyze a chest X-ray (CXR) and conduct a hypothetical realistic dialogue with a primary care physician. As above, Med-Gemini-M 1.5 is not suitable for this real-world use without further research. However, this example demonstrates initial promise, wherein Med-Gemini-M 1.5 identifies mild degenerative changes along the spine and can answer questions about the reasoning which led to this finding, demonstrate general medical knowledge about degenerative disk disease and distinguish between correlation and causation in relation to a patient history of back-pain. Finally, in this example Med-Gemini-M 1.5 is able to explain its findings in layperson's terms, demonstrating its potential for facilitating patient understanding and communication in clinical settings. The ground truth report for this CXR is provided. (b) Feedback from a radiologist about the quality of this radiology dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Med-Gemini shows long-context processing capability on long EHR and video tasks</head><p>Finally, we evaluate the long-context capability of Med-Gemini-M 1.5 via the "needle-in-a-haystack" medical condition retrieval task from long EHRs as well as three medical video tasks (two MAVL and one CVS assessment of surgical videos).</p><p>We demonstrate the utility of Med-Gemini-M 1.5 on the correct identification of rare and subtle problem entity (condition/symptom/procedure) in long EHR notes. The average precision and recall between Med-Gemini-M 1.5 and the baseline method are shown in Table <ref type="table" target="#tab_6">3</ref> (confidence intervals in Table <ref type="table" target="#tab_4">E2</ref>). Encouragingly, we observe that Med-Gemini-M 1.5's one-shot ability is on-par with a carefully-tuned heuristic-based annotation-aggregation baseline approach, which is highly taskdependent. The in-context learning capability of Med-Gemini-M 1.5 to process long documents or records can easily generalize to novel problem settings without the need of extensive manual engineering. We provide an illustrative example of the prompt used, along with our model's response in Figure <ref type="figure" target="#fig_9">8</ref>. We attempt to benchmark GPT-4 on this task but the average context token length in this dataset significantly exceeds the maximum context window supported in the public APIs.  We mark these tasks and benchmarks as out-of-distribution (OOD) because all the evaluation here is only with in-context learning and no fine-tuning Med-Gemini-M 1.5 also achieves SoTA performance on two MedVidQA MAVL tasks (one using both video and subtitles and the other being video only), outperforming the non-LLM based SoTA models which require considerable be-spoke tuning. We note that 7 questions in MedVidQA are not answerable due to YouTube video access (private, removed). Our results therefore are reported based on the remaining 148 questions. Details are shown in Table <ref type="table" target="#tab_6">3</ref>. We provide an illustrative example of the prompt used, along with our model's response in Figure <ref type="figure" target="#fig_10">10</ref>. While evaluating MedVidQA, we also observe that the embedded captions can significantly aid the model's understanding. Future research could explore how to optimize the use of multimodal video data, including images, text, and audio, for further improvements in video understanding. We attempt to benchmark GPT-4V on these tasks but once again run into context length limitations for most of the videos using the public APIs.</p><p>For the CVS assessment of the laparoscopic cholecystectomy video task, Med-Gemini-M 1.5 outperforms GPT-4V by 21%. However, we observe that the supervised baseline using a ResNet3D architecture performs better. Further investigations on prompting strategies or instruction fine-tuning may be required to improve the task performance of our models. We provide an illustrative example of the prompt used, along with our model's response in Figure <ref type="figure">9</ref>. Step 2 (Deciding the existence): Med-Gemini-M 1.5 then evaluates the relevance of each retrieved mention, categorizing them as explicit confirmation, strong indication, or relevant mention of hypothermia. Based on this analysis, the model concludes that the patient does have a history of hypothermia, providing clear reasoning for its decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Applications of long-context capabilities in biomedicine</head><p>In addition to quantitative results, we further preview the potentials of the long-context capabilities in medical education, facilitating clinician interaction with EHR systems and biomedical literature review and summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedural video in clinical practice and education</head><p>In Figure <ref type="figure" target="#fig_11">11</ref>, we qualitatively preview Med-Gemini-M 1.5's ability to identify surgical actions from a video in the AVOS dataset. This ability holds potential for surgical care, promising to enhance surgical training through automated assessment, optimize operating room efficiency by analyzing workflows, and potentially guide surgeons in real-time during complex procedures for improved accuracy and patient outcomes. In Figure <ref type="figure" target="#fig_12">12</ref>, we additionally present an example of Med-Gemini-M 1.5's long-context capabilities on surgical video dialogue where the model analyzes a video clip comprising footage from a laparoscopic cholecystectomy. Med-Gemini-M 1.5 demonstrates its ability to analyze the video and conduct a dialogue with a student that might be learning about the procedure. These promising abilities have the potential to provide useful assistive tools for clinicians, perhaps improving patient safety or enhancing the process of medical training through educational aids or automated in-procedure assistance and guidance. The model correctly informs the user that they are observing a laparoscopic cholecystectomy and refers correctly to the key structures underlying the "critical view of safety". These classification tasks, if performed scalably with high accuracy, could enable better audit of procedures (for example for quality assurance), or even prospective efficiency gains from anticipation of operative stages. For more ambitious goals such as benefits to education, operative guidance or patient safety, significant further work would need to be performed to assess more nuanced and complex capabilities. For example, we did not test Med-Gemini's ability to accurately segment or highlight physical structures in the video and ground the dialogue with the relevant anatomy; or retrieve and present useful educational assets like diagrammatic representations of the displayed anatomy or guides to key operative stages. For uses such as education, pedagogical dialogue objectives would also likely be of considerable importance. Further work should explore these and other exciting new capabilities in a wider range of settings for procedural video, which is increasingly common in medicine.</p><p>Clinician dialogue with EHR In Figure <ref type="figure" target="#fig_2">13</ref>, we demonstrate that Med-Gemini-M 1.5 effectively parses extensive medical records, synthesizing them into clear, concise summaries of active and historical conditions. Moreover, users can initiate conversations based on this summarized data, requesting more granular details from the records. Our example shows how this might include a user making natural language inquiries about specific conditions (like pneumonia) or associated diagnostic findings (such as CXR results). By streamlining access to long-form medical data and presenting the interaction in a conversational interface, this capability has the potential to significantly reduce cognitive load for clinicians and patients alike, potentially enhancing the efficiency and understanding of complex medical information without compromising staff well-being. To deliver upon this potential in real-world use would require considerable additional evaluation and research. As just one example, it would be necessary to closely examine the incidence of clinically-significant errors in retrieval or generation from grounded content; and to proactively measure and mitigate issues in dataset and model bias (as we discuss further below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biomedical research</head><p>In Figure <ref type="figure" target="#fig_14">14</ref>, we demonstrate Med-Gemini-M 1.5's ability to process multiple research articles concerning a specific genetic locus (FTO) and its association with obesity <ref type="bibr" target="#b73">(Loos and Yeo, 2022)</ref>. In this real-world application, Med-Gemini-M 1.5 successfully comprehends the information presented in current research (full content of 12 pre-curated research papers in portable document format) and compiles a concise summary for the user. The FTO locus we demonstrate in this example (a region of BMI-and obesity-associated variants within the gene FTO) is a classic example of a mechanistically understood genome-wide association studies (GWAS) hit. In this exemplar, the mechanism is a relatively complex multistep process which took extensive research to pinpoint-it involves variants altering the binding of a transcriptional repressor within an intronic super-enhancer region of the FTO gene, thereby leading to overexpression of two other genes, which ultimately promotes lipid accumulation <ref type="bibr" target="#b17">(Claussnitzer et al., 2015;</ref><ref type="bibr" target="#b64">Laber et al., 2021)</ref>.</p><p>We evaluate Med-Gemini-M 1.5's ability to parse a large collection of academic papers on the FTO locus and provide a succinct and accessible description of the mechanistic link between FTO and obesity, together with a list of concrete supporting experimental results. As seen in Figure <ref type="figure" target="#fig_14">14</ref>, the model provides a concise, informative, and accurate description of how the FTO locus contributes to obesity biology and presents it in a clear and digestible manner. Improvement can be made by the model listing other well-studied variants in high linkage equilibrium with rs1421085, and by providing references of where each piece of information originated from. This example shows how Med-Gemini-M 1.5's long-context capability has clear potential to reduce cognitive load for genomic researchers and clinicians, enhancing their access to the latest findings regarding gene-disease associations; and the potential has broad relevance in other domains of biomedical and scientific research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response</head><p>CVS Assessment: C1 -partially achieved: Two tubular structures are visible but the view is not entirely clear due to some bleeding obscuring the area. C2 -not achieved: The hepatocystic triangle is not clearly dissected, and there is not an unimpeded view of only the two cystic structures and the cystic plate. C3 -not achieved: The lower third of the gallbladder is not dissected off the cystic plate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>You are a helpful medical video assistant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task:</head><p>Assess whether Critical View of Safety (CVS) is fully achieved in the provided cholecystectomy video. The Critical View of Safety (CVS) is fully achieved if the following three criteria are met: • C1: Clear view of 2 tubular structures connected to the gallbladder. • C2: A carefully dissected hepatocystic triangle presenting an unimpeded view of only the 2 cystic structures and • the cystic plate.</p><p>• C3: The lower third of the gallbladder is dissected off the cystic plate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions:</head><p>1. Watch the video carefully, and answer which of the Critical View of Safety (CVS) criteria are met. 2. Output Format:</p><p>• CVS Assessment: ○ C1 -achieved / partially achieved / not achieved (Optional explanation) ○ C2 -achieved / partially achieved / not achieved (Optional explanation) ○ C3 -achieved / partially achieved / not achieved (Optional explanation)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… …</head><p>Figure <ref type="figure">9</ref> | Example of Med-Gemini-M 1.5's long-context capabilities on a surgical video. Med-Gemini-M 1.5 analyzes a video clip from the Cholec80 dataset to assess achievement of the Critical View of Safety (CVS) during a laparoscopic cholecystectomy (a keyhole operation to remove the gallbladder). The model assesses whether the three criteria defining the achievement of the CVS are met, with a detailed explanation for each criterion separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>You are a helpful medical video assistant.</p><p>Task: You are given a video, and a corresponding subtitle with start time and duration, followed by a question. Your task is to extract the precise video timestamps that answer the given question below.</p><p>Instructions: Provide one single timestamp that spans the entire length of the answer while considering the entire video. It is better to be exhaustive and providing the longest time span for the answer.</p><p>Question: How to relieve calf strain with foam roller massage? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>You are a helpful medical video assistant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task:</head><p>Annotate the surgical actions in the provided video, segmenting it into 1-second intervals. Assign each segment to one of the following classes:</p><p>• Cutting: If the surgeon is using a tool like scissors, scalpel, knife, or electrocautery device to cut or dissect tissues.</p><p>• Tying: If the surgeon is using their hands or needle holders to create secure knots.</p><p>• Suturing: If the surgeon is closing an open wound with a needle but not creating secure knots.</p><p>• Background: If no surgical action is being performed, including actions like using forceps, clamps, retractors, or dilators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions:</head><p>1. Watch the video carefully, segmenting it into 1-second intervals. 2. For each segment, identify the primary surgical action being performed and assign the appropriate class.</p><p>3. If no surgical action is performed in a segment, label it as "background" and provide a brief description of what is happening (e.g., "adjusting retractors"). Med-Gemini-M 1.5 analyzes a video from the Surgical Action Recognition (AVOS) dataset to annotate surgical actions in a video. It segments the video and assigns each segment a label based on the surgical action being performed (e.g., cutting, tying, suturing) or classifies it as background if no action is taking place. The ground truth AVOS annotation of this example is: 00:00-00:11: Background, 00:12-03:05: Cutting, 03:05-03:15: Background, 03:15-04:02: Suturing.</p><p>Hello, I am a medical student analyzing this surgical procedure video for my studies. Which surgical procedure is shown in this video?</p><p>The surgical procedure shown in this video is a laparoscopic cholecystectomy -the surgical removal of the gallbladder. This is a common procedure and is often performed when gallstones cause pain or complications, such as inflammation of the gallbladder (cholecystitis).</p><p>Which tools and how many are being used by the surgeon?</p><p>The surgeon in this video clip is using two tools: Laparoscopic Grasper: A long, thin instrument with a handle on one end and a grasping mechanism on the other, which can be opened and closed to hold tissues, and manipulate organs and other structures. Hook Electrocautery: A long thin instrument with a handle on one end and a hook-shaped tip on the other. The hook shape allows the tool to dissect, cut and cauterize tissue. Cauterizing tissue helps prevent bleeding during surgery and seals off blood vessels and lymphatic ducts to minimize the risk of leaks.</p><p>Which anatomical structures are visible?</p><p>The following anatomical structures are visible:</p><p>Liver: The large, reddish-brown organ in the upper right portion of the abdomen. Gallbladder: The small, pear-shaped organ located below the liver. The gallbladder is grasped and retracted by forceps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cystic duct:</head><p>The thin, tube-like structure that connects the gallbladder to the common bile duct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common bile duct:</head><p>The larger duct visible below the cystic duct that carries bile from the liver and gallbladder to the small intestine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cystic artery:</head><p>The small artery that supplies blood to the gallbladder, it is visible alongside the cystic duct.</p><p>Which specific surgical phase is shown this video clip?</p><p>The specific surgical phase shown in this video clip is the dissection and isolation of the cystic duct and artery. This phase comes after the initial access to the abdominal cavity and visualization of the gallbladder, which can be seen retracted superiorly by forceps, and before the clipping and dividing of the cystic duct and artery.</p><p>The surgeon is using a laparoscopic dissector to carefully separate the cystic duct and cystic artery from the surrounding tissue. This is a critical step as it allows the surgeon to clearly identify and isolate these structures before they are clipped and divided and can help to prevent injury to the common bile duct and hepatic artery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pros</head><p>• "Impressive understanding of which surgical procedure is being performed, which specific phase in the procedure this video clip is taken from, which tools are being used, and which anatomical structures are visible." • "Correctly identifies the risks in the current video clip, which is injuring the common bile duct, and correctly suggests taking the safety measures, highlighting the Critical View of Safety method first."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cons</head><p>• "The surgical procedure shown in this video is a laparoscopic cholecystectomy -the surgical removal of the gallbladder." --this describes "cholecystectomy", but does not refer to the "laparoscopic" part -that the procedure is performed "keyhole".</p><p>• "Hook Electrocautery: The hook shape allows the tool to dissect, cut and cauterize tissue" --this section does not explain what electrocautery itself actually is. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head><p>What did the CXR show when the patient had pneumonia?</p><p>The chest X-ray taken on January 14, 2019 showed:</p><p>• Bilateral airspace opacities: These are areas of increased density in the lungs, often indicating inflammation or fluid accumulation, which are common findings in pneumonia. • Mild interlobar septal thickening: The lines between the lobes of the lungs (the interlobar septa) were slightly thicker than normal, possibly due to inflammation or fluid. • Mildly decreased lung volumes: The overall space the lungs occupied was slightly reduced, which can occur in pneumonia due to fluid or inflammation. • Small bilateral pleural effusions: There was a small amount of fluid in the pleural space (the area between the lungs and the chest wall) on both sides. This can be a complication of pneumonia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impression:</head><p>The radiologist concluded that the findings were consistent with multifocal pneumonia, meaning the pneumonia was present in multiple areas of the lungs.</p><p>Treatment and Hospital Course: Initially, he was started on a fluoroquinolone antibiotic (levofloxacin) and given nebulizer treatments. However, his condition deteriorated, and he became increasingly short of breath and wheezy. He was admitted to the ICU, where he was put on high-flow nasal cannula (HFNC) for oxygen support and switched to broader spectrum antibiotics -vancomycin, zosyn, and azithromycin. He also received IV fluids and other supportive care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complications:</head><p>The pneumonia led to several complications:</p><p>• Severe Sepsis: The infection progressed to severe sepsis, and he even briefly required pressors to maintain his blood pressure. This indicates that the infection was affecting his entire body and not just his lungs. • Acute Kidney Injury (AKI): His kidney function worsened, likely due to a combination of the infection, decreased oral intake, and the effects of some of his medications.</p><p>• Elevated Troponin: There was a rise in his troponin levels, a marker of heart damage. This was likely due to the stress of the severe infection on his body, in combination with low blood pressure and low oxygen levels.</p><p>• Coagulopathy and Thrombocytopenia: He developed issues with blood clotting and low platelet count, possibly indicating disseminated intravascular coagulation (DIC), a serious complication of sepsis. • Anemia: He had a sudden drop in hemoglobin, requiring a blood transfusion. The source of bleeding was not clearly identified, but he was started on a PPI to protect against GI bleeding.</p><p>Recovery: After several days of treatment, his condition began to improve. He was weaned off of oxygen support, his kidney function started to recover, and his blood clotting issues resolved. He was eventually discharged on a course of antibiotics to complete at home.</p><p>Causative Organism: Blood cultures taken during his hospital stay did not grow any bacteria, but wound cultures from a subsequent hospitalization for cellulitis grew Methicillin-sensitive Staph aureus (MSSA). It's possible that MSSA was also the cause of his pneumonia, or that he had a polymicrobial infection.</p><p>Long-Term Implications: The patient's pneumonia was a serious event that had a significant impact on his health. He was left with a new baseline of decreased kidney function (CKD) and required adjustments to his medication regimen.</p><p>Onset and Symptoms: In January 2019, the patient started experiencing malaise, a non-productive cough, and shortness of breath. He also had fevers and chills. These symptoms gradually worsened over a week, leading to his visit to the emergency department.</p><p>Diagnosis: A portable chest X-ray done in the ED revealed multifocal infiltrates in his lungs, which is consistent with multifocal pneumonia. This means that the infection was present in multiple areas of his lungs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… …</head><p>Figure <ref type="figure" target="#fig_2">13</ref> | Example of Med-Gemini-M 1.5's long-context capabilities on EHR dialogue. Med-Gemini-M 1.5 demonstrates its ability to parse extensive medical records, providing a comprehensive summary of the patient's active and historical conditions. The conversation focuses on the patient's history of severe pneumonia, illustrating how the model's long-context understanding enables a user-friendly interface for interacting with and querying complex medical data. This capability has the potential to reduce cognitive load and improve information accessibility for both clinicians and patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response Mechanism linking the FTO locus with Obesity:</head><p>Variants Involved: The primary variant is the single-nucleotide polymorphism (SNP) rs1421085, a T-to-C substitution. This SNP is located in the first intron of the FTO gene and is strongly associated with an increased risk of obesity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Molecular Chain of Events:</head><p>1. ARID5B Binding Disruption: In individuals with the non-risk T allele, the transcription repressor ARID5B binds to the region containing rs1421085, effectively repressing enhancer activity.</p><p>• The T-to-C substitution (risk allele) disrupts a critical motif necessary for ARID5B binding. This leads to a loss of repression and a gain of enhancer activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Increased IRX3 and IRX5 Expression:</head><p>The increased enhancer activity results in the upregulation of two nearby genes, IRX3 and IRX5, during the early stages of adipocyte differentiation. This is particularly notable in subcutaneous (inguinal) white adipose tissue (iWAT).</p><p>• This upregulation is specific to adipocyte progenitor cells and is not observed in mature adipocytes or other tissues like the brain. 3. Adipocyte Fate Switch: IRX3 and IRX5 promote a shift in the developmental fate of adipocytes. Instead of differentiating into energy-dissipating beige (brite) adipocytes, the progenitors are more likely to become energy-storing white adipocytes.</p><p>• Beige adipocytes are characterized by high mitochondrial content and the expression of uncoupling protein 1 (UCP1), which allows them to burn energy and produce heat. • White adipocytes, on the other hand, are primarily involved in lipid storage. 4. Reduced Thermogenesis and Increased Lipid Storage: The decrease in beige adipocytes leads to reduced thermogenesis and increased lipid storage, contributing to weight gain and obesity.</p><p>Experimental Support:</p><p>• Mouse Models: CRISPR-Cas9 was used to create a mouse model with a deletion of the rs1421085-conserved cis-regulatory module (CRM). Mice homozygous for this deletion exhibited reduced body weight and fat mass when fed a high-fat diet, consistent with the human phenotype. These mice also showed increased mitochondrial activity and thermogenesis marker expression in adipose tissue, as well as decreased lipid storage marker expression and smaller adipocytes. • Human Adipocytes: CRISPR-Cas9 editing of rs1421085 in primary human adipocytes from a patient with the risk allele restored IRX3 and IRX5 repression and activated browning expression programs and thermogenesis. • Gene Expression and Chromatin Conformation: Studies have shown that the risk allele is associated with increased IRX3 and IRX5 expression in human adipocyte progenitor cells. Additionally, 3D fluorescence in situ hybridization experiments in mice demonstrated that the risk allele disrupts the interaction between the enhancer and the Irx3 promoter, providing further evidence for the proposed mechanism. • Metabolomic Studies: Untargeted metabolomic analysis of adipose tissue and blood plasma from mice and humans revealed an rs1421085-specific disruption of steroidal metabolic homeostasis, with a decrease in acyl steroids in risk allele carriers. This suggests that the FTO obesity variant may also exert its effects through a genetically determined metabotype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>Task: Given the provided [Research Articles], what is the mechanism linking locus FTO with disease Obesity?</p><p>Instructions: Provide the main variants involved, the chain of molecular events linking the variant to the phenotype, and the experimental support for your statement.</p><p>[Research Articles ]:  Gemini-M 1.5 is given the full content of 12 research articles in portable document format (PDF) as input and is instructed to provide a succinct description of the mechanistic link between the FTO locus (a region of BMI-and obesity-associated variants within the gene FTO) and obesity, together with a list of concrete supporting experimental results. The model generates a concise, informative, and accurate description of how the FTO locus contributes to obesity biology and presents it in a clear and digestible manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Med-Gemini, built upon the Gemini models, demonstrates significant advancements in clinical reasoning, multimodal understanding, and long-context processing within the medical domain. This is evidenced by its strong performance across a diverse range of 25 tasks spanning 14 medical benchmarks, encompassing medical knowledge, clinical reasoning, genomics, waveforms, medical imaging, health records and videos.</p><p>MedQA performance Notably, Med-Gemini-L 1.0 achieves a new SoTA on MedQA (USMLE), a popular benchmark for medical question answering with the use of self-training based fine-tuning and search integration. Our thorough relabeling of the MedQA test set (performed by attending clinicians) reveals important insights. While MedQA (USMLE) is a useful benchmark for assessing medical knowledge and reasoning, it is essential to acknowledge its limitations. We discover that approximately 4% of the questions contain missing information, and an additional 3% potentially have labeling errors. Establishing definitive ground truth is frequently challenging in medicine, where inter-reader variability and ambiguity are common and medical knowledge is constantly evolving.</p><p>Our observations suggest that further improvements in SoTA performance on the MedQA (USMLE) benchmark in isolation may not directly correlate to progress in the capabilities of medical LLMs for meaningful real-world tasks and as such it is important to perform more comprehensive benchmarking and evaluation representative of real-world clinical workflows <ref type="bibr" target="#b29">(Fleming et al., 2023)</ref>. In general, most benchmarks have limitations around dataset size and quality. While we focus our analysis here on MedQA (USMLE), prior work has suggested similar issues with other popular benchmark datasets <ref type="bibr" target="#b139">(Xu et al., 2023)</ref>. Retraining Med-Gemini-M 1.5 with a new split of the PAD-UFES-20 dermatology dataset leads to a drop of 7.1% as compared to our results in Table <ref type="table" target="#tab_4">2</ref>. As such, careful attention needs to be given to the size and quality of datasets when interpreting and contextualizing model performance.</p><p>Web search integration Med-Gemini's integration with web search presents exciting possibilities to provide more factually accurate and reliable answers to medical queries with LLMs. In this work, we focus on training Med-Gemini-L 1.0 to issue web search queries when uncertain and integrate the results when producing responses. While the results on MedQA, NEJM CPC, and GeneTuring benchmarks are promising, significant further research is necessary. For example, we haven't considered restricting the search results to more authoritative medical sources <ref type="bibr" target="#b144">(Zakka et al., 2024)</ref>, using multimodal search retrieval or performed analysis on accuracy and relevance of search results and the quality of the citations <ref type="bibr" target="#b138">(Wu et al., 2024)</ref>. Further, it remains to be seen if smaller LLMs can also be taught to make use of web search. We leave these explorations to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Promising multimodal conversational capabilities</head><p>The multimodal conversational capabilities of Med-Gemini-M 1.5 are promising given they are attained without any specific medical dialogue fine-tuning. Such capabilities allow for seamless and natural interactions between people, clinicians, and AI systems. As showcased in our qualitative examples, Med-Gemini-M 1.5 has the capability to engage in multi-turn clinical dialogues, request additional information such as images when needed, explain their reasoning in a comprehensible manner, and even help provide information useful for clinical decisions while appropriately deferring the final decision to human experts. This capability has significant potential for helpful real-world applications, including assisting clinicians and patients, but of course also entails highly significant associated risks. While highlighting the potential for future research in this domain, we have not rigorously benchmarked capabilities for clinical conversation in this work as previously explored by others in dedicated research towards conversational diagnostic AI <ref type="bibr">(Tu et al., 2024b)</ref>. In addition, in forthcoming work, we will also rigorously explore the capabilities of Gemini in clinically specific multimodal tasks such as radiology report generation.</p><p>Opportunities with long-context processing Perhaps the most notable aspect of Med-Gemini is the long-context processing capabilities because they open up new performance frontiers and novel, previously infeasible application possibilities for medical AI systems. In this work, we introduce a novel EHR task focused on identifying and verifying conditions, symptoms and procedures within very long electronic patient records. This "needle-in-a-haystack" retrieval task reflects a real-world challenge faced by clinicians <ref type="bibr" target="#b62">(Klerings et al., 2015)</ref>, and Med-Gemini-M 1.5's performance demonstrates its potential to significantly reduce cognitive load and augment clinicians' capabilities by efficiently extracting and analyzing crucial information from vast amounts of patient data. The medical video question answering and annotation performance suggests these capabilities can generalize to complex multimodal data. It is worth highlighting that the demonstration of long-context capabilities is in a few-shot fashion without any task-specific fine-tuning. Such capabilities open up the possibilities of fine grained analysis and annotation of genomic and multi-omic sequence data, complex imaging modalities such as pathology or volumetric images and integrative processing with health records to uncover novel insights and assist in clinical workflows.</p><p>Importance of medical specialization and fine-tuning Gemini models are inherently multimodal and have strong medical knowledge as a result of large-scale multimodal pretraining. This is reflected in impressive out-of-the-box performance on multimodal benchmarks such as NEJM Image Challenge surpassing similar generalist vision-language models such as GPT-4V by a large margin <ref type="bibr" target="#b9">(Buckley et al., 2023)</ref>. At the same time, medical knowledge and data (particularly multimodal data) is unique and complex and unlikely to be seen on the public internet commonly used to train LLMs. Gemini is a strong intelligence substrate but further fine-tuning, specialization and alignment of even such powerful models are necessary before use in the medical domain. At the same time, given the general capabilities of Gemini, the amount of data needed for such specialization and alignment is much lower than prior generation of medical AI systems <ref type="bibr" target="#b5">(Azizi et al., 2023)</ref> and it is indeed possible to efficiently adapt such models even to previously unseen but important medical modalities such as ECGs with relative efficiency as demonstrated here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Need for rigorous evaluation beyond benchmarks</head><p>To the best of our knowledge, this work is the most comprehensive evaluation of medical LLMs and LMMs. The work includes evidence of new capabilities for medical AI and tasks that suggest real-world utility. This is particularly reinforced by strong performance of our models in evaluations of medical summarization and referral note generation. Diagnostic tasks draw considerable attention in research, but carry significant regulatory, clinical and equity-related risks that require addressing before real-world implementation is safe and feasible. The more common real-world use cases of generative AI in healthcare are therefore in non-diagnostic tasks, where errors have a lower risk-profile yet model outputs can significantly improve the efficiency of care providers by alleviating administrative burdens and assisting complex information retrieval or synthesis required in day-to-day work. At the same time, even for such nondiagnostic tasks, assurance of real-world impact requires evaluation grounded in specific use-cases and environments. These evaluations lie beyond the scope of initial benchmarking, and our results should be interpreted with appropriate caution. To assess downstream consequence and generalization of the promise we demonstrate here to real-world clinical workflows, practitioners should adhere to best practices of responsible AI, rigorously measuring multiple endpoints including equity <ref type="bibr" target="#b97">(Pfohl et al., 2024)</ref>, fairness and safety in the intended environment while also considering the multiple socio-technical factors that are use-case specific determinants of impact. Finally, it is worth noting that while we have considered 14 diverse and challenging benchmarks in this study, over 350 medical benchmarks are available in the community <ref type="bibr" target="#b81">(Meta, 2024)</ref>.</p><p>Responsible AI Our work has been primarily focused on capabilities and improvements and the art of the possible with Gemini models. An important focal area for future exploration is the integration of the responsible AI principles throughout the model development process <ref type="bibr" target="#b97">(Pfohl et al., 2024)</ref>, including, but not limited to, the principles of fairness, privacy, equity, transparency and accountability. Privacy considerations in particular need to be rooted in existing healthcare policies and regulations governing and safeguarding patient information. Fairness is another area that may require attention, as there is a risk that AI systems in healthcare may unintentionally reflect or amplify historical biases and inequities <ref type="bibr" target="#b0">(Abràmoff et al., 2023;</ref><ref type="bibr" target="#b12">Char et al., 2018;</ref><ref type="bibr" target="#b16">Cirillo et al., 2020;</ref><ref type="bibr" target="#b36">Gichoya et al., 2022;</ref><ref type="bibr">Obermeyer et al., 2019;</ref><ref type="bibr" target="#b97">Pfohl et al., 2024)</ref>, potentially leading to disparate model performance and harmful outcomes for marginalised groups. Such health disparities have been identified across gender <ref type="bibr" target="#b61">(Kent et al., 2012)</ref>, race <ref type="bibr">(Obermeyer et al., 2019;</ref><ref type="bibr" target="#b137">Williams and Wyatt, 2015)</ref>, ethnicity <ref type="bibr" target="#b105">(Razai et al., 2021)</ref>, socioeconomic status <ref type="bibr" target="#b115">(Steptoe and Zaninotto, 2020)</ref>, sexual orientation <ref type="bibr" target="#b80">(Medina-Martínez et al., 2021)</ref>, age <ref type="bibr" target="#b53">(Jackson et al., 2019)</ref>, and other sensitive and/or protected personal characteristics.</p><p>There is an increasing need for a deep intersectional analysis of impact <ref type="bibr" target="#b52">(Iyer et al., 2008;</ref><ref type="bibr">López and Gadsden, 2017)</ref>, though this remains a hard technical problem <ref type="bibr" target="#b10">(Cabrera et al., 2019;</ref><ref type="bibr">Wang et al., 2022a;</ref><ref type="bibr" target="#b140">Yang et al., 2020)</ref>, and an active area of research.</p><p>As we demonstrate new capabilities for LLMs and LMMs, new opportunities arise for potential issues at the confluence of dataset bias <ref type="bibr" target="#b32">(Ganapathi et al., 2022)</ref>, model bias <ref type="bibr" target="#b71">(Liu et al., 2023)</ref>, and the socio-technical considerations for individual use cases. In the context of the capabilities we have discussed, these issues may potentially occur in in-context learning within the long-context utilization of potentially biased examples and instructions, in search integration, the dynamics of self-training, or multimodal understanding with fine-tuning and customized data encoders. Within each of these capabilities, there could be multiple points at which such biases may need to be considered. When it comes to web search integration, biases could come up at query construction time, get reflected in the returned result set <ref type="bibr" target="#b86">(Novin and Meyers, 2017)</ref>, or be embedded within each of the linked external sources, and manifest in various other subtle ways, e.g. how the results are integrated into the generative reasoning process when producing the final answer. With multimodal models, biases may occur in each of the individual modalities separately, or only be apparent jointly, across co-dependent modalities of the data <ref type="bibr" target="#b77">(Mandal et al., 2023;</ref><ref type="bibr" target="#b114">Srinivasan and Bisk, 2021)</ref>. A comprehensive analysis of potential issues may need to consider each of these points separately, but also holistically as they are all parts of a complex system. These systems may also need to be thoroughly evaluated not only in isolation, but also with human experts in the loop.</p><p>However, these new capabilities also present an opportunity to mitigate prior issues and dramatically improve accessibility across use-cases. For example, new long-context capabilities in medicine may enable a model's users to solve complex problems at inference time without the need for engaging in model fine-tuning, as the data can be utilized directly within the context of the query, followed by a set of natural language instructions. Previously, users of such systems would have needed to possess engineering expertise and invest additional time and resources in fine-tuning custom models for tackling such complex tasks. Web search integration, on the other hand, may prove to be invaluable when it comes to rapidly integrating newly developed pieces of medical knowledge and external consensus on what is a highly dynamic and non-stationary medical landscape. The COVID-19 pandemic has shown just how quickly the public health understanding and recommendations may need to get updated, and it also highlighted the overall danger posed by medical misinformation <ref type="bibr" target="#b63">(Kouzy et al., 2020)</ref>. Models that can reliably consume reputable up-to-date external sources may be far less likely to lead to such misinformation. Similar new opportunities are presented by the other model capabilities, though further study is needed to develop a robust evaluation framework to assess the associated risk of bias and unfair outputs (whether individually or jointly across complex use-cases), with such assessments sociotechnically grounded in real settings for specific clinical use-cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Large multimodal language models are ushering in a new era of possibilities for health and medicine. The capabilities demonstrated by Gemini and Med-Gemini suggest a significant leap forward in the depth and breadth of opportunities to accelerate biomedical discoveries and assist in healthcare delivery and experiences. However, it is paramount that advancements in model capabilities are accompanied by meticulous attention to the reliability and safety of these systems. By prioritizing both aspects, we can responsibly envision a future where the capabilities of AI systems are meaningful and safe accelerators of both scientific progress and care in medicine.  <ref type="figure" target="#fig_0">1</ref>. We display our aggregated results comparing Med-Gemini to the previous state-of-the-art (SoTA) and the best GPT-4 methods across text-based, multimodal, and long-context tasks. For benchmarks where we could not find GPT-4 (or GPT-4V) reported numbers in literature, we run evaluations on the same test sets using public APIs for a head-to-head comparison, using same few-shot prompts as the corresponding Med-Gemini model including instructions to ensure outputs are correctly formatted. Note that GPT-4 results are not available (N/A) for three long context tasks due to limitations of the context window of the public GPT-4 / GPT-4V APIs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related Works</head><p>Overview of large language model in medicine Large language models (LLMs) have revolutionized machine learning and artificial intelligence. Researchers have employed novel network architectures, such as transformers <ref type="bibr" target="#b126">(Vaswani et al., 2017)</ref> and pathways <ref type="bibr" target="#b6">(Barham et al., 2022)</ref>, to train these models on massive datasets. This self-supervised training across diverse domains includes models like BERT <ref type="bibr" target="#b24">(Devlin et al., 2018)</ref>, GPT <ref type="bibr" target="#b101">(Radford et al., 2018)</ref>, T5 <ref type="bibr" target="#b102">(Raffel et al., 2020)</ref>, FLAN <ref type="bibr" target="#b134">(Wei et al., 2021)</ref>, BLOOM (Le <ref type="bibr" target="#b65">Scao et al., 2022)</ref>, Flamingo <ref type="bibr" target="#b2">(Alayrac et al., 2022)</ref>, PaLM and PaLM2 <ref type="bibr" target="#b3">(Anil et al., 2023;</ref><ref type="bibr" target="#b15">Chowdhery et al., 2023)</ref>, LLaMA <ref type="bibr" target="#b120">(Touvron et al., 2023)</ref>, PaLI <ref type="bibr" target="#b14">(Chen et al., 2022)</ref>, PaLM-E <ref type="bibr" target="#b25">(Driess et al., 2023)</ref>, and the recent Gemini models <ref type="bibr" target="#b34">(Gemini Team, Google, 2023</ref><ref type="bibr" target="#b81">, 2024)</ref>. By processing text or multimodal information, these pretrained models develop a robust understanding of language, patterns, and relationships with remarkable adaptibility.</p><p>Minimal fine-tuning allows these models to adapt to diverse downstream tasks. In the medical domain, Med-PaLM <ref type="bibr">(Singhal et al., 2023a)</ref> and Med-PaLM 2 <ref type="bibr">(Singhal et al., 2023b)</ref> represent pioneering medical LLMs fine-tuned on EHRs, exam questions, and research literature. To achieve the goal of generalist medical AI (GMAI) <ref type="bibr">Moor et al. (2023a)</ref>, researchers use general LLMs with prompting strategies [e.g., GPT-4 with Medprompt <ref type="bibr" target="#b85">(Nori et al., 2023)</ref>], or refine them with multimodal data for enhanced medical understanding [e.g., Med-PaLM-M <ref type="bibr">(Tu et al., 2024a)</ref>]. These models show promise in diagnosis assistance <ref type="bibr" target="#b79">(McDuff et al., 2023)</ref>, risk prediction, drug discovery, diagnostic dialogue <ref type="bibr">(Tu et al., 2024b)</ref> and assessing psychiatric functioning <ref type="bibr" target="#b31">(Galatzer-Levy et al., 2023)</ref>. Our work leverages the latest Gemini models, using either direct instruction prompting or further finetuning for specialized medical tasks. Below, we discuss related works across the areas of language, multimodal learning, and long-context modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model reasoning and tool-use for language-based tasks</head><p>Reasoning is a process of logical thinking that leads to a conclusion, which can be significantly enhanced by recent advances in LLMs and large multimodal models (LMMs). These improvements stem from a combination of better models and that directly imitate human reasoning. Language model based reasoning techniques have been surveyed in prior works <ref type="bibr" target="#b48">(Huang and Chang, 2023;</ref><ref type="bibr" target="#b99">Qiao et al., 2023)</ref>, with such surveys extended into multimodal reasoning <ref type="bibr" target="#b131">(Wang et al., 2024)</ref>. Strategies to enhance language reasoning include prompt engineering, improved processes, and enhancing reasoning with access to external elements such as tools or knowledge. Prompt engineering is exemplified by approaches such as Chainof-Thought (CoT) prompting <ref type="bibr" target="#b135">(Wei et al., 2022)</ref>, which involves generating a series of intermediate reasoning steps, Least-to-Most prompting, which involves breaking down a problem into smaller subproblems and then sequentially solving them <ref type="bibr" target="#b148">(Zhou et al., 2023)</ref>, and other methods that explore different reasoning paths to arrive at a conclusion <ref type="bibr" target="#b7">(Besta et al., 2024;</ref><ref type="bibr" target="#b141">Yao et al., 2023)</ref>. Improved processes arise from methods such as model updates via self-improvement <ref type="bibr" target="#b146">(Zelikman et al., 2022)</ref> or ensemble-based approaches <ref type="bibr">(Wang et al., 2022b)</ref>.</p><p>Access to external elements such as tools <ref type="bibr" target="#b44">(Hao et al., 2024;</ref><ref type="bibr" target="#b109">Schick et al., 2024)</ref> or external knowledge bases through the use of retrieval augmented generation (RAG) <ref type="bibr" target="#b33">(Gao et al., 2024;</ref><ref type="bibr" target="#b147">Zhang et al., 2024)</ref> has also demonstrated improvements in language model reasoning. Recently LLMs have also evolved to interact with information and web tools. For tool-use, LLMs can learn to execute external tools or application programming interface (APIs), enabling them to perform actions in the real world like searching, calendar use, or using translation service via APIs <ref type="bibr" target="#b100">(Qin et al., 2023;</ref><ref type="bibr" target="#b109">Schick et al., 2024)</ref>. For web search specifically, LLMs incorporate traditional search engines by understanding complex queries and providing summaries that synthesize information from multiple sources <ref type="bibr" target="#b84">(Nakano et al., 2021;</ref><ref type="bibr" target="#b125">Varshney et al., 2023)</ref>. Furthermore, LLMs are able to not only retrieve information but also utilize tools and create ones based on user-defined needs <ref type="bibr" target="#b11">(Cai et al., 2023)</ref>. <ref type="bibr" target="#b144">Zakka et al. (2024)</ref> have demonstrated that search tool-use can be particularly useful in medical guideline and treatment recommendations. In this work, we integrate a strategy of self-training with search to improve Med-Gemini's capabilities for model reasoning.</p><p>Large multimodal models in medicine Medical practice often requires integration of multiple modalities to deliver effective care, for example, integrating data sources from patient history, medical imaging, genetic testing and lab results. Models that can integrate such modalities may provide a more comprehensive picture of a patient's condition. Existing approaches fall into two broad categories: specialist and generalist. Specialist models excel at specific tasks within a medical discipline. Examples include models optimized for radiology report generation <ref type="bibr">(Tanno et al., 2024;</ref><ref type="bibr" target="#b145">Zambrano Chaves et al., 2024)</ref>, pathology question answering or histopathology captioning <ref type="bibr" target="#b75">(Lu et al., 2023)</ref>, radiology-related tasks <ref type="bibr" target="#b139">(Xu et al., 2023)</ref>, and cardiology electrocardiogram captioning <ref type="bibr" target="#b128">(Wan et al., 2024)</ref>.</p><p>Conversely, "generalist medical AI" (GMAI) systems <ref type="bibr">(Moor et al., 2023a)</ref>, such as Med-PaLM M <ref type="bibr">(Tu et al., 2024a)</ref> and LLaVA-Med <ref type="bibr" target="#b68">(Li et al., 2024)</ref>, tackle a wider range of tasks across multiple specialties, aiming for broader applicability in clinical settings. The diversity of tasks performed by systems such as Med-PaLM M performance remains noteworthy as one of the earliest examples of generalist multimodal models in medicine, capable of addressing radiology, pathology, dermatology, and genomics tasks with competitive performance or exceeding SoTA across different specialties using a strong pretrained LLM with appropriate fine-tuning strategies. In this report, we further advance the evidence that AI systems can deliver strong generalist multimodal capabilities in medicine with Med-Gemini but the primary focus is on developing a model family considering application specific trade-offs.</p><p>Long-context capability of large language models Prior works addressing tasks with long-context windows have been limited by the capabilities of LLMs to effectively utilize large spans of text due to the memory and computation limitation of the Transformer-based models <ref type="bibr" target="#b72">(Liu et al., 2024;</ref><ref type="bibr" target="#b126">Vaswani et al., 2017)</ref>. Initial efforts used hierarchical approaches derive representations of clinical text that could not fit into a model's limited context window <ref type="bibr" target="#b20">(Dai et al., 2022)</ref>. Subsequent work such as Clinical-Longformer and Clinical-BigBird <ref type="bibr" target="#b69">(Li et al., 2023)</ref> focused on extending context lengths from 512 to 4096 tokens, enabling improvements in performance in question answering, document classification and information retrieval tasks. Subsequent approaches explored the use of such models in combination with imaging encoders to tackle multimodal tasks such as medical visual question answering <ref type="bibr" target="#b42">(Gupta and Demner-Fushman, 2022)</ref>. With the advancements of hardware and efficient algorithms, researchers have developed LLMs with larger context windows toward 100K tokens <ref type="bibr" target="#b21">(Dai et al., 2019;</ref><ref type="bibr" target="#b98">Poli et al., 2023)</ref>. Recently, Gemini further advanced the boundary of long-context capability to one million tokens <ref type="bibr" target="#b35">(Gemini Team, Google, 2024)</ref>.</p><p>However, in the domain of medicine the majority of LLMs continue to be evaluated on relatively short texts <ref type="bibr" target="#b95">(Parmar et al., 2023)</ref> and single images. Despite their importance to medicine and clinical practice, long-context capabilities in medicine, especially in multimodal settings, are underexplored. We address this unmet need and investigate the potential of Med-Gemini on different long-context use cases, including video and long EHR-related tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional details on advanced reasoning text-based tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. MedQA (USMLE) Relabeling</head><p>The main objectives of this rater study are to identify (a) unanswerable questions due to missing information, (b) potential label errors, and (c) potentially ambiguous questions <ref type="bibr" target="#b117">(Stutz et al., 2023)</ref>.</p><p>To this end, we carefully design a two-step study as follows:</p><p>•</p><p>Step 1: Given the MedQA (USMLE) question and all four answer options:</p><p>-(Q1) We ask "Are any of the options appropriate to answer this question?" -(Q2) If yes, "Select one or more options to answer the question." (Multi-select) -(Q3) We ask "Is there any additional information (such as figures, plots, lab results, or similar) referenced in the question that is missing?" -(Q4) If yes, we ask "Do you think having access to the missing information would change your answer?"</p><p>• Step After the rater completes step 1, they are presented with the ground truth answer from MedQA:</p><p>-(Q1) We ask "Having revealed the question bank's answer key, does your answer from before change?" -(Q2) If yes, we repeat the first two questions from above.</p><p>A key consideration that leads to this two-step approach is to reveal the MedQA (USMLE) ground truth at the right time to avoid biasing the rater with the ground truth when answering questions about potentially missing information in the question (Q3 and Q4). For properly identifying label errors, however, we present the raters with the ground truth so they can decide to disagree (Q1 and Q2 in step 2). To identify potentially ambiguous questions (allowing multiple "good" or true answers), we further allow raters to select multiple options as answers<ref type="foot" target="#foot_0">foot_0</ref> . When asking about potentially missing information, we aim to identify whether this missing information is critical to answer the question. We recruit a total of 18 primary care physicians (PCPs) from the US to participate in the study. We select PCPs located in the US because MedQA comprises USMLE-style questions across many specialties. For each MedQA (USMLE) question, we collect at least three ratings from independent raters. While the original MedQA work in <ref type="bibr" target="#b55">(Jin et al., 2021</ref>) evaluated expert performance with access to additional text material, our raters are not instructed to use any material. However, we do not explicitly control for this. PCPs take an average of 255 seconds to complete one question; 98% take less than 10 minutes.</p><p>For each question, we aggregate the ratings in order to identify, e.g., label errors with high certainty. First, we evaluate the agreement of each rater against the majority or unanimous vote in Table <ref type="table" target="#tab_12">C3</ref>. Specifically, we consider the rater agreement for four rating tasks of interest: whether information is missing, whether there is a label error (i.e., the rater's answer after revealing the MedQA (USMLE) ground truth does not include the ground truth answer from MedQA (USMLE)), whether a question is ambiguous (i.e., the rater's answer includes more than one option even after revealing the MedQA (USMLE) ground truth) and agreement between raw answer options selected in terms of average overlap (each rater can select none or multiple options). For the former three, agreement is generally high (&gt;87%), although it is usually lower on questions where Med-Gemini-L 1.0 makes mistakes. For the third, in contrast, in terms of average overlap between all pairs of answers is significantly lower: typically around 75% when raters have seen the MedQA (USMLE) ground truth, but agreement drops to around 50% if the ground truth is not revealed to raters.  and remaining MedQA (USMLE) questions (red) after filtering questions with missing information, label errors or questions deemed ambiguous when aggregating ratings using majority voting (left) or unanimous voting (right).</p><p>To measure the impact of filtering MedQA (USMLE) questions with missing information or label errors on evaluation while taking into account annotation uncertainty, we perform a bootstrapping experiment. Specifically, we repeatedly sample a committee of three raters per question (with replacement). For each committee of raters, we perform majority or unanimous voting to identify questions with missing information or label errors to be filtered for evaluation. This can be seen as an instance of the evaluation framework proposed in <ref type="bibr" target="#b117">(Stutz et al., 2023)</ref>. The advantage of bootstrapping over simple voting is that we get reliable uncertainty estimates that make sure we can identify performance changes as statistically significant. We repeat this experiment 1000 times and report average and standard deviation of accuracy and fraction of remaining questions in Figure <ref type="figure" target="#fig_15">C1</ref>.</p><p>While questions with missing information or label errors can be identified with high confidence due to high agreement, it is more difficult to judge whether a question is ambiguous. Here, we define a question as being ambiguous if it allows for multiple answer options to be correct. Most questions in the MedQA (USMLE) test set specifically ask for the "best", "most likely" or "most appropriate" option. However, it is largely unclear whether answers do indeed only allow for one option to be e.g. the "best next step in management" of a case. After excluding questions with missing information and label errors using majority voting, raters selected on average 1.065 options, indicating that some questions might indeed be ambiguous. This increases to 1.119 after revealing the ground truth. To take this into account during evaluation, we define a rating as ambiguous if the rater selects more than one option after revealing the ground truth. We then follow the same analysis as above and show results in Figure <ref type="figure" target="#fig_15">C1</ref>. Overall, we find that filtering label errors has the biggest impact on Med-Gemini-L 1.0's performance, while filtering for missing information or ambiguous questions can reduce the number of questions but does not change accuracy significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Additional results on NEJM clinico-pathological conference dataset</head><p>We present Top-1 and Top-10 performance on the NEJM clinical pathology case studies as broken down by the primary speciality of the cases (as identified by NEJM) in Table <ref type="table" target="#tab_15">C4</ref>, for all specialties with at least 10 cases. In most specialties, Internal Medicine, Pediatrics, and Psychiatry, the best Top-1 and Top-10 performance is achieved by either Med-Gemini-L 1.0 without search or the model with search.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Real-world use cases for advanced reasoning on text-based tasks</head><p>We instruction fine-tune and evaluate Med-Gemini-M 1.0 on three challenging real-world tasks requiring long-form text generation. Summary results are shown in Figure <ref type="figure" target="#fig_4">5</ref>. Detailed results of additional evaluation axes are shown in Table <ref type="table" target="#tab_16">C5</ref>. Datasets and evaluation procedures for each task are described in more detail below. Evaluation is performed by clinician raters. P-values are for whether the rate at which Med-Gemini-M 1.0 is preferred or tied with experts is 0.5 (two-sided t-test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical summarization evaluation</head><p>This task involves generating an after-visit summary (AVS) from a de-identified history and physical (H&amp;P) note. An H&amp;P note is a detailed document in which a healthcare provider records the findings of a patient visit, including a patient's health background, their current symptoms, and the findings of a physical examination. It is largely written for other healthcare providers to ensure coordinated care. An AVS, on the other hand, is a structured report that patients receive at the end of a medical visit summarizing the most important aspect of the visit and their health status.</p><p>A set of 31 de-identified H&amp;P notes is sampled from a dataset of de-identified medical notes from outpatient visits to family medicine or internal medicine providers. The expert after-visit summaries are written by U.S based clinicians following guidelines based on <ref type="bibr" target="#b111">(Sieferd et al., 2019)</ref>, and further refined by a second round of clinicians to further increase quality.</p><p>Med-Gemini-M 1.0 is prompted to generate an after-visit summary given the de-identified H&amp;P note as follows:</p><p>Please read through the provided medical note describing an outpatient visit and extract the relevant information for each of the following 12 fields: -Today we accomplished: This field should summarize the main topics of discussion and results of any procedures performed during the current visit. The summary could be a short list of procedures, or could be a text description of the patient's experience. Please be as brief as possible when providing details, such as test results or medication names. Describing the experience from the patient's point of view, using phrases like "my visit", "my condition".</p><p>-My important numbers: This field should provide the results of any measurements relevant to the visit, including vitals. Provide the results of any numeric measurements relevant to the visit, including vitals, laboratory studies, or pain scores. Please include the numbers that should be monitored. Do not fabricate numbers that are not presented in the note.</p><p>-Changes to my medications are: This field should specify any medications that were added, for which the doses were updated, or which are no longer needed after the visit. Please specify both newly added and stopped medications when possible. If no changes are apparent from the note, please answer "no changes".</p><p>-Other medications I have are: If the note indicates any existing medications for the patient that the patient should continue taking without changes, list them here. If no medications are indicated in the note, please "Not specified".</p><p>-My next steps are: This field should document the patient's next steps, including any actions they should take, test results they should expect, and follow-up visits they should schedule, along with the appropriate time frames for each.</p><p>-I should seek immediate medical attention if: If the note specifies any conditions for which the patient should immediately seek care, specify it here. Be sure to only include conditions that are mentioned in the note. If no conditions are mentioned, write "Not specified". -Other comments from my provider: This is an optional extra field that captures any additional relevant information the provider indicated in the notes that it would be useful for the patient to know. Do not include information that is already listed in the previous field.</p><p>For each field, write at a sixth-grade reading level and avoid using abbreviations or jargon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note: {MEDICAL_NOTE} After Visit Summary:</head><p>Physician raters are presented with the H&amp;P note, the clinician generated AVS and our model's generated AVS. Each example is evaluated once by one of three different U.S.-based physicians across the following axes:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions:</head><p>You are given a medical note that mentions a referral to another healthcare provider. Imagine you need to write a referral letter based on the information in the note. You are provided with draft referral letters written by two different assistants. Which draft do you prefer as a starting point for editing into a final version? Please also provide a brief justification for your preference in the 'Notes' column.</p><p>WARNING: Unfortunately, it is not guaranteed that the draft letters accurately reflect the referral reason or patient history. This will need to be ascertained based on the provided medical note and should heavily factor into your preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>-A Strongly Preferred -A Somewhat Preferred -Tied -B Somewhat Preferred -B Strongly Preferred Three different U.S. board certified physicians are recruited and each of them evaluates all 25 examples. Ratings are aggregated by mapping the Likert scales to a numerical range ([-2,2]) and taking the sign of the median value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical simplification evaluation</head><p>This task involves generating a plain language summary (PLS) from a technical abstract from a biomedical systematic review. A PLS is a version of the technical abstract that is written in plain English and meant to be understood by most readers without a university education <ref type="bibr" target="#b18">(Cochrane, 2014)</ref>.</p><p>A set of 25 technical abstracts and plain language summaries from systematic reviews conducted by Cochrane is sampled from the test split of the dataset introduced by <ref type="bibr" target="#b23">Devaraj et al. (2021)</ref>. The expert plain language summaries are written by the original authors of the Cochrane systematic reviews.</p><p>Med-Gemini-M 1.0 is prompted to generate a PLS given the technical abstract as follows:</p><p>Please read through the provided technical summary of a body of medical research and provide a simplified summary that is accessible to a lay audience without medical expertise. Technical Summary: {TECHNICAL_ABSTRACT} Simplified Summary:</p><p>Clinicians are presented with the technical abstract, the original PLS and our model's generated PLS. They are blinded to the source of each PLS and asked to perform the following comparisons: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional details on multimodal understanding tasks D.1. Multimodal fine-tuning datasets</head><p>For Med-Gemini-M 1.5's multimodal fine-tuning, we use four image-text datasets from <ref type="bibr">MultiMed-Bench (Tanno et al., 2024;</ref><ref type="bibr">Tu et al., 2024a)</ref> including Slake-VQA <ref type="bibr" target="#b70">(Liu et al., 2021)</ref>, Path-VQA <ref type="bibr" target="#b45">(He et al., 2020)</ref>, MIMIC-CXR <ref type="bibr">(Johnson et al., 2019a,b)</ref>, PAD-UFES-20 <ref type="bibr" target="#b94">(Pacheco et al., 2020)</ref>, in addition to the Radiology Objects in COntext (ROCO) dataset <ref type="bibr" target="#b96">(Pelka et al., 2018)</ref>. We further use a subset of ECG-QA <ref type="bibr">(Oh et al., 2023)</ref> to develop the health signal encoder for encoding sensor input in Med-Gemini-S 1.0. We describe the datasets in details below:  • MIMIC-CXR is a CXR dataset with free-text reports <ref type="bibr">(Johnson et al., 2019a,b)</ref>, consisting of 377110 chest X-ray images along with the corresponding protected health information (PHI)-removed text reports from 65379 patients (227835 image studies, with one or more image view positions). Each report is annotated with 13 common radiological conditions using the CheXpert labelling software <ref type="bibr" target="#b51">(Irvin et al., 2019)</ref>. We use the official train/test split as described in the MIMIC-CXR for all tasks. We consider four fine-tuning tasks using MIMIC-CXR: (1) normal vs. abnormal binary classification, (2) CXR abnormality condition VQA, (3) synthetic CXR VQA, and (4) text report generation. For the normal vs. abnormal binary classification task, we classify each image into either normal or abnoraml category based on the CheXpert "no finding" label using all frontal view images [anterior-posterior (AP) and posterioranterior (PA) views] with the task prompt listed in Figure <ref type="figure" target="#fig_0">D1</ref>. For CXR abnormality condition VQA, we exclude all images with normal findings, and group positive and uncertain labels as positive class for 13 abnormal conditions: atelectasis, cardiomegaly, consolidation, edema, enlarged cardiomediastinum, fracture, lung lesion, lung opacity, pleural effusion, pleural other, pneumonia, pneumothorax, and support devices. Then we frame the abnormality detection problem into a close-ended multi-class multiple-choice question setup as shown in Figure <ref type="figure" target="#fig_0">D1</ref>. To further enrich these VQA tasks, we generate an collection of synthetic question-and-answer pairs from radiology reports by querying Gemini base models.</p><p>We specifically prompt the LLM to extract pairs of yes-or-no question and the corresponding answer from each report such that they are independent of the presence of the above 13 conditions. We ensure that for each question, the number of "yes" and "no" are the same to avoid introducing spurious correlation. All VQA tasks are added as the auxiliary tasks for the report generation task which combines the image with the contextual information from the INDICATION section (reason for the study) as the model input to generate the FINDINGS and IMPRESSION sections of the report as the target, similar to prior works <ref type="bibr" target="#b50">(Hyland et al., 2023;</ref><ref type="bibr">Tu et al., 2024a)</ref>.  <ref type="bibr">et al., 2020)</ref>. Six types of skin lesions are included in the dataset: basal cell carcinoma, melanoma, squamous cell carcinoma, actinic keratosis, melanocytic nevus, and seborrheic keratosis. Each image is correlated with up to 21 clinical features (e.g., patient demographics, family cancer history lesion location, lesion size). Given no published official splits, we adopt two PAD-UFES-20 split setup. We use Med-PaLM M split (the image-level split) for a direct, fair evaluation and comparison against the previous SoTA method. We also evaluate on a new split, which is a split at the patient level (Table <ref type="table" target="#tab_4">2</ref>). We set up three classification tasks for fine-tuning: (1) 6-class classification using the original label distribution and 14 clinical features (age, gender, smoke, drink, skin cancer history, cancer history, region, Fitspatrick, horizontal and vertical diameters, itch, grew, bleed, and elevation); (2) 6-class classification using images and clinical features as the previous task, but with image augmentation on the training set using 8 RandAugment <ref type="bibr" target="#b19">(Cubuk et al., 2020)</ref> operations: autoContrast, equalize, invert, rotate, posterize, solarize, color, and contrast; (3) 6-class classification the same as previous task, but using an upsampled subset for four minor skin conditions (melanoma, squamous cell carcinoma, seborrheic keratosis, and nevus) with image augmentation during training to mitigate the class imbalance problem. The latter two auxiliary tasks are included in the training mixture to help the model to distinguish among different types of clinical observations. We also formulate the skin condition classification problem as a close-ended multiple-choice question setup as shown in Figure <ref type="figure" target="#fig_0">D1</ref>, and report the prediction accuracy for this task. • Path-VQA is a pathology VQA dataset, which consists of 998 pathology images with 32799 QA pairs <ref type="bibr" target="#b45">(He et al., 2020)</ref>. All images are extracted from medical textbooks and online digital libraries. Each image is associated one or more questions regarding different aspects of the pathology imaging including color, location, appearance, shape, etc. 50.2% of the QA pairs are open-ended questions (divided into 7 categories: what, where, when, whose, how, and how much/how many). 49.8% of the QA pairs are close-ended questions with simple "yes/no" answer. We adopt the official splits where the training/validation/testing splits contain 19755, 6279, and 6761 QA pairs, respectively.</p><p>• Slake-VQA is a bilingual (English and Chinese) radiology image VQA dataset <ref type="bibr" target="#b70">(Liu et al., 2021)</ref>, containing 642 annotated images with 14028 question-answer pairs covering three imaging modalities (CT, MRI, and chest X-Rays), 39 organ systems, and 12 diseases.</p><p>Questions are either open-ended or closed-ended related to various aspects of the radiology images, including plane, quality, position, organ, abnormality, size, color, shape, knowledge graph, etc. The training/validation/testing splits contain 9849, 2109, and 2070 QA pairs, respectively. • ROCO (Radiology Objects in Context) dataset is a large-scale medical and multimodal imaging dataset <ref type="bibr" target="#b96">(Pelka et al., 2018)</ref>. The ROCO images are from publications available on the PubMed Central Open Access FTP mirror, which are automatically labeled as either radiology or nonradiology. Each image has its caption, keywords, the corresponding UMLS Semantic Types (SemTypes), and UMLS Concept Unique Identifiers (CUIs). We use the official training set across radiology and non-radiology, which contain 29907 image-caption pairs, and set up a captioning task for fine-tuning. We only include the images under CC BY, CC BY ND, CC BY SA and CC0 licenses in ROCO. • ECG-QA is a sensor-text multimodal benchmark for assessing cardiac health <ref type="bibr">(Oh et al., 2023)</ref>.</p><p>It is the first QA dataset specifically designed for electrocardiogram analysis based on PTB-XL <ref type="bibr" target="#b127">(Wagner et al., 2020)</ref>, containing diverse question templates, each validated by an ECG expert to ensure clinical utility. Strong performance on ECG-QA indicates the ability to grasp complex medical concepts and their connections to raw waveform signals. ECG-QA contains two types of questions involving (1) single ECG and (2) comparing two ECGs; each question type consists of (1) yes/no questions, (2) multiple-choice questions, and</p><p>(3) open-ended questions to provide ECG-related attributes. We focus on single ECG questions in this work, which contain 159306, 31137, and 41093 samples for train, validation and test sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Multimodal evaluation datasets</head><p>In addition to the in-distribution datasets (details in the above section), we include three out-ofdistribution datasets to evaluate the mulitmodal capability of Gemini: Table <ref type="table" target="#tab_4">D2</ref> | Overview of the datasets used for multimodal understanding evaluation. OOD: out-of-distribution dataset.</p><p>• New England Journal of Medicine (NEJM) Image Challenge is a renowned clinical case challenge series that tests the diagnostic acumen and visual observation skills of medical professionals worldwide (The New England Journal of Medicine, 2024). Every week, the NEJM presents a clinical image accompanied by a brief case description. The images include radiographic images, natural and dermatoscopic skin images, electrocardiograms, histopathology images, endoscopy images, and ophthalmoscopy images. Readers are invited to carefully analyze the photograph, consider the patient's history, and select the final diagnosis from five possible diagnosis candidates. We collect 942 NEJM Image Challenge cases from 2005 to 2023. Each case consists of a medical image and an associated question (e.g., "What is the most likely diagnosis?"), five multiple-choice options, and a correct answer. Some cases additionally provide text captions with relevant clinical context or other background information in the question. We have collected 942 cases in total, yet 934 cases are evaluated in the end for the fair comparison (until October 12, and two cases, 20160519 and 20111103 were not evaluated due to GPT-4V filters preventing images that are assumed to be sexually explicit <ref type="bibr" target="#b9">(Buckley et al., 2023)</ref>). • USMLE-MM (Multimodal) is a multimodal multiple-choice question dataset with 46 questions identified in the sample exams provided by www.usmle.org, which includes images in the question. The sample exams are used for USMLE preparation.</p><p>• MMMU-HM (health and medicine) is a subset of the publicly available benchmark, MMMU (Massive Multi-discipline Multimodal Understanding) validation set <ref type="bibr" target="#b143">(Yue et al., 2023)</ref>. MMMU-HM includes 150 questions related to basic medical science, clinical medicine, diagnostics and laboratory medicine, pharmacy, and public health domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Additional results for multimodal tasks</head><p>ECG-QA To expand Med-Gemini's capability to process raw biomedical signals for ECG-QA tasks, we augment Gemini 1.0 Nano with an ECG-specific encoder and fine-tune using two approaches: keeping Gemini model unchanged (frozen) and fine-tuning Gemini model (unfrozen). We compare our Med-Gemini-S 1.0 to their baseline counterparts: our model with frozen Gemini model to GPT-4 with SE-WRN ECG features in input prompts <ref type="bibr">(Oh et al., 2023)</ref> and our model with unfrozen Gemini model to an ECG foundation model based on M 3 AE <ref type="bibr">(Oh et al., 2023)</ref>. Med-Gemini-S 1.0 with frozen and unfrozen Gemini yield accuracies of 57.7% and 58.4% on single ECG questions, respectively, outperforming GPT-4 (51.6%) by 6.1% and M 3 AE (57.6%) by 0.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional details on long-context understanding tasks E.1. Long-context evaluation datasets</head><p>• MIMIC-III Needle-in-a-Haystack is a specially curated dataset from MIMIC-III <ref type="bibr" target="#b57">(Johnson et al., 2016)</ref> for subtle medical condition search-retrieval task over long EHRs. It is designed to mimic a clinically-relevant "needle-in-a-haystack" challenging problem <ref type="bibr" target="#b34">(Gemini Team, Google, 2023)</ref>. MIMIC-III is a large publicly-available medical database that contains medical records of patients admitted to intensive care units. We randomly select unstructured medical notes from 44 unique patients with more than 100 "high-value"<ref type="foot" target="#foot_1">foot_1</ref> clinical notes. To construct "needle-in-a-haystack" examples, we use our prior work <ref type="bibr" target="#b27">(Feder et al., 2022)</ref>, which aims at identifying the problem list (conditions/symptoms/procedures) from patients' collection of EHR documents through (1) labeling all mentions (text spans) of problems on the medical records with machine learning based annotators; (2) rule-based selection and aggregation of mentions to decide whether a problem is actually existent or not. We select the examples where there is only 1 evidence snippet identified in the aggregation step, and then randomly sample 100 negative and 100 positive examples determined by the rule-based method. 200 selected examples are then sent to 3 human medical raters to decide whether the problem actually exists or not. Specifically, the from YouTube, and localized the visual answer to those questions by annotating their timestamps in the video, i.e., identifying the timestamp span given text question query. The mean duration time of these videos is 383.29 seconds. We follow the official data split, where 2710, 145, and 155 questions and visual answers are used for training, validation, and testing respectively. However, 7 questions are excluded due to the YouTube video access restriction (private videos, removed videos). • Cholec80 and Cholec80-CVS. Cholec80 is a dataset containing 80 high-quality videos of laparoscopic cholecystectomy performed by 13 surgeons <ref type="bibr" target="#b123">(Twinanda et al., 2016)</ref>. Cholec80 is one of the most popular benchmarks for research in laparoscopic cholecystectomy video analysis with deep learning, and it has been widely used in recent research, on different video understanding tasks, including temporal segmentation of surgical phases <ref type="bibr" target="#b13">(Chen et al., 2018;</ref><ref type="bibr" target="#b37">Golany et al., 2022)</ref>, and surgical tool detection <ref type="bibr" target="#b66">(Leifman et al., 2022;</ref><ref type="bibr" target="#b87">Nwoye et al., 2019)</ref>. Cholec80-CVS <ref type="bibr" target="#b106">(Ríos et al., 2023)</ref> contains Critical View of Safety (CVS) criteria annotations, provided by skilled surgeons, for each video in the Cholec80 dataset. The CVS <ref type="bibr" target="#b116">(Strasberg and Brunt, 2010</ref>) is a mandatory method, defined by three visual criteria, used for secure identification of the cystic duct and cystic artery to minimize the risk of Bile Duct Injury (BDI). For each video in Cholec80, skilled surgeons selected different video segments where at least one CVS criteria was satisfied, and then for each selected video segment, the surgeons assigned a score of 0, 1, or 2 for each of the three CVS criteria, following an extension of the original scoring system proposed by <ref type="bibr" target="#b107">(Sanford and Strasberg, 2014)</ref> and <ref type="bibr" target="#b78">(Mascagni et al., 2021)</ref>. In total, Cholec80-CVS provides CVS criteria annotations for 572 video segments within Cholec80 videos. We assess the performance of Med-Gemini-M 1.5 in comparison to GPT-4V and Resnet3D. It is important to note that GPT-4V does not officially support video data as input. Therefore, we sample frames from each video clip at a rate of 1 frame per second and combine a sequence of frames as input to the model. During our experimentation, we observe that GPT-4V's vision context length is limited, and we are able to insert up to 300 low-resolution images. Consequently, we filter out all video clips longer than 5 minutes. For fair comparison, we evaluate Med-Gemini-M 1.5 on the same filtered subset of video clips. To conduct the evaluation on Resnet3D, we randomly split the dataset into 5 consecutive folds and assess the performance on each validation fold separately. The average accuracy across all five folds is reported.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision Recall F1</head><p>Heuristic-based baseline 0.85 (0.78, 0.92) 0.73 (0.64, 0.80) 0.78 (0.72, 0.84) Med-Gemini-M 1.5 (one-shot) 0.77 (0.66, 0.86) 0.76 (0.67, 0.86) 0.77 (0.68, 0.84) Table <ref type="table" target="#tab_4">E2</ref> | Performance comparison of Med-Gemini-M 1.5 versus the heuristic-based annotation-aggregation baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 |</head><label>1</label><figDesc>Figure1| Overview of our contributions. We introduce Med-Gemini, a family of highly capable, multimodal medical</figDesc><graphic coords="2,441.65,526.73,52.92,73.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 |</head><label>2</label><figDesc>Figure 2 | Self-training and search tool-use. The left panel illustrates the self-training with search framework used to fine-tune Med-Gemini-L 1.0 for advanced medical reasoning and use of web search. This framework iteratively generates reasoning responses (CoTs) with and without web search, improving the model's ability to utilize external information for accurate answers. The right panel illustrates Med-Gemini-L 1.0's uncertainty-guided search process at inference time. This iterative process involves generating multiple reasoning paths, filtering based on uncertainty, generating search queries to resolve ambiguity, and incorporating retrieved search results for more accurate responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 |</head><label>3</label><figDesc>Figure 3 | Generalization of Med-Gemini-L 1.0 with web search to two additional text-based benchmarks. (a): Comparison of Med-Gemini-L 1.0's top-k accuracy on the NEJM CPC benchmark with prior SoTA LLMs and clinicians, with and without search. (b): Comparison between Med-Gemini-L 1.0 and SoTA models on the GeneTuring dataset modules. The bars represent the proportion of correct, incorrect, and abstention responses for each model.</figDesc><graphic coords="16,70.71,87.13,173.55,151.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 |</head><label>4</label><figDesc>Figure 4 | Ablation analysis and label uncertainty on MedQA. (a): Impact of self-training and uncertainty-guided search on Med-Gemini-L 1.0's accuracy on MedQA. Self-training and each round of search contribute to significant performance improvements. (b): Med-Gemini-L 1.0's accuracy (blue) and remaining questions (red) on MedQA after re-labeling by at least three US physicians per question. Filtering questions with missing information, label errors, or ambiguous groundtruth further improves accuracy. The error bars correspond to standard error across cases in (a) and standard deviation across bootstrapped annotations in (b).</figDesc><graphic coords="17,70.68,90.41,197.22,172.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 |</head><label>5</label><figDesc>Figure 5 | Evaluation of Med-Gemini-M 1.0 on long-form text-based tasks via side-by-side comparison with experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 |</head><label>6</label><figDesc>Figure 6 | Example of a hypothetical multimodal diagnostic dialogue with Med-Gemini-M 1.5 in a dermatology setting. (a)The user interacts with the our multimodal model, Med-Gemini-M 1.5, acting as a patient based on a case from SCIN<ref type="bibr" target="#b132">(Ward et al., 2024)</ref>, an external dataset that is not included in the fine-tuning mixture. This system would not be appropriate for use in real-world diagnostic tasks without considerable further research and development. Nonetheless, this example shows indicatively the future initial promise of how the combination of the conversational capability inherited from the original Gemini model and the newly acquired multimodal medical knowledge from fine-tuning can potentially enable attributes of a rich multi-turn diagnostic dialogue. In this interaction, Med-Gemini-M 1.5 asks for a picture when it is not provided (multimodal information acquisition), arrives at the right diagnosis efficiently (open-ended diagnosis), explains the reasoning by integrating the relevant visual features and other gathered patient symptoms (explainability), answers questions about treatment options while deferring aptly to experts for the final decision. (b) Shows feedback collected from dermatologists to evaluate qualitative the contents of the diagnostic dialogue, specifically asking them to comment on both positive and negative aspects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>FINDINGS:The heart is at the upper limits of normal size. The mediastinal and hilar contours appear within normal limits. The lungs appear clear. There are no pleural effusions or pneumothorax. Mild degenerative changes are noted along the lower thoracic spine. IMPRESSION: No evidence of acute disease.Mild degenerative changes are noted along the lower thoracic spine. This can be seen by the slight narrowing of the intervertebral disc spaces and the presence of small osteophytes (bone spurs) along the anterior margins of the vertebral bodies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(not provided to model) FINDINGS: The cardiomediastinal and hilar contours are normal. The lungs are well expanded and clear, without focal consolidation, pleural effusion or pneumothorax. Mild degenerative changes are seen in the thoracic spine. IMPRESSION: No acute cardiopulmonary pathology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 |</head><label>7</label><figDesc>Figure 7 | Example of a hypothetical multimodal diagnostically-assistive dialogue with Med-Gemini-M 1.5 in the radiology setting. (a)In this interaction, Med-Gemini-M 1.5 demonstrates its ability to analyze a chest X-ray (CXR) and conduct a hypothetical realistic dialogue with a primary care physician. As above, Med-Gemini-M 1.5 is not suitable for this real-world use without further research. However, this example demonstrates initial promise, wherein Med-Gemini-M 1.5 identifies mild degenerative changes along the spine and can answer questions about the reasoning which led to this finding, demonstrate general medical knowledge about degenerative disk disease and distinguish between correlation and causation in relation to a patient history of back-pain. Finally, in this example Med-Gemini-M 1.5 is able to explain its findings in layperson's terms, demonstrating its potential for facilitating patient understanding and communication in clinical settings. The ground truth report for this CXR is provided. (b) Feedback from a radiologist about the quality of this radiology dialogue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Response•Figure 8 |</head><label>8</label><figDesc>Figure 8 | Example of Med-Gemini-M 1.5's long-context capabilities on long EHR understanding (MIMIC-IIINeedle-in-a-Haystack). Med-Gemini-M 1.5 performs a two-step process for determining whether a patient has a history of a specific condition based on their extensive EHR records. (a) Step 1 (Retrieval): Med-Gemini-M 1.5 identifies all mentions of "hypothermia" within the EHR notes, providing direct quotes [e.g., "+sweats. Oral temp 93.7. Transferred to medical intensive care unit (MICU)"] and note IDs for each mention. (b) Step 2 (Deciding the existence): Med-Gemini-M 1.5 then evaluates the relevance of each retrieved mention, categorizing them as explicit confirmation, strong indication, or relevant mention of hypothermia. Based on this analysis, the model concludes that the patient does have a history of hypothermia, providing clear reasoning for its decision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>…Figure 10 |</head><label>10</label><figDesc>Figure 10 | Example of Med-Gemini-M 1.5's long-context capabilities on medical instructional videos. Med-Gemini-M 1.5 analyzes a video from the Medical Video Question Answering (MedVidQA) dataset to answer a specific question about relieving calf strain. The model identifies the relevant video segment (02:22-02:58) where the physical therapist explains and demonstrates the exercise for this condition. The MedVidQA ground truth time span annotation is 02:22-03:00.</figDesc><graphic coords="25,105.60,414.73,107.33,53.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 |</head><label>11</label><figDesc>Figure 11 | Example of Med-Gemini-M 1.5's long-context capabilities on surgical action video span prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>…Figure 12 |</head><label>12</label><figDesc>Figure12| Example of Med-Gemini-M 1.5's long-context capabilities on surgical video dialogue. Med-Gemini-M 1.5 analyzes a video clip from the Cholec80 dataset comprising footage from a laparoscopic cholecystectomy (a keyhole operation to remove the gallbladder). The model demonstrates its ability to analyse the video and conduct a realistic dialogue with a student learning about the procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>., Xiao, T., Guo, J., &amp; Su, Z.Frontiers in EndocrinologyCritical enzymatic functions of FTO in obesity and cancer Deng, X., Su, R., Stanford, S., &amp; Chen, J.Current Diabetes ReportsFTO and obesity: mechanisms of association. Zhao, X., Yang, Y., Sun, B. F., Zhao, Y. L., &amp; Yang, Y. G. The American Journal of Clinical Nutrition FTO genotype impacts food intake and corticolimbic activation Melhorn, S. J., Askren, M. K., Chung, W. K., Kratz, M., Bosch, T. A., Tyagi, V., ... &amp; Schur, E. A. New England Journal of Medicine FTO obesity variant circuitry and adipocyte browning in humans. New England Journal of Medicine Claussnitzer, M., Dankel, S. N., Kim, K. H., Quon, G., Meuleman, W., Haugen, C., ... &amp; Kellis, M. Obesity Genetics of obesity: what we have learned over decades of research Bouchard, C. Science Advances Linking the FTO obesity rs1421085 variant circuitry to cellular, metabolic, and organismal phenotypes in vivo Laber, S., Forcisi, S., Bentley, L., Petzold, J., Moritz, F., Smirnov, K. S., ... &amp; Cox, R. D. Nature Communications Multiple novel gene-by-environment interactions modify the effect of FTO variants on body mass index Young, A. I., Wauthier, F., &amp; Donnelly, P. Cell Metabolism Obesity and FTO: changing focus at a complex locus Tung, Y. L., Yeo, G. S., O'Rahilly, S., &amp; Coll, A. P. Obesity The fat mass-and obesity-associated (FTO) gene to obesity: lessons from mouse models Chang, J. Y., Park, J. H., Park, S. E., Shon, J., &amp; Park, Y. J. Nature Reviews Genetics The genetics of obesity: from discovery to biology. Loos, R. J., &amp; Yeo, G. S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 |</head><label>14</label><figDesc>Figure 14 | Example of Med-Gemini-M 1.5's long-context capabilities on scientific information synthesis. Med-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure C1 |</head><label>C1</label><figDesc>Figure C1 | MedQA (USMLE) results after re-annotation. Results complementary to Figure 3b showing accuracy (blue)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Is all the information in the simple summary factually supported by the technical summary? -A Strongly Preferred -A Somewhat Preferred -Tied -B Somewhat Preferred -B Strongly Preferred Coverage: Are the most important takeaways for a lay audience included in the simple summary? -A Strongly Preferred -A Somewhat Preferred -Tied -B Somewhat Preferred -B Strongly Preferred Succinctness: Does the simple summary only contain the most important takeaways for a lay audience? -A Strongly Preferred -A Somewhat Preferred -Tied -B Somewhat Preferred -B Strongly Preferred Reading Level: Is the reading-level of the simple summary appropriate for a lay audience? -A Strongly Preferred -A Somewhat Preferred -Tied -B Somewhat Preferred -B Strongly Preferred Overall: What is the overall quality of the simple summary for a lay audience? -A Strongly Preferred -A Somewhat Preferred -Tied -B Somewhat Preferred -B Strongly Preferred Three different U.S. board certified physicians each evaluates all 25 examples. Ratings are aggregated similar to the referral letter task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Furthermore, following the procedure proposed in Tanno et al. (2024), we filter out the training examples whose reports reference prior studies and only keep examples where the report only refers to findings present in the input image. This aims to mitigate hallucination of references to non-existing prior reports, a common issue raised by multiple lines of research Ramesh et al. (2022) and Hyland et al. (2023). The evaluation of MIMIC-CXR will be reported in a subsequent paper. • PAD-UFES-20 includes 2298 clinical skin lesion images collected from various smartphone devices with different resolutions, sizes, and lighting conditions through the Dermatological and Surgical Assistance Program at the Federal University of Espírito Santo (UFES-Brazil) (Pacheco</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Von Willebrand disease, which is the most common inherited bleeding disorder, affecting platelet function owing to decreased von Willebrand factor activity. Hypofibrinogenemia • Liver cirrhosis (the liver makes most of the clotting factors, including those that are vitamin K-dependent ones); diseases of the liver may result in an inadequate quantity of clotting factors, prolonging the aPTT. Vitamin K deficiency: The synthesis of some clotting factors requires vitamin K, so vitamin K deficiency results in an inadequate quantity of intrinsic system and common pathways clotting factors, as a result the aPTT is prolonged. Disseminated intravascular coagulation (DIC): The clotting factors involved in the intrinsic pathway are consumed, prolonging the aPTT. Source: https://emedicine.medscape.com/article/2085837-overviewTargetThe patient's symptoms of menorrhagia and easy bruising, along with a prolonged PTT, suggest a bleeding disorder. Hemophilia A (option A) is an X-linked recessive disorder that typically presents with a prolonged PTT and normal PT. However, it is unlikely in this case as the patient is female and has a family history of her mother having similar symptoms. Lupus anticoagulant (option B) can cause a prolonged PTT, but it is typically associated with thrombosis rather than bleeding. Protein C deficiency (option C) can cause thrombosis, but it does not affect PTT. According to the search results, "Von Willebrand disease (vWD) is the most common inherited bleeding disorder, affecting platelet function owing to decreased von Willebrand factor activity." It can present with menorrhagia, easy bruising, and a prolonged PTT. The patient's family history of her mother having similar symptoms also supports this diagnosis. Therefore, the most likely cause of this patient's symptoms is (D) Von Willebrand disease.</figDesc><table /><note><p><p><p>The patient's vital signs include: heart rate 98/min, respiratory rate 14/min, temperature 36.1°C (96.9°F), and blood pressure 110/87 mm Hg. Physical examination is unremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43 seconds. Which of the following is the most likely cause of this patient's symptoms? (A) Hemophilia A (B) Lupus anticoagulant (C) Protein C deficiency (D) Von Willebrand disease</p>Search Results</p>Partial thromboplastin time (PTT) and activated partial thromboplastin time (aPTT) are used to test for the same functions; however, in aPTT, an activator is added that speeds up the clotting time and results in a narrower reference range. The aPTT is considered a more sensitive version of the PTT and is used to monitor the patient's response to heparin therapy. The reference range of the aPTT is 30-40 seconds. The reference range of the PTT is 60-70 seconds. In patients receiving anticoagulant therapy, the reference range is 1.5-2.5 times the control value in seconds. Critical values that should prompt a clinical alert are as follows: aPTT: More than 70 seconds (signifies spontaneous bleeding). PTT: More than 100 seconds (signifies spontaneous bleeding). eMedicine Logo. Next: Interpretation. A prolonged aPTT result may indicate the following : Congenital deficiencies of intrinsic system clotting factors such as factors VIII, IX, XI, and XII, including hemophilia A and hemophilia B (Christmas disease), two inherited bleeding disorders resulting from a deficiency in factors VIII and IX, respectively. Congenital deficiency of Fitzgerald factor (prekallikrein). Answer: (D) Von Willebrand disease</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>clinico-pathological conferences (NEJM CPC): a</head><label></label><figDesc></figDesc><table><row><cell>• MedQA (USMLE): a close-ended multiple-choice (4 options) dataset with 1273 USMLE style</cell></row><row><cell>test questions curated by Jin et al. (2021).</cell></row><row><cell>• NEJM dataset comprising complex diagnostic</cell></row><row><cell>case challenges in the medical journal, New England Journal of Medicine (NEJM) curated</cell></row><row><cell>by McDuff et al. (2023).</cell></row><row><cell>• GeneTuring: a dataset that includes 600 open/close-ended QA pairs to evaluate genomic</cell></row><row><cell>knowledge of LLMs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 |</head><label>2</label><figDesc>Multimodal evaluation. Performance comparison of Med-Gemini versus state-of-the-art (SoTA) methods.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Multimodal fine-tuned</cell><cell>Metric</cell><cell cols="2">Med-Gemini SoTA</cell><cell>SoTA method</cell><cell>Reference</cell></row><row><cell>Close-ended VQA</cell><cell>NEJM Image Challenge</cell><cell></cell><cell>Accuracy</cell><cell>69.7  *</cell><cell>61.0</cell><cell>GPT-4V</cell><cell>Buckley et al. (2023)</cell></row><row><cell>Close-ended VQA</cell><cell>USMLE-MM</cell><cell></cell><cell>Accuracy</cell><cell>93.5  *</cell><cell>80.4</cell><cell>GPT-4V</cell><cell>Reproduced</cell></row><row><cell>Close/open-ended VQA</cell><cell>MMMU-HM</cell><cell></cell><cell>Accuracy</cell><cell>67.3  *</cell><cell>64.7</cell><cell>GPT-4V</cell><cell>Yue et al. (2023)</cell></row><row><cell>Close-ended Signal QA</cell><cell>ECG-QA</cell><cell>✓</cell><cell>Accuracy</cell><cell>57.7  ‡</cell><cell>51.6</cell><cell>GPT-4 with SE-WRN</cell><cell>Oh et al. (2023)</cell></row><row><cell>Open/Close-ended VQA</cell><cell>Slake-VQA</cell><cell>✓</cell><cell>Token F1</cell><cell>87.5  †</cell><cell>89.3</cell><cell>Med-PaLM M</cell><cell>Tu et al. (2024a)</cell></row><row><cell>Open/Close-ended VQA</cell><cell>Path-VQA</cell><cell>✓</cell><cell>Token F1</cell><cell>64.7  †</cell><cell>62.7</cell><cell>Med-PaLM M</cell><cell>Tu et al. (2024a)</cell></row><row><cell>Classification</cell><cell>PAD-UFES-20 6-class</cell><cell>✓</cell><cell>Accuracy</cell><cell>85.9  †</cell><cell>88.0</cell><cell>Med-PaLM M</cell><cell>Tu et al. (2024a)</cell></row><row><cell>Classification</cell><cell>PAD-UFES-20 6-class</cell><cell>✓</cell><cell>Accuracy</cell><cell>78.8  †</cell><cell>N/A</cell><cell>N/A</cell><cell>New Split</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 |</head><label>3</label><figDesc>Long</figDesc><table /><note><p>-context evaluation. Performance comparison of Med-Gemini-M 1.5 versus the state-of-the-art (SoTA) methods. Note that 7 out of 155 questions in MedVidQA are not answerable due to YouTube video access (private, removed).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A1 | Performance results of bar plot in Figure</head><label>A1</label><figDesc></figDesc><table><row><cell>Advanced Text Reasoning</cell><cell>NEJM CPC</cell><cell>Top-10 Accuracy</cell><cell>72.3</cell><cell>59.1</cell><cell>50.0</cell><cell>McDuff et al. (2023)</cell></row><row><cell></cell><cell>GeneTuring</cell><cell>Averaged accuracy</cell><cell>53.3</cell><cell>48.6</cell><cell>48.6</cell><cell>Hou and Ji (2023)</cell></row><row><cell></cell><cell>MedQA</cell><cell>Accuracy</cell><cell>91.1</cell><cell>90.2</cell><cell>90.2</cell><cell>Nori et al. (2023)</cell></row><row><cell>Multimodal Understanding</cell><cell>NEJM Image</cell><cell>Accuracy</cell><cell>69.7</cell><cell>61.0</cell><cell>61.0</cell><cell>Buckley et al. (2023)</cell></row><row><cell></cell><cell>USMLE-MM</cell><cell>Accuracy</cell><cell>93.5</cell><cell>80.4</cell><cell>80.4</cell><cell>Reproduced</cell></row><row><cell></cell><cell>ECG-QA</cell><cell>Accuracy</cell><cell>57.7</cell><cell>51.6</cell><cell>51.6</cell><cell>Oh et al. (2023)</cell></row><row><cell></cell><cell>MMMU-HM</cell><cell>Accuracy</cell><cell>67.3</cell><cell>64.7</cell><cell>64.7</cell><cell>Yue et al. (2023)</cell></row><row><cell></cell><cell>Path-VQA</cell><cell>Token F1</cell><cell>64.7</cell><cell>62.7</cell><cell>36.0 (Reproduced)</cell><cell>Tu et al. (2024a)</cell></row><row><cell></cell><cell>PAD-UFES-20</cell><cell>Accuracy</cell><cell>85.9</cell><cell>88.0</cell><cell>50.0 (Reproduced)</cell><cell>Tu et al. (2024a)</cell></row><row><cell></cell><cell>Slake-VQA</cell><cell>Token F1</cell><cell>87.5</cell><cell>89.3</cell><cell>41.0 (Reproduced)</cell><cell>Tu et al. (2024a)</cell></row><row><cell>Long-context Processing</cell><cell>MedVidQA</cell><cell>mIoU</cell><cell>43.4</cell><cell>27.5</cell><cell>N/A</cell><cell>Li et al. (2022)</cell></row><row><cell></cell><cell>MedVidQA w/ subtitles</cell><cell>mIoU</cell><cell>65.8</cell><cell>58.3</cell><cell>N/A</cell><cell>Weng and Li (2023)</cell></row><row><cell></cell><cell>Long EHR</cell><cell>F1</cell><cell>0.77</cell><cell>0.78</cell><cell>N/A</cell><cell>Feder et al. (2022)</cell></row><row><cell></cell><cell>Surgery Video CVS Assessment</cell><cell>Accuracy</cell><cell>50.0</cell><cell>67.0</cell><cell>0.290</cell><cell>Reproduced</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>C.1. Text-based fine-tuning &amp; evaluation datasets</head><label></label><figDesc></figDesc><table><row><cell>Task type</cell><cell>Datasets</cell><cell>Sample size</cell><cell>Description</cell><cell>Reference</cell></row><row><cell>Multiple-choice question answering</cell><cell>MedQA</cell><cell>10177</cell><cell>Multiple-choice questions from MedQA</cell><cell>Jin et al. (2021)</cell></row><row><cell>Multiple-choice question answering with reasoning</cell><cell>MedQA-R, MedQA-RS</cell><cell>20354</cell><cell>Multiple-choice questions from MedQA with synthetically generated reasoning examples</cell><cell>Novel</cell></row><row><cell>Long-form question answering</cell><cell>HealthSearchQA, LiveQA, MedicationQA</cell><cell>260</cell><cell>Clinician-written long-form responses</cell><cell>Singhal et al. (2023a)</cell></row><row><cell>Summarization</cell><cell>MIMIC-summaries</cell><cell>65</cell><cell>Clinician-written summaries of medical notes</cell><cell>Tu et al. (2024b)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table C1 | Overview of datasets used for text-based instruction fine-tuning.</head><label>C1</label><figDesc>The dataset mixture and synthetic data are curated to improve Med-Gemini-L 1.0's reasoning and ability to make use of web search.</figDesc><table><row><cell>Task Type</cell><cell>Modality</cell><cell>Dataset</cell><cell>Test sample size</cell><cell>Description</cell><cell>Reference</cell></row><row><cell>Close-ended QA</cell><cell>Text</cell><cell>MedQA</cell><cell>1273</cell><cell>US medical licensing exam-style, multiple-choice</cell><cell>Jin et al. (2021)</cell></row><row><cell>Open-ended QA</cell><cell>Text</cell><cell>NEJM CPC</cell><cell>303</cell><cell>Complex diagnostic challenging in NEJM</cell><cell>McDuff et al. (2023)</cell></row><row><cell>Open/Close-ended QA</cell><cell>Text</cell><cell>GeneTuring</cell><cell>600</cell><cell>Commonly seen tasks in genomics research</cell><cell>Hou and Ji (2023)</cell></row><row><cell>Long-form generation</cell><cell>Text</cell><cell>Clinical Abstraction</cell><cell>81</cell><cell>Meaningful summarization in clinical practice and research</cell><cell>Appendix C.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table C2 | Overview of the evaluation benchmarks used for text-based reasoning tasks.</head><label>C2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Rater agreement in % against vote</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>All</cell><cell cols="2">Med-Gemini incorrect Med-Gemini correct</cell></row><row><cell>info missing</cell><cell>majority</cell><cell>95.6</cell><cell>95.6</cell><cell>95.6</cell></row><row><cell cols="3">info missing unanimous 94.0</cell><cell>94.4</cell><cell>94.0</cell></row><row><cell>label errors</cell><cell>majority</cell><cell>89.6</cell><cell>80.8</cell><cell>90.5</cell></row><row><cell>label errors</cell><cell cols="2">unanimous 87.6</cell><cell>74.6</cell><cell>88.8</cell></row><row><cell>ambiguous</cell><cell>majority</cell><cell>94.9</cell><cell>92.9</cell><cell>95.0</cell></row><row><cell>ambiguous</cell><cell cols="2">unanimous 94.6</cell><cell>92.3</cell><cell>94.9</cell></row><row><cell></cell><cell cols="3">Rater agreement of answers as average overlap in %</cell><cell></cell></row><row><cell></cell><cell></cell><cell>All</cell><cell cols="2">Med-Gemini incorrect Med-Gemini correct</cell></row><row><cell>before</cell><cell>majority</cell><cell>49.9</cell><cell>36.4</cell><cell>51.2</cell></row><row><cell>before</cell><cell cols="2">unanimous 49.1</cell><cell>36.0</cell><cell>50.4</cell></row><row><cell>after</cell><cell>majority</cell><cell>75.9</cell><cell>54.8</cell><cell>77.9</cell></row><row><cell>after</cell><cell cols="2">unanimous 74.6</cell><cell>53.9</cell><cell>76.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table C3 |</head><label>C3</label><figDesc>Annotation agreement. Top: Agreement of individual ratings against the majority or unanimous vote for various rating tasks of interest. Bottom: Agreement of raters' answers in terms of average overlap before and after having revealed the MedQA ground truth.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>↑ Top-10 ↑ Top-1 ↑ Top-10 ↑ Top-1 ↑ Top-10 ↑ Top-1 ↑ Top-10 ↑ Top-1 ↑ Top-10 ↑</figDesc><table><row><cell></cell><cell cols="2">Previous SoTA</cell><cell></cell><cell cols="2">Clinician</cell><cell></cell><cell></cell><cell cols="2">Med-Gemini</cell><cell></cell></row><row><cell></cell><cell cols="2">AMIE</cell><cell cols="2">Without Search</cell><cell cols="2">with Search</cell><cell cols="2">Without Search</cell><cell cols="2">With Search</cell></row><row><cell cols="2">Metrics Top-1 Internal Med (159 Cases) 27.7%</cell><cell>61.6%</cell><cell>15.5%</cell><cell>34.6%</cell><cell>24.5%</cell><cell>47.8%</cell><cell>24.5%</cell><cell>64.8%</cell><cell>31.4%</cell><cell>74.8%</cell></row><row><cell>Neurology (42 Cases)</cell><cell>26.8%</cell><cell>56.1%</cell><cell>17.1%</cell><cell>31.7%</cell><cell>22.0%</cell><cell>36.6%</cell><cell>31.0%</cell><cell>66.7%</cell><cell>26.2%</cell><cell>61.9%</cell></row><row><cell>Pediatrics (33 Cases)</cell><cell>30.3%</cell><cell>45.5%</cell><cell>6.1%</cell><cell>22.7%</cell><cell>12.1%</cell><cell>33.3%</cell><cell>21.2%</cell><cell>45.5%</cell><cell>12.1%</cell><cell>48.5%</cell></row><row><cell>Psychiatry (10 Cases)</cell><cell>50.0%</cell><cell>70.0%</cell><cell>20.0%</cell><cell>50.0%</cell><cell>20.0%</cell><cell>60.0%</cell><cell>50.0%</cell><cell>100.0%</cell><cell>60.0%</cell><cell>90.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table C4 |</head><label>C4</label><figDesc>Breakdown of performance on NEJM case studies by speciality. is reported for the specialities with at least 10 cases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table C5 | Evaluation of Med-Gemini on long-form text-based tasks via side-by-side comparison with experts. Tasks</head><label>C5</label><figDesc></figDesc><table><row><cell>Task</cell><cell cols="5">Num. Examples Evaluation-Axis Expert Preferred Tied Med-Gemini</cell><cell>p-value</cell></row><row><cell>After-Visit Summary</cell><cell>31</cell><cell>Accuracy</cell><cell>19%</cell><cell>55%</cell><cell>26%</cell><cell>p &lt; 0.001</cell></row><row><cell></cell><cell></cell><cell>Coverage</cell><cell>48%</cell><cell>16%</cell><cell>35%</cell><cell>p = 0.861</cell></row><row><cell></cell><cell></cell><cell>Succinctness</cell><cell>29%</cell><cell>10%</cell><cell>61%</cell><cell>p = 0.017</cell></row><row><cell></cell><cell></cell><cell>Coherence</cell><cell>29%</cell><cell>13%</cell><cell>58%</cell><cell>p = 0.017</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>32%</cell><cell>3%</cell><cell>65%</cell><cell>p = 0.046</cell></row><row><cell>Referral Letter</cell><cell>25</cell><cell>Overall</cell><cell>0%</cell><cell>8%</cell><cell>92%</cell><cell>p &lt; 0.001</cell></row><row><cell>Cochrane Simplification</cell><cell>25</cell><cell>Accuracy</cell><cell>52%</cell><cell>8%</cell><cell>40%</cell><cell>p = 0.846</cell></row><row><cell></cell><cell></cell><cell>Coverage</cell><cell>12%</cell><cell>12%</cell><cell>76%</cell><cell>p &lt; 0.001</cell></row><row><cell></cell><cell></cell><cell>Succinctness</cell><cell>4%</cell><cell>4%</cell><cell>92%</cell><cell>p &lt; 0.001</cell></row><row><cell></cell><cell></cell><cell>Reading Level</cell><cell>0%</cell><cell>0%</cell><cell>100%</cell><cell>p &lt; 0.001</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>12%</cell><cell>0%</cell><cell>88%</cell><cell>p &lt; 0.001</cell></row></table><note><p>include generation of after-visit summaries, referral letters, and simplified summaries of systematic biomedical reviews.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>-</head><label></label><figDesc>Patient name/age/gender: This should summarize the patient's name, age and gender. It should use the format: "[Patient name], [age] year old [gender]". If the name is not mentioned in the note, please answer "Not available". -Today I was seen by: This field should provide the name of the provider. If the provider seen for the note being summarized is not mentioned, please answer "Not available". -I came in today for: This field should indicate the chief complaint or complaints that caused the visit. -New health issues identified today are: This field should indicate any new diagnoses or other issues identified as a result of the visit being summarized. If the issue is a pre-existing condition identified in the past, please answer "No new diagnosis". -Other health issues I have are: This field should indicate any pre-existing health issues identified in notes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table D1 | Overview of the datasets used for multimodal instruction fine-tuning.</head><label>D1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table E1 | Overview of the datasets used for the long-context capability evaluation</head><label>E1</label><figDesc></figDesc><table /><note><p>. MVAL: medical visual answer localization.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The additional question asking whether any option is appropriate (before revealing the multi-select) is due to technical constraints.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Consult notes, Progress notes, History and Physical notes and Discharge Summary notes authored by physicians/PAs/NPs/APRNs and Operative notes by physicians/PAs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Example of a negative example: a patient's records with one mention of Sepsis in a text segment "Received IV Ceftriaxone for concern of UTI/sepsis.". Here the patient should not be labeled as having history of sepsis as there is no definitive diagnosis of the condition without other context.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>This project was an extensive collaboration between many teams at <rs type="institution">Google Research and Google DeepMind</rs>. We thank <rs type="person">Taylan Cemgil</rs>, <rs type="person">Jake Sunshine</rs>, <rs type="person">Daniel Golden</rs>, <rs type="person">Pete Clardy</rs>, <rs type="person">Zoubin Ghahramani</rs> and <rs type="person">Dr. Gary Peltz</rs> (<rs type="affiliation">Stanford University</rs>) for their comprehensive review and detailed feedback on the manuscript. We also thank <rs type="person">Sami Lachgar</rs>, <rs type="person">Lauren Winer</rs>, <rs type="person">John Guilyard</rs>, and <rs type="person">Maggie Shiels</rs> for contributions to the narratives and visuals. We thank <rs type="person">Yun Liu</rs> for discussions, design, and preliminary analysis for the MedQA label uncertainty experiments. We are grateful to <rs type="person">Noam Velan</rs>, <rs type="person">Ira Ktena</rs>, <rs type="person">Eric Aboussouan</rs>, <rs type="person">Karan Singhal</rs>, <rs type="person">Shashir Reddy</rs>, <rs type="person">Aza Tulepbergenov</rs>, <rs type="person">Priya Gupta</rs>, <rs type="person">Rory Sayres</rs>, <rs type="person">Naama Hammel</rs>, <rs type="person">Jen McKay</rs>, <rs type="person">Peter Clardy</rs>, <rs type="person">Chu-ling Ko</rs>, <rs type="person">Abhinav Das</rs>, <rs type="person">Haiyang Yu</rs>, <rs type="person">Chang Liu</rs>, <rs type="person">Yuchen Liu</rs>, <rs type="person">Erica Moreira</rs>, <rs type="person">Jordan Grimstad</rs>, <rs type="person">Brett Hatfield</rs>, <rs type="person">Gordon Turner</rs>, <rs type="person">Jackie Barr</rs>, <rs type="person">Jim Winkens</rs>, <rs type="person">Jackie Barr</rs>, <rs type="person">Brian Cappy</rs>, <rs type="person">Pinal Bavishi</rs>, <rs type="person">Tim McConnell</rs>, <rs type="person">Ines Mezzorag</rs>, <rs type="person">Annisah Um</rs>'rani, <rs type="person">Christian Wright</rs>, <rs type="person">Divya Pandya</rs>, <rs type="person">Daireen Garcia</rs>, <rs type="person">Prachant Bradwell</rs>, <rs type="person">Alyssa Pierce</rs>, <rs type="person">Sarah-Jane Allen</rs>, <rs type="person">Erica Harland</rs>, <rs type="person">Jennifer Ye</rs>, <rs type="person">Praney Mittal</rs>, <rs type="person">Donny Cheung</rs>, <rs type="person">Andy Crowne</rs> and <rs type="person">Preeti Singh</rs> for their valuable technical support during our research. Finally, we are grateful to <rs type="person">Shravya Shetty</rs>, <rs type="person">Sushant Prakash</rs>, <rs type="person">Susan Thomas</rs>, <rs type="person">Michael Howell</rs>, <rs type="person">Karen DeSalvo</rs>, and <rs type="person">Zoubin Ghahramani</rs> for their support of this project.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-training with search</head><p>Uncertainty-guided search at inference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Data Availability</head><p>Except for the three clinical abstraction tasks, the remaining datasets used for development, benchmarking and evaluation of the AI systems are open source or otherwise accessible publicly with permissions. We will make our re-annotation of the MedQA (USMLE) dataset publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Code Availability</head><p>We are not open-sourcing model code and weights due to the safety implications of unmonitored use of such a system in medical settings. In the interest of responsible innovation, we will be working with research partners, regulators, and providers to validate and explore safe onward uses of our medical models and expect to make them available via Google Cloud APIs in due course.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Competing Interests</head><p>This study was funded by Alphabet Inc and/or a subsidiary thereof (' Alphabet'). All authors are (or were) employees of Alphabet and may own stock as part of the standard compensation package. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy:</head><p>Which summary is more accurate? (Are all statements in the summary correct?) -A -B -Tie Coverage: Which summary has better coverage? (Does it include all relevant aspects of the note?) -A -B -Tie Coherence: Which summary is easier to read? (Is the summary comprehensible to a consumer with no specific medical knowledge at a 6th-grade reading level?) -A -B -Tie Succinctness: Which summary is more succinct? (Is the summary longer than it needs to be?) -A -B -Tie Overall: Which summary feels higher quality to you? (Beyond these metrics, is there a gut feeling about the quality of the summary?) -A -B -Tie</p><p>Referral letter generation evaluation This task involves generating a referral letter to another healthcare provider given a de-identified outpatient medical note that contains a recommendation for a referral. A medical referral letter is a formal document written by a healthcare professional that requests another healthcare professional to evaluate or treat a patient. It serves as a communication tool between healthcare providers, ensuring continuity of care and facilitating appropriate treatment for the patient.</p><p>A set of de-identified medical notes requiring inter-specialty evaluation are manually selected by clinicians from a de-identified electronic healthcare record dataset. They then generate referral letters, which are further reviewed for quality by a U.S. board-certified clinician.</p><p>Med-Gemini-M 1.0 is prompted to generate a referral letter given the medical note as follows:</p><p>You will be provided with a medical note describing a patient visit. The medical note will contain a recommendation that the patient be referred to another healthcare provider. Your task is to generate the medical referral letter for this healthcare provider.</p><p>A medical referral letter is a formal document written by a healthcare professional that requests another healthcare professional to evaluate or treat a patient. It serves as a communication tool between healthcare providers, ensuring continuity of care and facilitating appropriate treatment for the patient.</p><p>Medical Note: {MEDICAL_NOTE} Referral Letter:</p><p>Physician raters are presented with the outpatient note, the clinician generated referral letter and our model's generated referral letter. They are blinded to the source of each referral letter and asked to perform the following comparison:  <ref type="bibr">)</ref>, demonstrating its ability to analyze and interpret diverse biomedical data formats. Note that the input for ECG-QA is the raw ECG sensor sequence, visualized here as a 12-lead ECG image from PTB-XL <ref type="bibr" target="#b127">(Wagner et al., 2020)</ref>. Also note that three MIMIC-CXR tasks are only used for instruction fine-tuning. raters are presented with the condition name and retrieved supporting evidence snippets. The raters are then asked to answer the question: "Select ALL problems the patient HAS HAD based on the evidence in the provided note excerpts." As a result, we have 121 positive examples and 79 negative examples based on the majority voting 3 (Krippendorff 's alpha at 0.77, see Table <ref type="table">E3</ref>). The majority-vote labels are then used as ground truth labels for the subsequent evaluation.</p><p>For each example, it consists of a set of medical records, a test question regarding to whether or not a condition of interest exists, and a binary ground truth label. The length of the medical records varies from 200K to 700K words.</p><p>• Medical Instructional Video QA (MedVidQA) is a video-language cross-modal dataset for the Medical Visual Answer Localization (MVAL) task <ref type="bibr" target="#b43">(Gupta et al., 2023)</ref>. Three medical informatics experts created 3010 health-related instructional questions for 899 videos extracted</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Rater agreement metrics for the long EHR understanding task</head><p>To ensure the reliability of the EHR benchmark, we collect ratings from three independent raters for each of the 200 example questions. The following metrics demonstrate strong consistency among raters:</p><p>• Jaccard Similarity Index: Measures the overlap between sets of rater selections. Let 𝐴, 𝐵, and 𝐶 represent the sets of selections made by each rater. The Jaccard similarity index for unanimous selections is defined as 𝐽  A Jaccard similarity index of 0.83 for unanimous selections indicates substantial agreement when all three raters select identical choices. An even higher Jaccard index of 0.915 reflects strong consistency when at least two out of three raters make the same selections. A Krippendorff 's alpha of 0.77 indicates good agreement on the existence of medical conditions within the EHR data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tarver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Loyo-Berrios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Char</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Eydelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Of Ophthalmic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Imaging</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName><surname>Maisel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Considerations for addressing bias in artificial intelligence for health equity</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Capabilities of GPT-4 in ophthalmology: an analysis of model entropy and progress towards humanlevel medical question answering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Antaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Milad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-É</forename><surname>Giguère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Touma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>El-Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Ophthalmology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Culp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mitrović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Strachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="756" to="779" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pathways: Asynchronous distributed dataflow for ML</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning and Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="430" to="449" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph of thoughts: Solving elaborate problems with large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kubicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gerstenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Podstawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nyczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Manrai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.05591</idno>
		<title level="m">Accuracy of a vision-language model on challenging medical cases</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fairvis: Visual analytics for discovering intersectional bias in machine learning</title>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">A</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large language models as tool makers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17126</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implementing machine learning in health care-addressing ethical challenges</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Char</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Magnus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New England journal of medicine</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">981</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Endo3d: Online workflow analysis for endoscopic surgeries based on 3d cnn and lstm. In OR 2.0 Context-Aware Operating Theaters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop, CARE 2018, 7th International Workshop, CLIP 2018, Third International Workshop, ISIC 2018, Held in Conjunction with MICCAI 2018</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2.0 2018. 5th. September 16 and 20, 2018. 2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="97" to="107" />
		</imprint>
	</monogr>
	<note>Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis: First International Workshop</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grycner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06794</idno>
		<title level="m">PaLI: A jointly-scaled multilingual language-image model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PaLM: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cirillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Catuara-Solarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Subirats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mellino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gigante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rementeria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chadha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fto obesity variant circuitry and adipocyte browning in humans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Claussnitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Dankel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Quon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meuleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haugen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Glunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Beaudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Puviindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="895" to="907" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Standards for reporting plain language summaries (pls) for cochrane diagnostic test accuracy reviews</title>
		<author>
			<persName><surname>Cochrane</surname></persName>
		</author>
		<ptr target="https://methods.cochrane.org/sites/methods.cochrane.org.sdt/files/uploads/DraftPLSdocu-ment.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Revisiting transformer-based models for long document classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06683</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Challenges and opportunities facing medical education</title>
		<author>
			<persName><forename type="first">P</forename><surname>Densen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Clinical and Climatological Association</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Paragraph-level simplification of medical texts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Devaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.naacl-main.395" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="4972" to="4984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<title level="m">An embodied multimodal language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Use of GPT-4 to diagnose complex clinical cases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Eriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ryg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building a clinically-focused problem list from medical notes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peled-Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)</title>
		<meeting>the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">120</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A clinician-generated dataset for instruction following with electronic medical records</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Haberkorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blankemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Genkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.14089</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extracting information from the text of electronic medical records to improve case detection: a systematic review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1007" to="1015" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Galatzer-Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malgaroli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.01834</idno>
		<title level="m">The capability of large language models to measure psychiatric functioning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tackling bias in ai health datasets through the standing together initiative</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Alderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Calvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Espinoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2232" to="2233" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for large language models: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gemini: A family of highly capable multimodal models</title>
		<author>
			<persName><surname>Gemini Team</surname></persName>
		</author>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename></persName>
		</author>
		<ptr target="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ai recognition of patient race in medical imaging: a modelling study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gichoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bhimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dullerud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="406" to="e414" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Artificial intelligence for phase recognition in complex laparoscopic cholecystectomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Golany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashtan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical Endoscopy</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9215" to="9223" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Locke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07219</idno>
		<title level="m">A real-time spatiotemporal ai model analyzes skill in open surgical videos</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Analyzing surgical technique in diverse open surgical videos with multitask machine learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Locke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Downing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA surgery</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="192" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">When less is more: a practical approach to searching for evidence-based answers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Grandage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Slawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Shaughnessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">298</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Clinical reasoning: defining it, teaching it, assessing it, studying it</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Gruppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Western Journal of Emergency Medicine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overview of the MedVidQA 2022 shared task on medical video question-answering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.bionlp-1.25</idno>
		<ptr target="https://aclanthology.org/2022.bionlp-1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on Biomedical Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 21st Workshop on Biomedical Language Processing<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="264" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A dataset for medical instructional video classification and question answering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Attal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">158</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ToolkenGPT: Augmenting frozen language models with massive tools via tool embeddings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">PathVQA: 30000+ questions for medical visual question answering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12435</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Diagnostic strategies in the hypothesis-directed pathfinder system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Nathwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Fagan</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/diagnostic-strategies-hypothesis-directed-pathfinder-system/" />
		<imprint>
			<date type="published" when="1984-01">January 1984</date>
			<biblScope unit="page" from="630" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">GeneTuring tests GPT models in genomics</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Towards reasoning in large language models: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generative artificial intelligence for chest radiograph interpretation in the emergency department</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wittbrodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Melnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Network Open</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2336100" to="e2336100" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">MAIRA-1: A specialised large multimodal model for radiology report generation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouzid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Salvatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13668</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The intersections of gender and class in health status and health care</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Östlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global public health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Associations between age discrimination and health and wellbeing: cross-sectional and prospective analysis of the english longitudinal study of ageing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steptoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Public Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="200" to="e208" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mining electronic health records: towards better research applications and clinical care</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brunak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="395" to="405" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">GeneGPT: Augmenting large language models with domain tools for improved access to biomedical information</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e075</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Anthony</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>-Y. Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>-Y. Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<title level="m">MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Accuracy of a generative artificial intelligence model in a complex diagnostic challenge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kanjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="80" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gender disparities in health care</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Varela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mount Sinai Journal of Medicine: A Journal of Translational and Personalized Medicine</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="555" to="559" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Information overload in healthcare: too much of a good thing? Zeitschrift für Evidenz</title>
		<author>
			<persName><forename type="first">I</forename><surname>Klerings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weinhandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Thaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fortbildung und Qualität im Gesundheitswesen</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="285" to="290" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Coronavirus goes viral: quantifying the covid-19 misinformation epidemic on twitter</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kouzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abi Jaoude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kraitem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>El Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zarka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Traboulsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Akl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baddour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cureus</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Linking the fto obesity rs1421085 variant circuitry to cellular, metabolic, and organismal phenotypes in vivo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Forcisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petzold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Al Sadat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Agnew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A 176b-parameter open-access multilingual language model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gallé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pixel-accurate segmentation of surgical tools based on bounding box annotations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Leifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Golany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 26th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5096" to="5103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Towards visual-prompt temporal answering grounding in medical instructional video</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06667</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">LLaVa-Med: Training a large language-and-vision assistant for biomedicine in one day</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A comparative study of pretrained language models for long clinical text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="340" to="347" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Slake: A semantically-labeled knowledgeenhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1650" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A translational perspective towards clinical ai fairness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teixayavong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S W</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C L</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">172</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Lost in the middle: How language models use long contexts</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The genetics of obesity: from discovery to biology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Yeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="133" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Gadsden</surname></persName>
		</author>
		<title level="m">Perspectives on health equity and social determinants of health</title>
		<meeting><address><addrLine>US</addrLine></address></meeting>
		<imprint>
			<publisher>National Academies Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Health inequities, social determinants, and intersectionality</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Parwani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.07814</idno>
		<title level="m">A foundational multimodal vision language ai assistant for human pathology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">409</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Multimodal composite association score: Measuring gender bias in generative multimodal models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13855</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alapatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vardazaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Costamagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dallemagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10916</idno>
		<title level="m">Surgical data science for safe cholecystectomy: a protocol for segmentation of hepatocystic anatomy and assessment of the critical view of safety</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Towards accurate differential diagnosis with large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Palepu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00164</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Health inequities in lgbt people and nursing interventions to reduce them: A systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Medina-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saus-Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Sánchez-Lorente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Sosa-Palanca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mármol-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Environmental Research and Public Health</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">11801</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Papers with code -medical</title>
		<author>
			<persName><forename type="first">Meta</forename></persName>
		</author>
		<ptr target="https://paperswithcode.com/area/medical" />
		<imprint>
			<date type="published" when="2024-04-26">2024. 2024-04-26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Foundation models for generalist medical artificial intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S H</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Krumholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<biblScope unit="issue">7956</biblScope>
			<biblScope unit="page" from="259" to="265" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Med-flamingo: a multimodal medical few-shot learner</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health (ML4H)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="353" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09332</idno>
		<title level="m">Browser-assisted question-answering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Edgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16452</idno>
		<title level="m">Can generalist foundation models outcompete special-purpose tuning? case study in medicine</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Making sense of conflicting science information: Exploring bias in the search engine result page</title>
		<author>
			<persName><forename type="first">A</forename><surname>Novin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 conference on conference human information interaction and retrieval</title>
		<meeting>the 2017 conference on conference human information interaction and retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Weakly supervised convolutional lstm approach for tool tracking in laparoscopic videos</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1059" to="1067" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">6464</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>-M. Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="66277" to="66288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2023/file/d0b67349dd16b83b2cf6167fb4e2be50-Paper-Datasets_and_Benchmarks.pdf" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Large language models propagate race-based medicine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Omiye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Spichak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">195</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">PAD-UFES-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Salomao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krohling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Biral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>De Angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Alves</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Esgario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Simora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data in brief</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">106221</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">LongBoX: Evaluating transformers on long-sequence clinical tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Radiology objects in context (roco): a multimodal image dataset</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Asiedu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.12025</idno>
		<title level="m">A toolbox for surfacing health equity harms and biases in large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Hyena hierarchy: Towards larger convolutional language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="28043" to="28078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Reasoning with language model prompting: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16789</idno>
		<title level="m">ToolLLM: Facilitating large language models to master 16000+ real-world apis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">AI in health and medicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Improving radiology report generation systems by removing hallucinated references to non-existent priors</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N A</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Machine Learning for Health symposium</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Parziale</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Oala</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Subbaswamy</surname></persName>
		</editor>
		<meeting>the 2nd Machine Learning for Health symposium</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-11-28">28 Nov 2022</date>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Mitigating ethnic disparities in covid-19 and beyond</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Razai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Kankam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esmail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bmj</title>
		<imprint>
			<biblScope unit="page">372</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Cholec80-cvs: An open dataset with an evaluation of strasberg&apos;s critical view of safety for ai</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ríos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Molina-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Londoño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Guillén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Giraldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">194</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A simple effective method for generation of a permanent record of the critical view of safety during laparoscopic cholecystectomy by intraoperative &quot;doublet&quot; photography</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Strasberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Surgeons</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="178" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Information overload in emergency medicine physicians: a multisite case study exploring the causes, impact, and solutions in four north england national health service trusts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sbaffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blenkinsopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Walton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e19126</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Toolformer: Language models can teach themselves to use tools</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dessì</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">After visit summary: Not an afterthought</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sieferd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Holden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Human Factors and Ergonomics in Health Care</title>
		<meeting>the International Symposium on Human Factors and Ergonomics in Health Care<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications Sage CA</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="page" from="172" to="180" />
			<date type="published" when="2023">7972. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Towards expert-level medical question answering with large language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09617</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08666</idno>
		<title level="m">Worst of both worlds: Biases compound in pre-trained vision-and-language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Lower socioeconomic status and the acceleration of aging: An outcomewide analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Steptoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zaninotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="14911" to="14917" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Rationale and use of the critical view of safety in laparoscopic cholecystectomy</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Strasberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Brunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Surgeons</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="138" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Evaluating AI systems under uncertain ground truth: a case study in dermatology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matejovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barsbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rikhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Telang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sellergren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghaisas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<ptr target="https://www.nejm.org/image-challenge" />
	</analytic>
	<monogr>
		<title level="j">The New England Journal of Medicine</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note>Image challenge</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Lawler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12031</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Towards generalist biomedical AI</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ktena</surname></persName>
		</author>
		<idno>AIoa</idno>
	</analytic>
	<monogr>
		<title level="j">NEJM AI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2300138</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Palepu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05654</idno>
		<title level="m">Towards conversational diagnostic AI</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Med-halt: Medical domain hallucination test for large language models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sankarasubbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15343</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03987</idno>
		<title level="m">A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">PTB-XL, a large publicly available electrocardiography dataset</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-D</forename><surname>Bousseljot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kreiseler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Lunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaeffter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Electrocardiogram instruction tuning for report generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arcucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Towards intersectionality in machine learning: Including more identities, handling underrepresentation, and performing evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2022 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="336" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tiyasirichokchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Virmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18545</idno>
		<title level="m">Crowdsourcing dermatology images with google search ads: Creating a real-world skin condition dataset</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Causes and prevention of laparoscopic bile duct injuries: analysis of 252 cases from a human factors and cognitive psychology perspective</title>
		<author>
			<persName><forename type="first">L</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gantert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of surgery</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="460" to="469" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Visual answer localization with cross-modal mutual knowledge transfer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Racial bias in health care and health: challenges and opportunities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wyatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">314</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="556" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassasola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riantawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Riantawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02008</idno>
		<title level="m">How well do llms cite relevant medical references? an evaluation framework and analyses</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Towards a general purpose x-ray artificial intelligence system through alignment of large language models and radiology vision encoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sieniek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kiraly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Melamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.01317</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Fairness with overlapping groups; a probabilistic perspective</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4067" to="4078" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<title level="m">Coca: Contrastive captioners are image-text foundation models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16502</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Almanac-retrieval-augmented language models for clinical medicine</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ashley</surname></persName>
		</author>
		<idno>AIoa</idno>
	</analytic>
	<monogr>
		<title level="j">NEJM AI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2300068</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Training small multimodal models to bridge biomedical competency gap: A case study in radiology imaging</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zambrano Chaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08002</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Star: Self-taught reasoner bootstrapping reasoning with reasoning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Raft: Adapting language model to domain specific rag</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.10131</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Least-to-most prompting enables complex reasoning in large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
